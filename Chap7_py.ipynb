{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 畳み込みネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Convolution / Poolingレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1 4次元配列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.rand(10, 1, 28, 28) # ランダムにデータを生成\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape # 1つ目のデータにアクセス. (1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1].shape # 2つ目のデータにアクセス. (1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50314773, 0.02010111, 0.29824062, 0.41081747, 0.1104862 ,\n",
       "        0.71809124, 0.99714848, 0.46179259, 0.42546508, 0.75866732,\n",
       "        0.53650481, 0.62224765, 0.39140766, 0.21440149, 0.64959127,\n",
       "        0.47155295, 0.13775211, 0.27653699, 0.08869034, 0.11548296,\n",
       "        0.96230659, 0.63627579, 0.22067741, 0.77279314, 0.22321556,\n",
       "        0.96208114, 0.90043464, 0.12921593],\n",
       "       [0.12935989, 0.36268397, 0.74365969, 0.26779202, 0.55628933,\n",
       "        0.798729  , 0.16749911, 0.39823327, 0.6961941 , 0.85125801,\n",
       "        0.84553473, 0.58123161, 0.09306764, 0.70251515, 0.74076846,\n",
       "        0.26890364, 0.73307079, 0.3331027 , 0.47342265, 0.88940869,\n",
       "        0.47005862, 0.22758157, 0.78452763, 0.17309371, 0.98530086,\n",
       "        0.36313938, 0.4865346 , 0.5114959 ],\n",
       "       [0.21332189, 0.62454135, 0.93820707, 0.90796258, 0.68486689,\n",
       "        0.40830686, 0.27907715, 0.89228979, 0.51531886, 0.30110219,\n",
       "        0.50768791, 0.25833619, 0.29597463, 0.53797972, 0.25921112,\n",
       "        0.17599448, 0.70881626, 0.56345697, 0.87714591, 0.66144449,\n",
       "        0.83406951, 0.1040727 , 0.30931756, 0.42633794, 0.32630454,\n",
       "        0.42243678, 0.82533079, 0.0432786 ],\n",
       "       [0.63939166, 0.50008968, 0.85071769, 0.45030965, 0.43462887,\n",
       "        0.74932772, 0.99455816, 0.5401256 , 0.61375247, 0.36124373,\n",
       "        0.02559653, 0.79985238, 0.56089128, 0.8071805 , 0.24611969,\n",
       "        0.34531366, 0.08887833, 0.95363477, 0.91900096, 0.28931355,\n",
       "        0.51385607, 0.13009287, 0.22714064, 0.1659855 , 0.57978011,\n",
       "        0.07581769, 0.41212907, 0.02805357],\n",
       "       [0.10790691, 0.18349648, 0.30781076, 0.94555496, 0.59783371,\n",
       "        0.34456057, 0.05193345, 0.36795851, 0.7357763 , 0.41553458,\n",
       "        0.55525623, 0.93972019, 0.60477304, 0.46767691, 0.11026403,\n",
       "        0.48081985, 0.60479836, 0.80111097, 0.52608901, 0.90484274,\n",
       "        0.29624896, 0.38192277, 0.40075123, 0.76459234, 0.97217788,\n",
       "        0.69751704, 0.48847434, 0.13938624],\n",
       "       [0.70546781, 0.08557451, 0.11341352, 0.57829257, 0.12903961,\n",
       "        0.7109173 , 0.53969442, 0.53001643, 0.41184658, 0.53106552,\n",
       "        0.7386057 , 0.25559681, 0.11071945, 0.90693506, 0.59526674,\n",
       "        0.38895192, 0.91112434, 0.00228849, 0.76424585, 0.64649356,\n",
       "        0.09105824, 0.91120686, 0.09399528, 0.46815545, 0.95449813,\n",
       "        0.77682719, 0.8226958 , 0.18224103],\n",
       "       [0.87607067, 0.40519842, 0.31515616, 0.65676016, 0.58871702,\n",
       "        0.98208361, 0.10150303, 0.74562636, 0.22225568, 0.08309002,\n",
       "        0.91224752, 0.31877807, 0.58322738, 0.86263411, 0.34313835,\n",
       "        0.31307919, 0.83932409, 0.04066194, 0.93878909, 0.64233399,\n",
       "        0.64380982, 0.01791229, 0.87376621, 0.94002563, 0.62301842,\n",
       "        0.78794293, 0.44121058, 0.16458559],\n",
       "       [0.19982909, 0.5086446 , 0.46202246, 0.52951072, 0.17230661,\n",
       "        0.57563108, 0.79918023, 0.86867883, 0.95703146, 0.79212668,\n",
       "        0.60280221, 0.73738531, 0.79762059, 0.41711864, 0.7317602 ,\n",
       "        0.54601295, 0.97307754, 0.66848786, 0.3252413 , 0.70189177,\n",
       "        0.37774327, 0.0070762 , 0.28848046, 0.24468746, 0.70244908,\n",
       "        0.45899255, 0.29252608, 0.01725311],\n",
       "       [0.44457773, 0.13568409, 0.23808834, 0.35759385, 0.82411688,\n",
       "        0.55110504, 0.2369352 , 0.04162018, 0.87474143, 0.63064456,\n",
       "        0.47335875, 0.12146286, 0.80915851, 0.9318443 , 0.3089053 ,\n",
       "        0.33830842, 0.39912318, 0.52851768, 0.32247245, 0.52431866,\n",
       "        0.32631315, 0.9121853 , 0.31012837, 0.74620221, 0.35167859,\n",
       "        0.36005381, 0.68868023, 0.05628   ],\n",
       "       [0.70492181, 0.6146745 , 0.16682374, 0.0455228 , 0.41809173,\n",
       "        0.58654765, 0.08041916, 0.45350578, 0.58156302, 0.90280211,\n",
       "        0.28186543, 0.44893485, 0.83203439, 0.52544457, 0.99525973,\n",
       "        0.77301002, 0.4017626 , 0.64618201, 0.53287871, 0.37575209,\n",
       "        0.92537195, 0.54090451, 0.84189459, 0.20558204, 0.09746211,\n",
       "        0.74488839, 0.81376069, 0.37251983],\n",
       "       [0.5930402 , 0.82118272, 0.51682342, 0.53351233, 0.44886824,\n",
       "        0.75493248, 0.15493194, 0.79673616, 0.89880278, 0.86029552,\n",
       "        0.69385313, 0.76816987, 0.74778515, 0.50931918, 0.00307314,\n",
       "        0.9535118 , 0.18065283, 0.72087622, 0.88705054, 0.67991196,\n",
       "        0.76568042, 0.1528634 , 0.11575054, 0.74553455, 0.64143815,\n",
       "        0.42350842, 0.64837723, 0.73502739],\n",
       "       [0.73472363, 0.75922182, 0.12585875, 0.28264761, 0.78995067,\n",
       "        0.30031809, 0.30193782, 0.72744743, 0.48835297, 0.99199234,\n",
       "        0.89466791, 0.4139511 , 0.96607426, 0.97285126, 0.38099959,\n",
       "        0.08710403, 0.97229758, 0.682728  , 0.12472054, 0.36119039,\n",
       "        0.19652429, 0.70314829, 0.87110991, 0.35021718, 0.40648127,\n",
       "        0.81737023, 0.1350065 , 0.01948069],\n",
       "       [0.99755356, 0.83604234, 0.6062298 , 0.92383894, 0.23480556,\n",
       "        0.51339663, 0.46158643, 0.05098568, 0.65634798, 0.53086724,\n",
       "        0.22775669, 0.41642612, 0.86818879, 0.91054931, 0.13931944,\n",
       "        0.54916651, 0.71434585, 0.33216101, 0.0903104 , 0.20727149,\n",
       "        0.45550493, 0.39330688, 0.85927043, 0.03626365, 0.12102386,\n",
       "        0.24301961, 0.35531944, 0.00155489],\n",
       "       [0.99228606, 0.69859303, 0.25075762, 0.44585228, 0.45662285,\n",
       "        0.55993298, 0.9765383 , 0.22982038, 0.57685043, 0.9316561 ,\n",
       "        0.24742414, 0.78436383, 0.12367031, 0.80810958, 0.15392334,\n",
       "        0.53006169, 0.75177222, 0.44483315, 0.93371187, 0.35659058,\n",
       "        0.68362849, 0.633319  , 0.32082356, 0.88203254, 0.8519953 ,\n",
       "        0.68521981, 0.06847646, 0.00414338],\n",
       "       [0.06453267, 0.16248897, 0.46799354, 0.1149972 , 0.13130961,\n",
       "        0.95199818, 0.8243706 , 0.35207477, 0.39892655, 0.11089122,\n",
       "        0.99213517, 0.51768071, 0.50054094, 0.06964158, 0.05671964,\n",
       "        0.17275689, 0.22490037, 0.51958405, 0.17765193, 0.24236858,\n",
       "        0.43879935, 0.17666733, 0.36889618, 0.83020944, 0.28866258,\n",
       "        0.38313522, 0.0781006 , 0.48557611],\n",
       "       [0.63017818, 0.76687766, 0.89733759, 0.4157002 , 0.58217876,\n",
       "        0.17803197, 0.07288688, 0.55162024, 0.67500253, 0.69744706,\n",
       "        0.7612765 , 0.48207721, 0.84094448, 0.60024508, 0.24258975,\n",
       "        0.58781427, 0.96941832, 0.42513067, 0.29588248, 0.91793911,\n",
       "        0.57264498, 0.22464652, 0.59506286, 0.1363793 , 0.72643905,\n",
       "        0.82276247, 0.93196719, 0.74145734],\n",
       "       [0.69782415, 0.02188741, 0.37471373, 0.84484188, 0.52500633,\n",
       "        0.94093334, 0.45501966, 0.37571031, 0.751964  , 0.28673388,\n",
       "        0.67256247, 0.34833081, 0.72862278, 0.64037789, 0.54965966,\n",
       "        0.70406262, 0.43077923, 0.214589  , 0.70366586, 0.46646881,\n",
       "        0.44276112, 0.27471356, 0.38346599, 0.17192507, 0.24624033,\n",
       "        0.17729088, 0.12987972, 0.20298413],\n",
       "       [0.21259171, 0.06345286, 0.13278249, 0.91005657, 0.39054934,\n",
       "        0.56035092, 0.46256812, 0.89268659, 0.71490695, 0.40030961,\n",
       "        0.94486956, 0.5944326 , 0.21904635, 0.89400376, 0.37704373,\n",
       "        0.6481505 , 0.50022073, 0.15330377, 0.50263006, 0.19844128,\n",
       "        0.0071526 , 0.11066602, 0.03391212, 0.00136999, 0.12223668,\n",
       "        0.28730535, 0.22585478, 0.14415102],\n",
       "       [0.29106194, 0.06725305, 0.75492085, 0.36061658, 0.48014401,\n",
       "        0.18030148, 0.11921151, 0.62503555, 0.99379156, 0.96333663,\n",
       "        0.66857781, 0.2633039 , 0.44676934, 0.63346326, 0.18406178,\n",
       "        0.97295205, 0.33109259, 0.17785546, 0.33343211, 0.02167889,\n",
       "        0.10178056, 0.37153022, 0.21254199, 0.90224545, 0.93643731,\n",
       "        0.04402662, 0.07552598, 0.16910848],\n",
       "       [0.24295034, 0.97521546, 0.65742342, 0.09674208, 0.23857805,\n",
       "        0.3906279 , 0.90043864, 0.30270481, 0.88679399, 0.61774372,\n",
       "        0.28887956, 0.24879326, 0.67174123, 0.7999989 , 0.70907677,\n",
       "        0.0213076 , 0.48261019, 0.08532301, 0.76753074, 0.58517911,\n",
       "        0.48442355, 0.07047589, 0.26669699, 0.00165871, 0.26207096,\n",
       "        0.279006  , 0.71432745, 0.64955168],\n",
       "       [0.92506439, 0.97027169, 0.89215371, 0.33530237, 0.06859312,\n",
       "        0.33707035, 0.31201638, 0.45159253, 0.4125648 , 0.65049999,\n",
       "        0.44870131, 0.17457157, 0.39810528, 0.05070202, 0.25741391,\n",
       "        0.92389093, 0.3371425 , 0.46702171, 0.90049735, 0.17451763,\n",
       "        0.19382429, 0.70470708, 0.5156242 , 0.10099247, 0.97015589,\n",
       "        0.14670587, 0.46899408, 0.71220072],\n",
       "       [0.66304106, 0.94188173, 0.86240952, 0.33771718, 0.46532637,\n",
       "        0.31485875, 0.38863267, 0.82277781, 0.9111016 , 0.77718389,\n",
       "        0.6246767 , 0.59440724, 0.4551741 , 0.99991513, 0.93201754,\n",
       "        0.77670449, 0.37367862, 0.9025888 , 0.13924299, 0.76566205,\n",
       "        0.47871669, 0.22177752, 0.06342988, 0.7135108 , 0.6633184 ,\n",
       "        0.27561921, 0.38466492, 0.60723492],\n",
       "       [0.52519004, 0.85960559, 0.38101705, 0.78812839, 0.78997297,\n",
       "        0.4468696 , 0.14747308, 0.14874388, 0.08343162, 0.08108269,\n",
       "        0.56245635, 0.87090069, 0.87290249, 0.99081926, 0.38262616,\n",
       "        0.48502429, 0.94859938, 0.46128819, 0.00599638, 0.12974253,\n",
       "        0.41137447, 0.79496157, 0.24497552, 0.01907101, 0.68298586,\n",
       "        0.85354367, 0.11509531, 0.33324398],\n",
       "       [0.16738663, 0.27022469, 0.65267925, 0.38984451, 0.03353658,\n",
       "        0.37343576, 0.59568224, 0.59475771, 0.3050103 , 0.84591355,\n",
       "        0.19304393, 0.33048546, 0.74064783, 0.16570893, 0.8539575 ,\n",
       "        0.8003405 , 0.63456224, 0.62252086, 0.21079193, 0.60990253,\n",
       "        0.07469274, 0.24759174, 0.35720469, 0.75990189, 0.84359476,\n",
       "        0.1477308 , 0.5243702 , 0.92824236],\n",
       "       [0.74177929, 0.20562566, 0.42209158, 0.75482507, 0.0587242 ,\n",
       "        0.03932338, 0.62771354, 0.56765031, 0.1997836 , 0.97926374,\n",
       "        0.67113786, 0.75571679, 0.67562713, 0.01066397, 0.61698625,\n",
       "        0.40359656, 0.70869221, 0.26925636, 0.66326896, 0.97318375,\n",
       "        0.23130973, 0.01222912, 0.15936914, 0.18344405, 0.64625161,\n",
       "        0.0223867 , 0.63530249, 0.12084928],\n",
       "       [0.88564147, 0.74914495, 0.85587169, 0.8266675 , 0.13109589,\n",
       "        0.28767625, 0.39365955, 0.42558474, 0.07444378, 0.53852873,\n",
       "        0.02829634, 0.07455691, 0.04976381, 0.82009382, 0.30286245,\n",
       "        0.39312504, 0.08690441, 0.48420694, 0.37196067, 0.76486014,\n",
       "        0.37978628, 0.07606977, 0.68435906, 0.99757808, 0.61577549,\n",
       "        0.25008097, 0.48360072, 0.84867612],\n",
       "       [0.97596462, 0.35176462, 0.27192434, 0.44712559, 0.48510433,\n",
       "        0.67141201, 0.63935594, 0.34925099, 0.17758718, 0.76606603,\n",
       "        0.38502165, 0.77673703, 0.03441352, 0.08817541, 0.55754787,\n",
       "        0.80700223, 0.04693363, 0.05071242, 0.04601957, 0.52881722,\n",
       "        0.02456379, 0.18262224, 0.91407467, 0.26313087, 0.82394138,\n",
       "        0.97159025, 0.6102047 , 0.78503268],\n",
       "       [0.32248184, 0.84246798, 0.176375  , 0.92434241, 0.20670414,\n",
       "        0.66453233, 0.20608444, 0.37409603, 0.29540228, 0.0845134 ,\n",
       "        0.59607827, 0.19311737, 0.51084539, 0.35486306, 0.96502829,\n",
       "        0.11884252, 0.92395107, 0.35129428, 0.10518336, 0.11480899,\n",
       "        0.68107184, 0.34534262, 0.03634968, 0.22047738, 0.06396596,\n",
       "        0.24657797, 0.36489963, 0.60510683]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0] # 1チャンネル目の空間データにアクセス, もしくはx[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3 Convolutionレイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\n",
    "    filter_h : フィルターの高さ\n",
    "    filter_w : フィルターの幅\n",
    "    stride : ストライド\n",
    "    pad : パディング\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2次元配列\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "coll = im2col(x1, 5, 5, stride = 1, pad = 0)\n",
    "print(coll.shape) # (9, 75)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7) # 10個のデータ\n",
    "col2 = im2col(x2, 5, 5, stride = 1, pad = 0)\n",
    "print(col2.shape) #(90, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution: \n",
    "    def __init__(self, W, b, stride = 1, pad = 0): \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "    \n",
    "    def forward(self, x): \n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T # フィルターの展開\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(o, 3, 1, 2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col :\n",
    "    input_shape : 入力データの形状（例：(10, 1, 28, 28)）\n",
    "    filter_h :\n",
    "    filter_w\n",
    "    stride\n",
    "    pad\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 中間データ（backward時に使用）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 重み・バイアスパラメータの勾配\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.4 Poolingレイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "#        self.x = None\n",
    "#        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        # 展開(1)\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "#        arg_max = np.argmax(col, axis=1)\n",
    "        # 最大値(2)\n",
    "        out = np.max(col, axis=1)\n",
    "        # 最大値(3)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "#        self.x = x\n",
    "#        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 CNNの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"単純なConvNet\n",
    "\n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 入力サイズ（MNISTの場合は784）\n",
    "    hidden_size_list : 隠れ層のニューロンの数のリスト（e.g. [100, 100, 100]）\n",
    "    output_size : 出力サイズ（MNISTの場合は10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
    "        'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
    "        'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"損失関数を求める\n",
    "        引数のxは入力データ、tは教師ラベル\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（数値微分）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（誤差逆伝搬法）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299748755245998\n",
      "=== epoch:1, train acc:0.363, test acc:0.389 ===\n",
      "train loss:2.297429905722912\n",
      "train loss:2.290090204182562\n",
      "train loss:2.2881329416864076\n",
      "train loss:2.2764572697026106\n",
      "train loss:2.272383712675069\n",
      "train loss:2.2575599988509585\n",
      "train loss:2.246611064239028\n",
      "train loss:2.2035093834255304\n",
      "train loss:2.1997930297992205\n",
      "train loss:2.1809009817342564\n",
      "train loss:2.151497414651663\n",
      "train loss:2.083762059540119\n",
      "train loss:2.0164656134103556\n",
      "train loss:1.9862587965607255\n",
      "train loss:1.9307814581916114\n",
      "train loss:1.847066981292377\n",
      "train loss:1.7366085159148228\n",
      "train loss:1.7607630491835529\n",
      "train loss:1.6421829172276878\n",
      "train loss:1.603955351891702\n",
      "train loss:1.4985054607703479\n",
      "train loss:1.372026151904675\n",
      "train loss:1.3626628363549032\n",
      "train loss:1.2326938825102938\n",
      "train loss:1.2752914833132858\n",
      "train loss:1.047200374977956\n",
      "train loss:1.0483299692214585\n",
      "train loss:0.9850571050985794\n",
      "train loss:0.9628816176423968\n",
      "train loss:0.9512399063760506\n",
      "train loss:0.8864245991415064\n",
      "train loss:0.8959990822642878\n",
      "train loss:0.7536400107373076\n",
      "train loss:0.7498515790904353\n",
      "train loss:0.7484740849889577\n",
      "train loss:0.8554114435120306\n",
      "train loss:0.7084778083136709\n",
      "train loss:0.7941771878535744\n",
      "train loss:0.6166070498057297\n",
      "train loss:0.6240572781609625\n",
      "train loss:0.7132619936371172\n",
      "train loss:0.5669000624802277\n",
      "train loss:0.5151256294494029\n",
      "train loss:0.6416531526225248\n",
      "train loss:0.8410380289316575\n",
      "train loss:0.44624277081960206\n",
      "train loss:0.6723732056010883\n",
      "train loss:0.6342956223026472\n",
      "train loss:0.6523103119230461\n",
      "train loss:0.6519461778714434\n",
      "train loss:0.4599857357224088\n",
      "train loss:0.4647714849086613\n",
      "train loss:0.6024702083037485\n",
      "train loss:0.6050657144128151\n",
      "train loss:0.4528870391956562\n",
      "train loss:0.41920429797188097\n",
      "train loss:0.6365840766259088\n",
      "train loss:0.5677913205292914\n",
      "train loss:0.46749683840314593\n",
      "train loss:0.5693128901845985\n",
      "train loss:0.41414668044630853\n",
      "train loss:0.37753705562856227\n",
      "train loss:0.5433394223447752\n",
      "train loss:0.2253102754964829\n",
      "train loss:0.4468346043781458\n",
      "train loss:0.4519507613312219\n",
      "train loss:0.46748844016247376\n",
      "train loss:0.4424226163047226\n",
      "train loss:0.4620432018561659\n",
      "train loss:0.618575153302544\n",
      "train loss:0.36053810650475\n",
      "train loss:0.5268292866347926\n",
      "train loss:0.4949049859274013\n",
      "train loss:0.6533031987021986\n",
      "train loss:0.41978743220049625\n",
      "train loss:0.6696265890777543\n",
      "train loss:0.5158153369717183\n",
      "train loss:0.4227342160064371\n",
      "train loss:0.39260659435332246\n",
      "train loss:0.30024291879797593\n",
      "train loss:0.41397672548743736\n",
      "train loss:0.3081719774089949\n",
      "train loss:0.5318844551262693\n",
      "train loss:0.44004597888676117\n",
      "train loss:0.40876819110733015\n",
      "train loss:0.5528327893851802\n",
      "train loss:0.3691237335511366\n",
      "train loss:0.43449304138229794\n",
      "train loss:0.4826978562248716\n",
      "train loss:0.4860291884136344\n",
      "train loss:0.4871248523320162\n",
      "train loss:0.39159483022868863\n",
      "train loss:0.3671388576283448\n",
      "train loss:0.30965277987887146\n",
      "train loss:0.5389757765817932\n",
      "train loss:0.6179130790283819\n",
      "train loss:0.24406233560201407\n",
      "train loss:0.43818519548838075\n",
      "train loss:0.659258205023563\n",
      "train loss:0.312041628588429\n",
      "train loss:0.3113314006090678\n",
      "train loss:0.350983321285707\n",
      "train loss:0.4014645648426115\n",
      "train loss:0.3642178769933573\n",
      "train loss:0.3728094336646428\n",
      "train loss:0.48566550528150826\n",
      "train loss:0.2854648527149703\n",
      "train loss:0.33752247620950876\n",
      "train loss:0.29586972735968436\n",
      "train loss:0.29875699196083966\n",
      "train loss:0.4265811521971177\n",
      "train loss:0.3672713708935516\n",
      "train loss:0.3405528470671442\n",
      "train loss:0.43259532908834\n",
      "train loss:0.2355919346885198\n",
      "train loss:0.3210407237814653\n",
      "train loss:0.3978168503660302\n",
      "train loss:0.2882506626431603\n",
      "train loss:0.404691802632593\n",
      "train loss:0.3074659344795898\n",
      "train loss:0.5240473964412298\n",
      "train loss:0.31234532511926394\n",
      "train loss:0.2878242550602619\n",
      "train loss:0.44306714958970184\n",
      "train loss:0.17840430306057647\n",
      "train loss:0.2789288520821196\n",
      "train loss:0.2684523859975535\n",
      "train loss:0.23803674773628036\n",
      "train loss:0.3795935274356336\n",
      "train loss:0.28031464990478044\n",
      "train loss:0.3807609251562126\n",
      "train loss:0.24896455343833196\n",
      "train loss:0.4675368374308074\n",
      "train loss:0.3078436653516856\n",
      "train loss:0.3808848185826015\n",
      "train loss:0.36396262729364603\n",
      "train loss:0.2812962656986707\n",
      "train loss:0.4148824902493989\n",
      "train loss:0.22473035842982164\n",
      "train loss:0.33420545193133533\n",
      "train loss:0.34935225373196693\n",
      "train loss:0.2595393294437607\n",
      "train loss:0.3147973911729219\n",
      "train loss:0.34544233748894926\n",
      "train loss:0.3385577220894726\n",
      "train loss:0.4046199790328967\n",
      "train loss:0.40557317795115316\n",
      "train loss:0.3186825695396295\n",
      "train loss:0.2738318981457267\n",
      "train loss:0.28100786520242044\n",
      "train loss:0.39278015254610266\n",
      "train loss:0.285172991509612\n",
      "train loss:0.21881780997363232\n",
      "train loss:0.29317127532998455\n",
      "train loss:0.4664973871361532\n",
      "train loss:0.293615683221628\n",
      "train loss:0.254382346178799\n",
      "train loss:0.3849082332928462\n",
      "train loss:0.3029997158304165\n",
      "train loss:0.2812395944271417\n",
      "train loss:0.36624225144819084\n",
      "train loss:0.3358653263755101\n",
      "train loss:0.4196345302300606\n",
      "train loss:0.40017020491963684\n",
      "train loss:0.39987590200847734\n",
      "train loss:0.18626988956238805\n",
      "train loss:0.42604243869401026\n",
      "train loss:0.4426052214915485\n",
      "train loss:0.19741300116934268\n",
      "train loss:0.42504482634496865\n",
      "train loss:0.4334253718846107\n",
      "train loss:0.253704703231166\n",
      "train loss:0.3375955872298136\n",
      "train loss:0.31291263193297225\n",
      "train loss:0.3333067661731938\n",
      "train loss:0.3252302883085001\n",
      "train loss:0.18601800033775118\n",
      "train loss:0.24351701905137163\n",
      "train loss:0.22259352599400004\n",
      "train loss:0.27338360923339716\n",
      "train loss:0.32869384485851394\n",
      "train loss:0.4096663679043261\n",
      "train loss:0.2830610401606906\n",
      "train loss:0.21069656723668673\n",
      "train loss:0.22103180720131238\n",
      "train loss:0.2528332819722025\n",
      "train loss:0.23272253971843881\n",
      "train loss:0.2257071264316488\n",
      "train loss:0.4563679291626883\n",
      "train loss:0.24471402031339548\n",
      "train loss:0.28907318418408406\n",
      "train loss:0.46075014734650005\n",
      "train loss:0.2501632588173438\n",
      "train loss:0.34028766395712245\n",
      "train loss:0.32680031813289306\n",
      "train loss:0.3628579084629849\n",
      "train loss:0.3392469060666493\n",
      "train loss:0.19816489188931816\n",
      "train loss:0.28800990207902194\n",
      "train loss:0.34005722220315315\n",
      "train loss:0.3786716214381379\n",
      "train loss:0.44540902741258587\n",
      "train loss:0.30081589754403515\n",
      "train loss:0.3822391824779073\n",
      "train loss:0.23662875815034337\n",
      "train loss:0.2710549735046113\n",
      "train loss:0.3589818264736604\n",
      "train loss:0.262195534169728\n",
      "train loss:0.21590570063595155\n",
      "train loss:0.3303684680092136\n",
      "train loss:0.2552229489431497\n",
      "train loss:0.2961997290950687\n",
      "train loss:0.30182084391412384\n",
      "train loss:0.266448895076404\n",
      "train loss:0.2547296748564648\n",
      "train loss:0.251163767576381\n",
      "train loss:0.33384580835175426\n",
      "train loss:0.21709859782793114\n",
      "train loss:0.24981008709238875\n",
      "train loss:0.2242911554677423\n",
      "train loss:0.2246730142490569\n",
      "train loss:0.24740725392661325\n",
      "train loss:0.1927590626717054\n",
      "train loss:0.19351163257857976\n",
      "train loss:0.4205832353063586\n",
      "train loss:0.3690941867381127\n",
      "train loss:0.22240497342519386\n",
      "train loss:0.2596580217598894\n",
      "train loss:0.16332352027107305\n",
      "train loss:0.24962483390967546\n",
      "train loss:0.2426764060358623\n",
      "train loss:0.17293321886308363\n",
      "train loss:0.2502129678288374\n",
      "train loss:0.3076857244340437\n",
      "train loss:0.20824473943191027\n",
      "train loss:0.16846854750575233\n",
      "train loss:0.24092668361943667\n",
      "train loss:0.18840129198631977\n",
      "train loss:0.07664387954765037\n",
      "train loss:0.20634844896761365\n",
      "train loss:0.183086990588282\n",
      "train loss:0.20796207527559918\n",
      "train loss:0.24084270054754703\n",
      "train loss:0.2241206955001866\n",
      "train loss:0.1425033520219077\n",
      "train loss:0.1795695570577617\n",
      "train loss:0.20261151952951234\n",
      "train loss:0.21902622130016547\n",
      "train loss:0.19162373818700892\n",
      "train loss:0.1974037649685075\n",
      "train loss:0.29414191952121954\n",
      "train loss:0.38697888118784\n",
      "train loss:0.26248894308062753\n",
      "train loss:0.22008889789940678\n",
      "train loss:0.23066181016119114\n",
      "train loss:0.2995114903043404\n",
      "train loss:0.14844664195321636\n",
      "train loss:0.19470486366779433\n",
      "train loss:0.1693377214462159\n",
      "train loss:0.3783131562443382\n",
      "train loss:0.3225533374355765\n",
      "train loss:0.34112231026778866\n",
      "train loss:0.418238190549323\n",
      "train loss:0.2832688847151208\n",
      "train loss:0.28357657555312926\n",
      "train loss:0.22406768484648587\n",
      "train loss:0.16267687082264473\n",
      "train loss:0.24269333079695893\n",
      "train loss:0.3068764079169973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.27430715059890276\n",
      "train loss:0.2736885242494234\n",
      "train loss:0.2915911685430388\n",
      "train loss:0.3073572495023866\n",
      "train loss:0.208381151101929\n",
      "train loss:0.2304389180946482\n",
      "train loss:0.19925289732568083\n",
      "train loss:0.17408540706493844\n",
      "train loss:0.1658249418662356\n",
      "train loss:0.21396794070984318\n",
      "train loss:0.23404868856286423\n",
      "train loss:0.22823600843743097\n",
      "train loss:0.10670614956698754\n",
      "train loss:0.15711913711005537\n",
      "train loss:0.18915402609886223\n",
      "train loss:0.22986550246640217\n",
      "train loss:0.16117388462177046\n",
      "train loss:0.23681508602805304\n",
      "train loss:0.303725578007057\n",
      "train loss:0.1687970033866992\n",
      "train loss:0.14670405315210824\n",
      "train loss:0.15529020068618402\n",
      "train loss:0.11130703814132663\n",
      "train loss:0.2642077691299533\n",
      "train loss:0.13963776263052402\n",
      "train loss:0.17261384120598744\n",
      "train loss:0.2049858389491234\n",
      "train loss:0.19816915709077662\n",
      "train loss:0.09547630755435592\n",
      "train loss:0.1501308827132404\n",
      "train loss:0.2649214371307962\n",
      "train loss:0.15583812024856852\n",
      "train loss:0.2403629903584406\n",
      "train loss:0.3640644117982299\n",
      "train loss:0.281275121665022\n",
      "train loss:0.1743807481197587\n",
      "train loss:0.2777572721390089\n",
      "train loss:0.19047250815841923\n",
      "train loss:0.11643072106258191\n",
      "train loss:0.2231913754947022\n",
      "train loss:0.239570631704626\n",
      "train loss:0.2593246003296907\n",
      "train loss:0.11271169517397583\n",
      "train loss:0.1377954374864718\n",
      "train loss:0.18672558503585798\n",
      "train loss:0.2548624993689101\n",
      "train loss:0.2489922992367752\n",
      "train loss:0.12545517333167325\n",
      "train loss:0.1790859880018597\n",
      "train loss:0.3194102562881346\n",
      "train loss:0.2421206184742472\n",
      "train loss:0.23984035982295218\n",
      "train loss:0.1247490170766136\n",
      "train loss:0.1799202099233286\n",
      "train loss:0.1591679939388373\n",
      "train loss:0.18062362838184304\n",
      "train loss:0.15769674673022652\n",
      "train loss:0.17878738990829698\n",
      "train loss:0.15033444277323252\n",
      "train loss:0.17460486481615914\n",
      "train loss:0.3736124037636754\n",
      "train loss:0.1950401992457989\n",
      "train loss:0.19296999244836643\n",
      "train loss:0.1364017269685517\n",
      "train loss:0.23128745632941589\n",
      "train loss:0.27203009339194173\n",
      "train loss:0.10893468033494327\n",
      "train loss:0.19342806142132285\n",
      "train loss:0.22173090795540337\n",
      "train loss:0.17628964687493354\n",
      "train loss:0.34778255498236815\n",
      "train loss:0.1913152568427919\n",
      "train loss:0.15563179784973757\n",
      "train loss:0.1651631397555048\n",
      "train loss:0.15058491267945662\n",
      "train loss:0.19298889475849165\n",
      "train loss:0.18535106045414387\n",
      "train loss:0.11704567411937257\n",
      "train loss:0.32398178346129514\n",
      "train loss:0.23291001551671392\n",
      "train loss:0.25920103388459415\n",
      "train loss:0.24880583704864925\n",
      "train loss:0.08604706283175688\n",
      "train loss:0.2144934597994628\n",
      "train loss:0.24549985869305024\n",
      "train loss:0.24799051053039978\n",
      "train loss:0.23870914039275903\n",
      "train loss:0.14697224349067828\n",
      "train loss:0.20962125635608367\n",
      "train loss:0.2872047378330434\n",
      "train loss:0.18478117706879577\n",
      "train loss:0.0788891398060364\n",
      "train loss:0.12873936921671505\n",
      "train loss:0.1736679510644693\n",
      "train loss:0.1339567638867589\n",
      "train loss:0.13752580866745542\n",
      "train loss:0.142747350571513\n",
      "train loss:0.1495509050135017\n",
      "train loss:0.2118039320885257\n",
      "train loss:0.10082236762390115\n",
      "train loss:0.21973788324391044\n",
      "train loss:0.09207549495026832\n",
      "train loss:0.20323084160664592\n",
      "train loss:0.18565481959372027\n",
      "train loss:0.12646465801407059\n",
      "train loss:0.26513174990016347\n",
      "train loss:0.11036417541523147\n",
      "train loss:0.09242839533631551\n",
      "train loss:0.2093900013030543\n",
      "train loss:0.21075009085732585\n",
      "train loss:0.13970302111638677\n",
      "train loss:0.060946456393524716\n",
      "train loss:0.09874765570526894\n",
      "train loss:0.11943931441946795\n",
      "train loss:0.2986213037716151\n",
      "train loss:0.16222055841225436\n",
      "train loss:0.3212772009817593\n",
      "train loss:0.1433168798361467\n",
      "train loss:0.2607112333066816\n",
      "train loss:0.29422138244700335\n",
      "train loss:0.14550692555864672\n",
      "train loss:0.39052756276875095\n",
      "train loss:0.12118632759754205\n",
      "train loss:0.15094467578309748\n",
      "train loss:0.29182011661607976\n",
      "train loss:0.2670245898372361\n",
      "train loss:0.13385389729701602\n",
      "train loss:0.1871318632464453\n",
      "train loss:0.14730822320945705\n",
      "train loss:0.21856098921440037\n",
      "train loss:0.09876675843009464\n",
      "train loss:0.12232296568440205\n",
      "train loss:0.2839139882210182\n",
      "train loss:0.18739586385123913\n",
      "train loss:0.13643187156252604\n",
      "train loss:0.14227453219157737\n",
      "train loss:0.16656205008253075\n",
      "train loss:0.20143393443879568\n",
      "train loss:0.2775995502341733\n",
      "train loss:0.12896766459117667\n",
      "train loss:0.10798108287337316\n",
      "train loss:0.14922161315527\n",
      "train loss:0.18653212820497408\n",
      "train loss:0.2193600920820884\n",
      "train loss:0.23472405148046296\n",
      "train loss:0.18278842216067606\n",
      "train loss:0.07612393294495515\n",
      "train loss:0.18721592275291932\n",
      "train loss:0.1828173796439797\n",
      "train loss:0.2796360722348896\n",
      "train loss:0.10102335764529281\n",
      "train loss:0.2673424687472146\n",
      "train loss:0.2468716166166687\n",
      "train loss:0.1261524012567289\n",
      "train loss:0.23213598085887907\n",
      "train loss:0.1635545421373122\n",
      "train loss:0.20964240859359695\n",
      "train loss:0.246587202811456\n",
      "train loss:0.11388666363710359\n",
      "train loss:0.13417791562727277\n",
      "train loss:0.20628001864186007\n",
      "train loss:0.21589315033308407\n",
      "train loss:0.21851350894590052\n",
      "train loss:0.20471636832501777\n",
      "train loss:0.12217761677911065\n",
      "train loss:0.16134462181525044\n",
      "train loss:0.16238715227595285\n",
      "train loss:0.07365738350932965\n",
      "train loss:0.16731927771447977\n",
      "train loss:0.07389735322037873\n",
      "train loss:0.2040826523992054\n",
      "train loss:0.10195929563117388\n",
      "train loss:0.13176833842215602\n",
      "train loss:0.23331659053405202\n",
      "train loss:0.1540962284784924\n",
      "train loss:0.1566145803869031\n",
      "train loss:0.1808892242576314\n",
      "train loss:0.15343490455033737\n",
      "train loss:0.23212830767947323\n",
      "train loss:0.17088000320170085\n",
      "train loss:0.13038096631737242\n",
      "train loss:0.09484632579156317\n",
      "train loss:0.08521351191459789\n",
      "train loss:0.1747965942523783\n",
      "train loss:0.1079084290558886\n",
      "train loss:0.14817403463966908\n",
      "train loss:0.0618652400967421\n",
      "train loss:0.15441447607119144\n",
      "train loss:0.07131504438099462\n",
      "train loss:0.24409604778608007\n",
      "train loss:0.07620693691759554\n",
      "train loss:0.2279398474645455\n",
      "train loss:0.09368181543462323\n",
      "train loss:0.143745198879929\n",
      "train loss:0.21570646989180647\n",
      "train loss:0.1801846123531545\n",
      "train loss:0.10875067589313157\n",
      "train loss:0.15505333810502836\n",
      "train loss:0.08329459759543673\n",
      "train loss:0.14954245526419396\n",
      "train loss:0.08658562832366376\n",
      "train loss:0.10927953675126932\n",
      "train loss:0.1138568007980835\n",
      "train loss:0.1066292791602237\n",
      "train loss:0.11327870172954098\n",
      "train loss:0.192497739080248\n",
      "train loss:0.055975648903622235\n",
      "train loss:0.057710153779056556\n",
      "train loss:0.2296829710866387\n",
      "train loss:0.13081481631734193\n",
      "train loss:0.12283365689778748\n",
      "train loss:0.17541303342169198\n",
      "train loss:0.15448876761046976\n",
      "train loss:0.12468686881503484\n",
      "train loss:0.17729819849854028\n",
      "train loss:0.14100369245854386\n",
      "train loss:0.1657515627737936\n",
      "train loss:0.10010737157459394\n",
      "train loss:0.1276795752189129\n",
      "train loss:0.15858339071322164\n",
      "train loss:0.1331452763952583\n",
      "train loss:0.20555503847502357\n",
      "train loss:0.08497616916577527\n",
      "train loss:0.1261762967618605\n",
      "train loss:0.07627498281374766\n",
      "train loss:0.15921717063237836\n",
      "train loss:0.16479936284984056\n",
      "train loss:0.18604433030242132\n",
      "train loss:0.13278962258716137\n",
      "train loss:0.1382710294345622\n",
      "train loss:0.11680050272000805\n",
      "train loss:0.12805684161976946\n",
      "train loss:0.11121858162575936\n",
      "train loss:0.1924633074350904\n",
      "train loss:0.18089013186609243\n",
      "train loss:0.16741081040213163\n",
      "train loss:0.09971240943966253\n",
      "train loss:0.1599195776868102\n",
      "train loss:0.17791419655800367\n",
      "train loss:0.10452320291185128\n",
      "train loss:0.13793890361401218\n",
      "train loss:0.14475050800808928\n",
      "train loss:0.054649403155556865\n",
      "train loss:0.0890320547686731\n",
      "train loss:0.1331663125153525\n",
      "train loss:0.06380127710884612\n",
      "train loss:0.08311216855669018\n",
      "train loss:0.12000001795924042\n",
      "train loss:0.15256200593223063\n",
      "train loss:0.1520645312555865\n",
      "train loss:0.08165543910771234\n",
      "train loss:0.11747187454606348\n",
      "train loss:0.1739519259660039\n",
      "train loss:0.0783439724197667\n",
      "train loss:0.07160990183668345\n",
      "train loss:0.12963829661700482\n",
      "train loss:0.12186200312976875\n",
      "train loss:0.14348184645845394\n",
      "train loss:0.11638901734436688\n",
      "train loss:0.21177017946588836\n",
      "train loss:0.1683029627782787\n",
      "train loss:0.08102436952124675\n",
      "train loss:0.15757212758099765\n",
      "train loss:0.12205222714167686\n",
      "train loss:0.058089130288049996\n",
      "train loss:0.15103754292477975\n",
      "train loss:0.09077779906970335\n",
      "train loss:0.21030720771372954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.09573910266044022\n",
      "train loss:0.07710905697406972\n",
      "train loss:0.06566829275539778\n",
      "train loss:0.10419152457196094\n",
      "train loss:0.10407874994569367\n",
      "train loss:0.1351523995034106\n",
      "train loss:0.11253099339785946\n",
      "train loss:0.04293178148075612\n",
      "train loss:0.07829065086837925\n",
      "train loss:0.10425507193470829\n",
      "train loss:0.1306867233910233\n",
      "train loss:0.06274352379030225\n",
      "train loss:0.09618449561689507\n",
      "train loss:0.13850299297643004\n",
      "train loss:0.050596757185201964\n",
      "train loss:0.0802167256588599\n",
      "train loss:0.10346553364864185\n",
      "train loss:0.25072701716516654\n",
      "train loss:0.12486352346802836\n",
      "train loss:0.08099853276637979\n",
      "train loss:0.048215122460969095\n",
      "train loss:0.1072211278544525\n",
      "train loss:0.10320827699505986\n",
      "train loss:0.18094974933692304\n",
      "train loss:0.13324361973206425\n",
      "train loss:0.1352193178489119\n",
      "train loss:0.08669835368039762\n",
      "train loss:0.12769448028496222\n",
      "train loss:0.1744987562668713\n",
      "train loss:0.08175873366444115\n",
      "train loss:0.052641643167760904\n",
      "train loss:0.09867233451256523\n",
      "train loss:0.06734708137112269\n",
      "train loss:0.10199079444741638\n",
      "train loss:0.17472978842858677\n",
      "train loss:0.06643585105419259\n",
      "train loss:0.06199555977921771\n",
      "train loss:0.0706351679123884\n",
      "train loss:0.07805443321264656\n",
      "train loss:0.12096981387160279\n",
      "train loss:0.06569754322674667\n",
      "train loss:0.21106718288297274\n",
      "train loss:0.08376160846084664\n",
      "train loss:0.11177758170092851\n",
      "train loss:0.1431257206979214\n",
      "train loss:0.1741247038765918\n",
      "train loss:0.13475746013698064\n",
      "train loss:0.14416634106045662\n",
      "train loss:0.11529584930904821\n",
      "train loss:0.10467999238100707\n",
      "train loss:0.17236746338012407\n",
      "train loss:0.07252552377653496\n",
      "train loss:0.14830841962267868\n",
      "train loss:0.024010785035875334\n",
      "train loss:0.12062288966032182\n",
      "train loss:0.161170190009841\n",
      "train loss:0.13632132751592171\n",
      "train loss:0.22127324573447152\n",
      "train loss:0.09594662391731895\n",
      "train loss:0.05600511285581908\n",
      "train loss:0.12563113628025255\n",
      "train loss:0.0938509874865087\n",
      "train loss:0.11670674515394634\n",
      "=== epoch:2, train acc:0.963, test acc:0.964 ===\n",
      "train loss:0.09775244098755767\n",
      "train loss:0.07343352688827155\n",
      "train loss:0.12274903174646981\n",
      "train loss:0.04992622048744691\n",
      "train loss:0.1501326659753425\n",
      "train loss:0.07820786424683644\n",
      "train loss:0.11047102871828653\n",
      "train loss:0.08235456556629829\n",
      "train loss:0.052577841672094804\n",
      "train loss:0.14241770742916976\n",
      "train loss:0.051030444487625566\n",
      "train loss:0.16342434095210842\n",
      "train loss:0.11610018203481774\n",
      "train loss:0.20664048930805268\n",
      "train loss:0.08523649502700469\n",
      "train loss:0.14937513148111223\n",
      "train loss:0.08305828121342924\n",
      "train loss:0.08000780179393496\n",
      "train loss:0.1452095415117846\n",
      "train loss:0.10003863575608328\n",
      "train loss:0.14241935975278194\n",
      "train loss:0.05219573762276336\n",
      "train loss:0.2880518728507897\n",
      "train loss:0.08672159018546037\n",
      "train loss:0.040115315881964125\n",
      "train loss:0.06531011270493964\n",
      "train loss:0.17218583596158088\n",
      "train loss:0.09249264940577624\n",
      "train loss:0.08127524050620195\n",
      "train loss:0.09666387567998645\n",
      "train loss:0.10293345997390357\n",
      "train loss:0.11522999266132392\n",
      "train loss:0.125038064764534\n",
      "train loss:0.15392688675625277\n",
      "train loss:0.1370575051280772\n",
      "train loss:0.07671881439477836\n",
      "train loss:0.03208746191710606\n",
      "train loss:0.07015902004450753\n",
      "train loss:0.0884635640032786\n",
      "train loss:0.12679316356123876\n",
      "train loss:0.07505965343544525\n",
      "train loss:0.04977264677310607\n",
      "train loss:0.1472800980692673\n",
      "train loss:0.07051690756188794\n",
      "train loss:0.13769914574529604\n",
      "train loss:0.17717148044687986\n",
      "train loss:0.07166614141393017\n",
      "train loss:0.10246500258738309\n",
      "train loss:0.12521121664283047\n",
      "train loss:0.08328313355010226\n",
      "train loss:0.0811015389720871\n",
      "train loss:0.06378894588299301\n",
      "train loss:0.1449647538782993\n",
      "train loss:0.18531815128496093\n",
      "train loss:0.07506100291491681\n",
      "train loss:0.13641214727261602\n",
      "train loss:0.08832033316617119\n",
      "train loss:0.11120042406715312\n",
      "train loss:0.11167524598675468\n",
      "train loss:0.0938702507491603\n",
      "train loss:0.07123618742251901\n",
      "train loss:0.08321712346876536\n",
      "train loss:0.11156404668492437\n",
      "train loss:0.0691887569936265\n",
      "train loss:0.13500546967269456\n",
      "train loss:0.07360462931806726\n",
      "train loss:0.07580713848099366\n",
      "train loss:0.046656201304802254\n",
      "train loss:0.11143623168550024\n",
      "train loss:0.09391665366702333\n",
      "train loss:0.07472172453378528\n",
      "train loss:0.05229672845157774\n",
      "train loss:0.1413608503862319\n",
      "train loss:0.10739845778022954\n",
      "train loss:0.027883139108825925\n",
      "train loss:0.12139303522235817\n",
      "train loss:0.07934809565152859\n",
      "train loss:0.09519009224441742\n",
      "train loss:0.0771645549563233\n",
      "train loss:0.17751822938836412\n",
      "train loss:0.11143465617494756\n",
      "train loss:0.055493793631684565\n",
      "train loss:0.07284509993682374\n",
      "train loss:0.03796175536467296\n",
      "train loss:0.1324578776665276\n",
      "train loss:0.08147697333774982\n",
      "train loss:0.14573302266060115\n",
      "train loss:0.17032131497098876\n",
      "train loss:0.06917431499587717\n",
      "train loss:0.15840737895095292\n",
      "train loss:0.10788903701257153\n",
      "train loss:0.07680936750031385\n",
      "train loss:0.05395780483871212\n",
      "train loss:0.04362457561787745\n",
      "train loss:0.0640060443728335\n",
      "train loss:0.1140054260214096\n",
      "train loss:0.127470822568041\n",
      "train loss:0.08736689995491422\n",
      "train loss:0.14349084582476623\n",
      "train loss:0.04098795740690165\n",
      "train loss:0.08342405912477524\n",
      "train loss:0.1050777691728471\n",
      "train loss:0.2553904195294364\n",
      "train loss:0.19993061895302555\n",
      "train loss:0.09297923635915711\n",
      "train loss:0.14329617762151528\n",
      "train loss:0.08993618320447644\n",
      "train loss:0.10830451913743065\n",
      "train loss:0.12541698504370877\n",
      "train loss:0.13821623026195903\n",
      "train loss:0.0729495276980876\n",
      "train loss:0.0672158149981079\n",
      "train loss:0.2341658658357567\n",
      "train loss:0.07918846292313417\n",
      "train loss:0.033131663612906176\n",
      "train loss:0.055216927207406986\n",
      "train loss:0.10544342383125148\n",
      "train loss:0.055496580178705866\n",
      "train loss:0.0834878576690163\n",
      "train loss:0.039447328397980326\n",
      "train loss:0.12662975196684104\n",
      "train loss:0.23601326955962512\n",
      "train loss:0.12474509562461518\n",
      "train loss:0.03081916683016774\n",
      "train loss:0.08569164602772056\n",
      "train loss:0.040967722757098333\n",
      "train loss:0.1518982985495673\n",
      "train loss:0.04641922486331453\n",
      "train loss:0.03598857133544443\n",
      "train loss:0.10252927222449237\n",
      "train loss:0.04436551071940898\n",
      "train loss:0.10694201347555678\n",
      "train loss:0.0915554725729664\n",
      "train loss:0.0708884932991886\n",
      "train loss:0.0852792637042939\n",
      "train loss:0.05977705329250362\n",
      "train loss:0.05200083804005109\n",
      "train loss:0.09088901417575385\n",
      "train loss:0.04615526127134958\n",
      "train loss:0.04151606420722678\n",
      "train loss:0.04613902047134327\n",
      "train loss:0.09189160132534502\n",
      "train loss:0.0665891266371781\n",
      "train loss:0.12164189411441924\n",
      "train loss:0.04237386490705152\n",
      "train loss:0.08987450309539981\n",
      "train loss:0.04814695799477912\n",
      "train loss:0.06356095291416677\n",
      "train loss:0.2185073489569658\n",
      "train loss:0.04985034628345566\n",
      "train loss:0.07624394660099724\n",
      "train loss:0.08420889196098746\n",
      "train loss:0.10705571278871578\n",
      "train loss:0.05200486986825109\n",
      "train loss:0.05178122770604146\n",
      "train loss:0.1004084299471851\n",
      "train loss:0.12472459383410398\n",
      "train loss:0.06830380852391181\n",
      "train loss:0.04855157618031605\n",
      "train loss:0.05119965752975312\n",
      "train loss:0.055533283383781704\n",
      "train loss:0.04907542618898465\n",
      "train loss:0.15103215877663687\n",
      "train loss:0.23707025079886568\n",
      "train loss:0.11057029001201556\n",
      "train loss:0.15466182804166395\n",
      "train loss:0.12481687273925504\n",
      "train loss:0.05997060812064574\n",
      "train loss:0.1360629445381058\n",
      "train loss:0.15344502722944944\n",
      "train loss:0.05244656090996193\n",
      "train loss:0.17950133316156425\n",
      "train loss:0.09390537316540971\n",
      "train loss:0.057816999072661635\n",
      "train loss:0.050746384504814585\n",
      "train loss:0.11554097480569368\n",
      "train loss:0.06918287635361985\n",
      "train loss:0.17707558286324535\n",
      "train loss:0.08270153250472081\n",
      "train loss:0.05165278085799775\n",
      "train loss:0.06691683484762827\n",
      "train loss:0.10505547106062943\n",
      "train loss:0.1509987800515688\n",
      "train loss:0.07946044573897577\n",
      "train loss:0.05162784985110744\n",
      "train loss:0.13012168366271237\n",
      "train loss:0.06382229126492088\n",
      "train loss:0.04061425528457228\n",
      "train loss:0.13986045945266648\n",
      "train loss:0.06344130540745757\n",
      "train loss:0.0399598427328494\n",
      "train loss:0.058236172231207854\n",
      "train loss:0.0755796835087651\n",
      "train loss:0.1093205796319381\n",
      "train loss:0.06973080666040996\n",
      "train loss:0.14493889204255472\n",
      "train loss:0.15597778064272572\n",
      "train loss:0.12785384487911608\n",
      "train loss:0.16837696412833375\n",
      "train loss:0.1715658404314892\n",
      "train loss:0.028900922589210926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.06732237749135316\n",
      "train loss:0.05627209903821813\n",
      "train loss:0.13277094998476519\n",
      "train loss:0.059631999329679664\n",
      "train loss:0.051258809813370394\n",
      "train loss:0.08762786094044937\n",
      "train loss:0.04695982233347923\n",
      "train loss:0.12132064787966079\n",
      "train loss:0.10469589452639987\n",
      "train loss:0.11789011129637438\n",
      "train loss:0.04913659965838236\n",
      "train loss:0.05998201286722499\n",
      "train loss:0.05623625924728975\n",
      "train loss:0.07626201247515939\n",
      "train loss:0.040286777668407464\n",
      "train loss:0.039813761231417194\n",
      "train loss:0.08861364593868254\n",
      "train loss:0.06513237978397025\n",
      "train loss:0.06504251254056574\n",
      "train loss:0.14822046306588235\n",
      "train loss:0.17538751232836425\n",
      "train loss:0.08188873809137029\n",
      "train loss:0.1477413699509693\n",
      "train loss:0.03736729559617044\n",
      "train loss:0.20280466092152546\n",
      "train loss:0.0654982948444053\n",
      "train loss:0.1057318913621579\n",
      "train loss:0.06808827830019287\n",
      "train loss:0.0473155752585366\n",
      "train loss:0.0643015569256174\n",
      "train loss:0.03876871471192302\n",
      "train loss:0.13420478502652716\n",
      "train loss:0.06891143073553466\n",
      "train loss:0.06083249893551768\n",
      "train loss:0.12678648698887426\n",
      "train loss:0.04011155107999878\n",
      "train loss:0.08573987413666079\n",
      "train loss:0.0405169456061342\n",
      "train loss:0.059328112782234176\n",
      "train loss:0.025448428547712024\n",
      "train loss:0.08200052165495983\n",
      "train loss:0.04837871977625687\n",
      "train loss:0.07584142978873035\n",
      "train loss:0.0361587456033519\n",
      "train loss:0.15786541688411038\n",
      "train loss:0.16554544952669914\n",
      "train loss:0.05907028545852998\n",
      "train loss:0.07136385724480117\n",
      "train loss:0.07765563764409672\n",
      "train loss:0.035435136366943815\n",
      "train loss:0.061631874150960816\n",
      "train loss:0.10186166425639198\n",
      "train loss:0.06867681961668712\n",
      "train loss:0.047683183828588174\n",
      "train loss:0.04256143847326554\n",
      "train loss:0.07078955840393138\n",
      "train loss:0.04228085680093506\n",
      "train loss:0.0615727037525562\n",
      "train loss:0.03231943694426322\n",
      "train loss:0.018405663747709414\n",
      "train loss:0.04668900776916327\n",
      "train loss:0.05935520662591008\n",
      "train loss:0.11610793113608367\n",
      "train loss:0.050537451965784755\n",
      "train loss:0.08061750616829749\n",
      "train loss:0.06328846004202761\n",
      "train loss:0.08532285201887252\n",
      "train loss:0.039626309062621136\n",
      "train loss:0.07474887575602313\n",
      "train loss:0.09909962649075439\n",
      "train loss:0.044335686972785476\n",
      "train loss:0.04653111580420787\n",
      "train loss:0.04947020871177555\n",
      "train loss:0.11399475924626916\n",
      "train loss:0.0653032291044583\n",
      "train loss:0.07254118018101595\n",
      "train loss:0.03715661711116578\n",
      "train loss:0.07028847547749323\n",
      "train loss:0.12589217078895806\n",
      "train loss:0.08923183824728383\n",
      "train loss:0.027792473603112367\n",
      "train loss:0.06434825761378105\n",
      "train loss:0.07120404950626115\n",
      "train loss:0.1132320265708299\n",
      "train loss:0.03499220904518237\n",
      "train loss:0.07166119141073468\n",
      "train loss:0.12157011476194021\n",
      "train loss:0.03517818897889398\n",
      "train loss:0.09253130779874935\n",
      "train loss:0.07129634699038903\n",
      "train loss:0.10747787279246744\n",
      "train loss:0.08151321427451125\n",
      "train loss:0.12080865086346694\n",
      "train loss:0.13935916173133475\n",
      "train loss:0.085724660806835\n",
      "train loss:0.04247900828649514\n",
      "train loss:0.049074263748842516\n",
      "train loss:0.05040472669091162\n",
      "train loss:0.02404445762543633\n",
      "train loss:0.06462525918934026\n",
      "train loss:0.10225684350979365\n",
      "train loss:0.054760405448434364\n",
      "train loss:0.12615240347449536\n",
      "train loss:0.044575811077058373\n",
      "train loss:0.11448928181050912\n",
      "train loss:0.10083299924347054\n",
      "train loss:0.02183778099971044\n",
      "train loss:0.12930397611862887\n",
      "train loss:0.0342109015772274\n",
      "train loss:0.05866153975014269\n",
      "train loss:0.07025114029113202\n",
      "train loss:0.08075683809662668\n",
      "train loss:0.1115646106899954\n",
      "train loss:0.046999641224035715\n",
      "train loss:0.17432498633599225\n",
      "train loss:0.13078507088461358\n",
      "train loss:0.07001765636657913\n",
      "train loss:0.029040375698205268\n",
      "train loss:0.09661111738088411\n",
      "train loss:0.04917487186685929\n",
      "train loss:0.12973184785859945\n",
      "train loss:0.15791752833129902\n",
      "train loss:0.02362882025662596\n",
      "train loss:0.04943850202275791\n",
      "train loss:0.06325784830383635\n",
      "train loss:0.018843093328842365\n",
      "train loss:0.07476274080036001\n",
      "train loss:0.14151069836608693\n",
      "train loss:0.10002798995856602\n",
      "train loss:0.0558276353172191\n",
      "train loss:0.040391741593224124\n",
      "train loss:0.0441772368820231\n",
      "train loss:0.044077405580517676\n",
      "train loss:0.15302012858029015\n",
      "train loss:0.063886986609868\n",
      "train loss:0.03788577021717536\n",
      "train loss:0.025818800049665414\n",
      "train loss:0.03259096359367527\n",
      "train loss:0.11091399755048031\n",
      "train loss:0.07118862844715271\n",
      "train loss:0.04332143442621795\n",
      "train loss:0.08713704581058819\n",
      "train loss:0.14610923287507438\n",
      "train loss:0.0825407775275731\n",
      "train loss:0.0652260719417202\n",
      "train loss:0.11071835420032596\n",
      "train loss:0.17983456847451287\n",
      "train loss:0.025046013409880462\n",
      "train loss:0.06528951239091157\n",
      "train loss:0.03495527686981926\n",
      "train loss:0.1222879917017951\n",
      "train loss:0.13097514342393546\n",
      "train loss:0.036664865944833584\n",
      "train loss:0.102596262201029\n",
      "train loss:0.05160870767815615\n",
      "train loss:0.1264920646860177\n",
      "train loss:0.028810595093743327\n",
      "train loss:0.047574576218223245\n",
      "train loss:0.034647036525829816\n",
      "train loss:0.049829314966629575\n",
      "train loss:0.046854437428101636\n",
      "train loss:0.06599901972399992\n",
      "train loss:0.04055144345765294\n",
      "train loss:0.08030890237398312\n",
      "train loss:0.10502532969010306\n",
      "train loss:0.09306434690661129\n",
      "train loss:0.11668001858803949\n",
      "train loss:0.07860001290431043\n",
      "train loss:0.04804353186326738\n",
      "train loss:0.03862671282973988\n",
      "train loss:0.07340843568942106\n",
      "train loss:0.0859095331083746\n",
      "train loss:0.11218264707004158\n",
      "train loss:0.03078798300999954\n",
      "train loss:0.10160431703833087\n",
      "train loss:0.07332121243534147\n",
      "train loss:0.06305598527498524\n",
      "train loss:0.051082210500068344\n",
      "train loss:0.03500441057804021\n",
      "train loss:0.07343760626079779\n",
      "train loss:0.08493539623181362\n",
      "train loss:0.06360115826699668\n",
      "train loss:0.05638071892514685\n",
      "train loss:0.05472620353990218\n",
      "train loss:0.0888897833275271\n",
      "train loss:0.060900158403571686\n",
      "train loss:0.09234548975889098\n",
      "train loss:0.028384220819114483\n",
      "train loss:0.09460584399727948\n",
      "train loss:0.1198054971158745\n",
      "train loss:0.11763679043511671\n",
      "train loss:0.04492498021972888\n",
      "train loss:0.12597961356855078\n",
      "train loss:0.03481212217920822\n",
      "train loss:0.07532450466471895\n",
      "train loss:0.09534895470988072\n",
      "train loss:0.10616879530109978\n",
      "train loss:0.07633592852652772\n",
      "train loss:0.11984876456644464\n",
      "train loss:0.08303820264299495\n",
      "train loss:0.07098377684107259\n",
      "train loss:0.09393745446584145\n",
      "train loss:0.059495588866566196\n",
      "train loss:0.1614924389221043\n",
      "train loss:0.10903051739250047\n",
      "train loss:0.2305628078051458\n",
      "train loss:0.10261333642784873\n",
      "train loss:0.13425363674068133\n",
      "train loss:0.06275289753635216\n",
      "train loss:0.06389290580820947\n",
      "train loss:0.03145317393265232\n",
      "train loss:0.09450369333409042\n",
      "train loss:0.1710561666617149\n",
      "train loss:0.11010491347858703\n",
      "train loss:0.05966953946053494\n",
      "train loss:0.031959072840867006\n",
      "train loss:0.06638416859572947\n",
      "train loss:0.07307032817793875\n",
      "train loss:0.06614458737775988\n",
      "train loss:0.059859398282573625\n",
      "train loss:0.09994791132145692\n",
      "train loss:0.13191629688057943\n",
      "train loss:0.08870544410428603\n",
      "train loss:0.03508196687835449\n",
      "train loss:0.07209217948969257\n",
      "train loss:0.07285971355521592\n",
      "train loss:0.06204096717919876\n",
      "train loss:0.15137123657695578\n",
      "train loss:0.07206293328249963\n",
      "train loss:0.026176638595424594\n",
      "train loss:0.057413991384873526\n",
      "train loss:0.06445518943417122\n",
      "train loss:0.07619778485359247\n",
      "train loss:0.03325712836828378\n",
      "train loss:0.09130304220811315\n",
      "train loss:0.1751088704735183\n",
      "train loss:0.0855020572675157\n",
      "train loss:0.06858122890828873\n",
      "train loss:0.08761347809301281\n",
      "train loss:0.06312904803715554\n",
      "train loss:0.14267132890307543\n",
      "train loss:0.052157101167019985\n",
      "train loss:0.08345749927135467\n",
      "train loss:0.05016508609616948\n",
      "train loss:0.08075168335380396\n",
      "train loss:0.140222142354429\n",
      "train loss:0.061811627302810014\n",
      "train loss:0.019696973123492237\n",
      "train loss:0.0389015176456685\n",
      "train loss:0.0323064181882447\n",
      "train loss:0.04632118689571035\n",
      "train loss:0.03295425990526731\n",
      "train loss:0.037738773042976737\n",
      "train loss:0.049116763291533305\n",
      "train loss:0.04708271105722457\n",
      "train loss:0.0661120931596958\n",
      "train loss:0.07935964724830082\n",
      "train loss:0.06615969429540991\n",
      "train loss:0.019858822507093924\n",
      "train loss:0.10000456299785349\n",
      "train loss:0.0655771566298076\n",
      "train loss:0.018559699795707877\n",
      "train loss:0.08823231273946272\n",
      "train loss:0.11147435010849926\n",
      "train loss:0.0197413638526975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.034287145705486924\n",
      "train loss:0.03758502634243471\n",
      "train loss:0.04859641082513006\n",
      "train loss:0.12538735212263646\n",
      "train loss:0.07304361333198532\n",
      "train loss:0.03331761824763707\n",
      "train loss:0.04796085672943792\n",
      "train loss:0.0669164600662477\n",
      "train loss:0.017567281958342885\n",
      "train loss:0.029700157965126994\n",
      "train loss:0.03606854073705594\n",
      "train loss:0.08787208331590929\n",
      "train loss:0.060781190853389976\n",
      "train loss:0.05987886408717143\n",
      "train loss:0.024535727033010105\n",
      "train loss:0.05169223463249456\n",
      "train loss:0.06492502281842144\n",
      "train loss:0.06337981618892158\n",
      "train loss:0.05126414563948934\n",
      "train loss:0.06345307271980954\n",
      "train loss:0.05055719655066536\n",
      "train loss:0.08544690999650342\n",
      "train loss:0.028471678758486857\n",
      "train loss:0.04925899411010527\n",
      "train loss:0.09895539573076073\n",
      "train loss:0.09196936738766523\n",
      "train loss:0.04593974888151938\n",
      "train loss:0.10915141861961597\n",
      "train loss:0.05415469751772748\n",
      "train loss:0.06511155641682108\n",
      "train loss:0.01948158735199941\n",
      "train loss:0.0818115353948724\n",
      "train loss:0.05265388269679216\n",
      "train loss:0.10653261449405518\n",
      "train loss:0.07027906081036017\n",
      "train loss:0.02346699451383708\n",
      "train loss:0.06178892070188044\n",
      "train loss:0.10294397994748877\n",
      "train loss:0.10545028634901708\n",
      "train loss:0.018393570989240618\n",
      "train loss:0.030986700056756786\n",
      "train loss:0.08675817964977024\n",
      "train loss:0.0660418362976242\n",
      "train loss:0.09712191523062468\n",
      "train loss:0.03733139351174892\n",
      "train loss:0.09041999767539378\n",
      "train loss:0.1512215890866231\n",
      "train loss:0.09549775019338598\n",
      "train loss:0.020797483847365603\n",
      "train loss:0.16476740020235503\n",
      "train loss:0.07097998540125466\n",
      "train loss:0.2088947549555393\n",
      "train loss:0.07312029554640455\n",
      "train loss:0.06029261503089522\n",
      "train loss:0.05276815042855634\n",
      "train loss:0.043354774284784084\n",
      "train loss:0.084159658508266\n",
      "train loss:0.039604851115525\n",
      "train loss:0.06902995482542328\n",
      "train loss:0.053387904000057614\n",
      "train loss:0.07453311110485651\n",
      "train loss:0.04980863341710967\n",
      "train loss:0.024776490110788488\n",
      "train loss:0.0527030156742348\n",
      "train loss:0.06898646841759878\n",
      "train loss:0.03836811096255725\n",
      "train loss:0.09882868376033872\n",
      "train loss:0.04747714188457329\n",
      "train loss:0.08236015744962653\n",
      "train loss:0.061740170426022\n",
      "train loss:0.031210867128336407\n",
      "train loss:0.07443168052553738\n",
      "train loss:0.03736384613943096\n",
      "train loss:0.11427612077422893\n",
      "train loss:0.08951211191897279\n",
      "train loss:0.04131817339123644\n",
      "train loss:0.07286840433176564\n",
      "train loss:0.06394353720893457\n",
      "train loss:0.15855649522333434\n",
      "train loss:0.0408509301558779\n",
      "train loss:0.05227666326647355\n",
      "train loss:0.17596153604937922\n",
      "train loss:0.06844046823552173\n",
      "train loss:0.10857091970658\n",
      "train loss:0.046577901884449886\n",
      "train loss:0.07780716725054697\n",
      "train loss:0.04642543467336861\n",
      "train loss:0.04352858920525884\n",
      "train loss:0.03924195653992859\n",
      "train loss:0.025869846193535745\n",
      "train loss:0.07178665228422353\n",
      "train loss:0.08837455183311459\n",
      "train loss:0.09612382245535006\n",
      "train loss:0.08275240656779692\n",
      "train loss:0.06554211135487215\n",
      "train loss:0.03265026352931194\n",
      "train loss:0.08091926223541795\n",
      "train loss:0.010060842924207542\n",
      "train loss:0.13476018925074107\n",
      "train loss:0.127110196431201\n",
      "train loss:0.1926531721177252\n",
      "train loss:0.029147580031874183\n",
      "train loss:0.04621735643013618\n",
      "train loss:0.12446861235239498\n",
      "train loss:0.1660902399189506\n",
      "train loss:0.13730737533573764\n",
      "train loss:0.12080311427411917\n",
      "train loss:0.032404833488394985\n",
      "train loss:0.014336295765760725\n",
      "train loss:0.04243311973498996\n",
      "train loss:0.06541046425849148\n",
      "train loss:0.11553318928870386\n",
      "train loss:0.07182987111259964\n",
      "train loss:0.037715418859980886\n",
      "train loss:0.02828243667400947\n",
      "train loss:0.07108321079193952\n",
      "train loss:0.02266025642747362\n",
      "train loss:0.06791264079928959\n",
      "train loss:0.061545461765313766\n",
      "train loss:0.09498178167236328\n",
      "train loss:0.11495092142528182\n",
      "train loss:0.07919359531954838\n",
      "train loss:0.04544688460907881\n",
      "train loss:0.08884419930759808\n",
      "train loss:0.044274837660642295\n",
      "train loss:0.035410623617539605\n",
      "train loss:0.09297570171457348\n",
      "train loss:0.08364265312661658\n",
      "train loss:0.03755286834461204\n",
      "train loss:0.05155553017495916\n",
      "train loss:0.024804021482128985\n",
      "train loss:0.046328501297610174\n",
      "train loss:0.04150317814388203\n",
      "train loss:0.07486374871290177\n",
      "=== epoch:3, train acc:0.979, test acc:0.979 ===\n",
      "train loss:0.03461743979121944\n",
      "train loss:0.07211871633747037\n",
      "train loss:0.04171529863221889\n",
      "train loss:0.01244249123306945\n",
      "train loss:0.07930539529725919\n",
      "train loss:0.027860529622715468\n",
      "train loss:0.028521029007775037\n",
      "train loss:0.06905366808400497\n",
      "train loss:0.07636580260297005\n",
      "train loss:0.07489589762655602\n",
      "train loss:0.0697847698730315\n",
      "train loss:0.1278606764193665\n",
      "train loss:0.05588525517002301\n",
      "train loss:0.04453412504433039\n",
      "train loss:0.02784927579435672\n",
      "train loss:0.12211652080548217\n",
      "train loss:0.059879962153788456\n",
      "train loss:0.12246229239258023\n",
      "train loss:0.05558376368209633\n",
      "train loss:0.1870145552319309\n",
      "train loss:0.03644866707765581\n",
      "train loss:0.08007059716388419\n",
      "train loss:0.023122506792482823\n",
      "train loss:0.030934680312190002\n",
      "train loss:0.09030257682877081\n",
      "train loss:0.07215489524711056\n",
      "train loss:0.1261522424422224\n",
      "train loss:0.024045184634197893\n",
      "train loss:0.08231097136888087\n",
      "train loss:0.04090600556789213\n",
      "train loss:0.08635029107084352\n",
      "train loss:0.04925394299453855\n",
      "train loss:0.05594956199055483\n",
      "train loss:0.043422135831085465\n",
      "train loss:0.051215011717787313\n",
      "train loss:0.11567658059928912\n",
      "train loss:0.015516664021900606\n",
      "train loss:0.05214299042830534\n",
      "train loss:0.03326017001276042\n",
      "train loss:0.049530331641997216\n",
      "train loss:0.05655192290616983\n",
      "train loss:0.09048872371332534\n",
      "train loss:0.14741087606337244\n",
      "train loss:0.14141484177769284\n",
      "train loss:0.029424052676004838\n",
      "train loss:0.07505526117410782\n",
      "train loss:0.049760862357064804\n",
      "train loss:0.01774498390958061\n",
      "train loss:0.08612692488006887\n",
      "train loss:0.051213511416747765\n",
      "train loss:0.0857808946235302\n",
      "train loss:0.07543905796548228\n",
      "train loss:0.05474763583390272\n",
      "train loss:0.07046387384359214\n",
      "train loss:0.05616463051389717\n",
      "train loss:0.03945955471470926\n",
      "train loss:0.06771045715642128\n",
      "train loss:0.025644751282530333\n",
      "train loss:0.03486609101565678\n",
      "train loss:0.0209162265972465\n",
      "train loss:0.052913051589367946\n",
      "train loss:0.07728244053752874\n",
      "train loss:0.034489352418591135\n",
      "train loss:0.045880186894998766\n",
      "train loss:0.07715430736181596\n",
      "train loss:0.01327382902240748\n",
      "train loss:0.020569097606318202\n",
      "train loss:0.10721519569108671\n",
      "train loss:0.07133176956404243\n",
      "train loss:0.03491176173126242\n",
      "train loss:0.042975111619338265\n",
      "train loss:0.05405693171193524\n",
      "train loss:0.042878019540939546\n",
      "train loss:0.0974019888414793\n",
      "train loss:0.06800716964850842\n",
      "train loss:0.08007754017666006\n",
      "train loss:0.12609127850737156\n",
      "train loss:0.03517307217426019\n",
      "train loss:0.028786663724499794\n",
      "train loss:0.08916838887176015\n",
      "train loss:0.040865847812033965\n",
      "train loss:0.10162095134758292\n",
      "train loss:0.032949586754411946\n",
      "train loss:0.052014401636229016\n",
      "train loss:0.02808721431196541\n",
      "train loss:0.0623616365939687\n",
      "train loss:0.053117887382719185\n",
      "train loss:0.017944553451938235\n",
      "train loss:0.027755597498949753\n",
      "train loss:0.024964526284929053\n",
      "train loss:0.04689393419828202\n",
      "train loss:0.10188623748433823\n",
      "train loss:0.019416812043166847\n",
      "train loss:0.03449242162295183\n",
      "train loss:0.037146505371458416\n",
      "train loss:0.03609164570631495\n",
      "train loss:0.08743760983210092\n",
      "train loss:0.08147245444256646\n",
      "train loss:0.16491981393571972\n",
      "train loss:0.020423637724968084\n",
      "train loss:0.09309990112381936\n",
      "train loss:0.06908272267893797\n",
      "train loss:0.12039388477881141\n",
      "train loss:0.046656098871424564\n",
      "train loss:0.05747404912617105\n",
      "train loss:0.03611015486112725\n",
      "train loss:0.05342042437512351\n",
      "train loss:0.09224522640508029\n",
      "train loss:0.020504613431928526\n",
      "train loss:0.023950986002644007\n",
      "train loss:0.030463308272321122\n",
      "train loss:0.06666135201880997\n",
      "train loss:0.033324719575057095\n",
      "train loss:0.017049157839759736\n",
      "train loss:0.045054973392290615\n",
      "train loss:0.03425587180610614\n",
      "train loss:0.026039151425605568\n",
      "train loss:0.012607213076656356\n",
      "train loss:0.03184187282987141\n",
      "train loss:0.03618321542919651\n",
      "train loss:0.01969763282260076\n",
      "train loss:0.040443680544765694\n",
      "train loss:0.044486880943849394\n",
      "train loss:0.023910979187313205\n",
      "train loss:0.035681689169342244\n",
      "train loss:0.043478839173538605\n",
      "train loss:0.031123075717635923\n",
      "train loss:0.06304337500693176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.024353558877722423\n",
      "train loss:0.04128904432783055\n",
      "train loss:0.02922891295216541\n",
      "train loss:0.07405591338879618\n",
      "train loss:0.08074897811917367\n",
      "train loss:0.07929795856171097\n",
      "train loss:0.03698644822472126\n",
      "train loss:0.0608178290432994\n",
      "train loss:0.14756620922183336\n",
      "train loss:0.046435163681719765\n",
      "train loss:0.05273002650633459\n",
      "train loss:0.02579745568149928\n",
      "train loss:0.018092571692132447\n",
      "train loss:0.046743988577642136\n",
      "train loss:0.04814250641540286\n",
      "train loss:0.03338433082199605\n",
      "train loss:0.07096350598322013\n",
      "train loss:0.019564121508179922\n",
      "train loss:0.07704682678194094\n",
      "train loss:0.02401579222937007\n",
      "train loss:0.02823127996545706\n",
      "train loss:0.008340393815697351\n",
      "train loss:0.03779792253548607\n",
      "train loss:0.14252652965610532\n",
      "train loss:0.02550817742291957\n",
      "train loss:0.039856011709620345\n",
      "train loss:0.03712011619187613\n",
      "train loss:0.009682306659921414\n",
      "train loss:0.04474240802967234\n",
      "train loss:0.07889100615430927\n",
      "train loss:0.02605892418336631\n",
      "train loss:0.09497098157604401\n",
      "train loss:0.029939162144980953\n",
      "train loss:0.02907213645279237\n",
      "train loss:0.044671717171539595\n",
      "train loss:0.017988518056093035\n",
      "train loss:0.054317122558988275\n",
      "train loss:0.024536953854694513\n",
      "train loss:0.015465441920103424\n",
      "train loss:0.0628358091842926\n",
      "train loss:0.02505971046882251\n",
      "train loss:0.07362558869463184\n",
      "train loss:0.04609851049391919\n",
      "train loss:0.07421036872569275\n",
      "train loss:0.014775664439388415\n",
      "train loss:0.0392728175239056\n",
      "train loss:0.029918204777911545\n",
      "train loss:0.03398588560654894\n",
      "train loss:0.036354728362769775\n",
      "train loss:0.013434073874561338\n",
      "train loss:0.033027737466243566\n",
      "train loss:0.08838606359128968\n",
      "train loss:0.04618560141785265\n",
      "train loss:0.08740097243375516\n",
      "train loss:0.018046355722847057\n",
      "train loss:0.023593052826578562\n",
      "train loss:0.10309125988885497\n",
      "train loss:0.08939798963387113\n",
      "train loss:0.07257433123588844\n",
      "train loss:0.04279169541057779\n",
      "train loss:0.02533722102533777\n",
      "train loss:0.036219749766705854\n",
      "train loss:0.01802385034091357\n",
      "train loss:0.03318062207847636\n",
      "train loss:0.11965833474573889\n",
      "train loss:0.08813646610694789\n",
      "train loss:0.016879858369722445\n",
      "train loss:0.039708149525702584\n",
      "train loss:0.020849389977550432\n",
      "train loss:0.03315915957732658\n",
      "train loss:0.028116171297181924\n",
      "train loss:0.015213794354589376\n",
      "train loss:0.042439197851190824\n",
      "train loss:0.1338474613115269\n",
      "train loss:0.07989200448192753\n",
      "train loss:0.03055915994197445\n",
      "train loss:0.11970998796500743\n",
      "train loss:0.04354859551663882\n",
      "train loss:0.0404307038100037\n",
      "train loss:0.014301647700115128\n",
      "train loss:0.03761198720523231\n",
      "train loss:0.03217744107178431\n",
      "train loss:0.02884106048646358\n",
      "train loss:0.030551883691529648\n",
      "train loss:0.06453873256129922\n",
      "train loss:0.026360202176892576\n",
      "train loss:0.03010920827740811\n",
      "train loss:0.19708546933164933\n",
      "train loss:0.04069668564980842\n",
      "train loss:0.1382007321145959\n",
      "train loss:0.09233855130472776\n",
      "train loss:0.040576969844797424\n",
      "train loss:0.06109519857479025\n",
      "train loss:0.034610706973112346\n",
      "train loss:0.05879582149689702\n",
      "train loss:0.0446640776791921\n",
      "train loss:0.0626719608136459\n",
      "train loss:0.015199428169738788\n",
      "train loss:0.04165175591643345\n",
      "train loss:0.02437360157785521\n",
      "train loss:0.054922881935446206\n",
      "train loss:0.041178139229203305\n",
      "train loss:0.030973567307983077\n",
      "train loss:0.018426439158170427\n",
      "train loss:0.016884545300264717\n",
      "train loss:0.0423561976707679\n",
      "train loss:0.04558750669335173\n",
      "train loss:0.024784640898854463\n",
      "train loss:0.031944232775433595\n",
      "train loss:0.10754724827445203\n",
      "train loss:0.03316553505250447\n",
      "train loss:0.010426947620648028\n",
      "train loss:0.009334431300021527\n",
      "train loss:0.06693928973540986\n",
      "train loss:0.023692250763935167\n",
      "train loss:0.030101208230004294\n",
      "train loss:0.1736341902826679\n",
      "train loss:0.0493433162103195\n",
      "train loss:0.04172395792979623\n",
      "train loss:0.009182689780776451\n",
      "train loss:0.027485402043571125\n",
      "train loss:0.07426754225092004\n",
      "train loss:0.020899205023261334\n",
      "train loss:0.03433166744126474\n",
      "train loss:0.004949286407003064\n",
      "train loss:0.03959119210375424\n",
      "train loss:0.09615537054795896\n",
      "train loss:0.031091859862344168\n",
      "train loss:0.0730206774766676\n",
      "train loss:0.04715488791607394\n",
      "train loss:0.014282020926754523\n",
      "train loss:0.0372095024807399\n",
      "train loss:0.04305182692145056\n",
      "train loss:0.008394298604891668\n",
      "train loss:0.027178083601301604\n",
      "train loss:0.0357554124155461\n",
      "train loss:0.01978009510570515\n",
      "train loss:0.005596918508512453\n",
      "train loss:0.10172141343502288\n",
      "train loss:0.02736235063875882\n",
      "train loss:0.05000594939745238\n",
      "train loss:0.01944787000146053\n",
      "train loss:0.04155778067975114\n",
      "train loss:0.04082776531858112\n",
      "train loss:0.05270297106172838\n",
      "train loss:0.07733528748449248\n",
      "train loss:0.01674787049395584\n",
      "train loss:0.008117796572333372\n",
      "train loss:0.027239072751677855\n",
      "train loss:0.040468083012185095\n",
      "train loss:0.0625952564071946\n",
      "train loss:0.09156272295733073\n",
      "train loss:0.034945185694856544\n",
      "train loss:0.0997587231635632\n",
      "train loss:0.06440982936230269\n",
      "train loss:0.021522457737884484\n",
      "train loss:0.03789115307366924\n",
      "train loss:0.029453235578738118\n",
      "train loss:0.034775093944706396\n",
      "train loss:0.05143538906735826\n",
      "train loss:0.03494207486047895\n",
      "train loss:0.04048126729210929\n",
      "train loss:0.02817017984776257\n",
      "train loss:0.03500157066274465\n",
      "train loss:0.08097220440251102\n",
      "train loss:0.03460330994547517\n",
      "train loss:0.09996225605585166\n",
      "train loss:0.01141703207111012\n",
      "train loss:0.02615767759637556\n",
      "train loss:0.0224674829278015\n",
      "train loss:0.015369989081726038\n",
      "train loss:0.050322545157474786\n",
      "train loss:0.09437149917340353\n",
      "train loss:0.047358034670954556\n",
      "train loss:0.042373223871580545\n",
      "train loss:0.016818144221769596\n",
      "train loss:0.05655445661198638\n",
      "train loss:0.027144474359793846\n",
      "train loss:0.1188346283794087\n",
      "train loss:0.014198049544792512\n",
      "train loss:0.02980099652642589\n",
      "train loss:0.012129931462844145\n",
      "train loss:0.008921168542605286\n",
      "train loss:0.015186620547059961\n",
      "train loss:0.11949226280578772\n",
      "train loss:0.03280057877801062\n",
      "train loss:0.011940688007171763\n",
      "train loss:0.02783928441819599\n",
      "train loss:0.04059042405105905\n",
      "train loss:0.02227171047956393\n",
      "train loss:0.03326348670681276\n",
      "train loss:0.05570383231355328\n",
      "train loss:0.050145095116662125\n",
      "train loss:0.04855620241113356\n",
      "train loss:0.08079301603551778\n",
      "train loss:0.014748150599022385\n",
      "train loss:0.02438306973492371\n",
      "train loss:0.04690389865612854\n",
      "train loss:0.019324143103845542\n",
      "train loss:0.031351786672651184\n",
      "train loss:0.040028253289055904\n",
      "train loss:0.10211575782854077\n",
      "train loss:0.020374829156368252\n",
      "train loss:0.022169707499685724\n",
      "train loss:0.03637195224954271\n",
      "train loss:0.11372844486754521\n",
      "train loss:0.024714637471445918\n",
      "train loss:0.12037940908307757\n",
      "train loss:0.036075075735487246\n",
      "train loss:0.0643973392488373\n",
      "train loss:0.03625116666408654\n",
      "train loss:0.1172602666763656\n",
      "train loss:0.00853735787221036\n",
      "train loss:0.04955251040024561\n",
      "train loss:0.011738023368585114\n",
      "train loss:0.027039125849345425\n",
      "train loss:0.015147490195538786\n",
      "train loss:0.07391494392139013\n",
      "train loss:0.08857777439650688\n",
      "train loss:0.012518349926713421\n",
      "train loss:0.037998605595149344\n",
      "train loss:0.09464331671340441\n",
      "train loss:0.0383867862025396\n",
      "train loss:0.025811095982717514\n",
      "train loss:0.05167835717744561\n",
      "train loss:0.04821181821581501\n",
      "train loss:0.018165339396427255\n",
      "train loss:0.024984007866450098\n",
      "train loss:0.037803331058975646\n",
      "train loss:0.10254071944484464\n",
      "train loss:0.056106587302909586\n",
      "train loss:0.06080732353487767\n",
      "train loss:0.05479503690800695\n",
      "train loss:0.06450443624850287\n",
      "train loss:0.039758448276995796\n",
      "train loss:0.07450901640165555\n",
      "train loss:0.03371202296009935\n",
      "train loss:0.041412888886191525\n",
      "train loss:0.042354311348666275\n",
      "train loss:0.04575439641261151\n",
      "train loss:0.08210262734238949\n",
      "train loss:0.08374312319336617\n",
      "train loss:0.025727786127119488\n",
      "train loss:0.024889848566115962\n",
      "train loss:0.05616116915813172\n",
      "train loss:0.031163395132722954\n",
      "train loss:0.02714705554842229\n",
      "train loss:0.06564595376416549\n",
      "train loss:0.02165358961626528\n",
      "train loss:0.060773556271167914\n",
      "train loss:0.018431601340640476\n",
      "train loss:0.012538051919978234\n",
      "train loss:0.05159894132968448\n",
      "train loss:0.03865735436744455\n",
      "train loss:0.01947139697899851\n",
      "train loss:0.01805738005997293\n",
      "train loss:0.14940951368514582\n",
      "train loss:0.03484662401602486\n",
      "train loss:0.0437766100822052\n",
      "train loss:0.019225260862653606\n",
      "train loss:0.05483419584934638\n",
      "train loss:0.126519259386982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.09308621643970345\n",
      "train loss:0.09936121212563749\n",
      "train loss:0.029721339982894546\n",
      "train loss:0.03868870296853221\n",
      "train loss:0.058218958653161984\n",
      "train loss:0.05606861289372891\n",
      "train loss:0.11460352881005414\n",
      "train loss:0.014644830920214028\n",
      "train loss:0.04846942935933295\n",
      "train loss:0.11441837402910827\n",
      "train loss:0.04023812138619987\n",
      "train loss:0.055465369509622046\n",
      "train loss:0.030864422026822495\n",
      "train loss:0.013865401270448831\n",
      "train loss:0.06179970008325809\n",
      "train loss:0.050847280860682645\n",
      "train loss:0.051879148809152884\n",
      "train loss:0.13989119124104477\n",
      "train loss:0.10627583951673017\n",
      "train loss:0.06966281975497791\n",
      "train loss:0.08601772020075125\n",
      "train loss:0.07520056793872408\n",
      "train loss:0.09791491143611691\n",
      "train loss:0.05085235749828759\n",
      "train loss:0.018058804472850867\n",
      "train loss:0.01562514403692034\n",
      "train loss:0.040048946022799646\n",
      "train loss:0.09625754360302069\n",
      "train loss:0.03697308013254377\n",
      "train loss:0.09264967070557678\n",
      "train loss:0.016230540656951162\n",
      "train loss:0.04048544662512718\n",
      "train loss:0.03530445282083056\n",
      "train loss:0.06613042162435077\n",
      "train loss:0.036874843321025995\n",
      "train loss:0.022235022236617295\n",
      "train loss:0.030401843724419845\n",
      "train loss:0.037131663046431646\n",
      "train loss:0.06515245589764648\n",
      "train loss:0.01113631072432159\n",
      "train loss:0.032913341366864655\n",
      "train loss:0.04439487628555153\n",
      "train loss:0.044591767609235215\n",
      "train loss:0.033500782858006284\n",
      "train loss:0.03930284917681226\n",
      "train loss:0.07062215126754509\n",
      "train loss:0.04851185101827569\n",
      "train loss:0.1950425816959564\n",
      "train loss:0.06288164535372279\n",
      "train loss:0.10359322973654683\n",
      "train loss:0.048297316293992434\n",
      "train loss:0.056877073754315666\n",
      "train loss:0.025746938184534704\n",
      "train loss:0.05799531198992759\n",
      "train loss:0.05008101178964539\n",
      "train loss:0.07209652726683526\n",
      "train loss:0.04524507139255789\n",
      "train loss:0.038573579085070676\n",
      "train loss:0.00882686613250623\n",
      "train loss:0.06280915988439312\n",
      "train loss:0.03709942259348432\n",
      "train loss:0.005185186993919937\n",
      "train loss:0.083616156759216\n",
      "train loss:0.09301414298525262\n",
      "train loss:0.10316483833921167\n",
      "train loss:0.02733256456591287\n",
      "train loss:0.0631247827055515\n",
      "train loss:0.06191292024101433\n",
      "train loss:0.10530340019054629\n",
      "train loss:0.023587259094011236\n",
      "train loss:0.06276568458336967\n",
      "train loss:0.05049570602665098\n",
      "train loss:0.007904293574454428\n",
      "train loss:0.01808035034046538\n",
      "train loss:0.06878089266741912\n",
      "train loss:0.02372264629030652\n",
      "train loss:0.014297980409646784\n",
      "train loss:0.10179267764376927\n",
      "train loss:0.03746533505637589\n",
      "train loss:0.008980721617581228\n",
      "train loss:0.04477602729399866\n",
      "train loss:0.06085279117092745\n",
      "train loss:0.012564748088890427\n",
      "train loss:0.031989843424907835\n",
      "train loss:0.026913227286435384\n",
      "train loss:0.015737861458876547\n",
      "train loss:0.018721591382683\n",
      "train loss:0.019279118467598787\n",
      "train loss:0.07830054254501817\n",
      "train loss:0.014785203663880227\n",
      "train loss:0.05847480636558384\n",
      "train loss:0.02720463031006946\n",
      "train loss:0.06247279616288999\n",
      "train loss:0.021886548991057325\n",
      "train loss:0.023645571893770354\n",
      "train loss:0.05951116624396404\n",
      "train loss:0.03902214333572923\n",
      "train loss:0.013280036363138202\n",
      "train loss:0.046978278731447026\n",
      "train loss:0.026713787077030468\n",
      "train loss:0.042035355376673404\n",
      "train loss:0.08620690611740162\n",
      "train loss:0.0840882288388518\n",
      "train loss:0.025551206175111984\n",
      "train loss:0.03904954089681216\n",
      "train loss:0.06029487092670543\n",
      "train loss:0.037118643654816944\n",
      "train loss:0.03609560336757095\n",
      "train loss:0.02298868823365266\n",
      "train loss:0.05954409711314359\n",
      "train loss:0.060038778158271244\n",
      "train loss:0.0709297263354981\n",
      "train loss:0.004336273424537978\n",
      "train loss:0.06678259724674135\n",
      "train loss:0.04030797217702097\n",
      "train loss:0.011885611160028571\n",
      "train loss:0.05341569187677925\n",
      "train loss:0.02868845890599143\n",
      "train loss:0.024612011162057402\n",
      "train loss:0.013307142118202614\n",
      "train loss:0.05344553258622297\n",
      "train loss:0.03805190196717102\n",
      "train loss:0.05913475722799322\n",
      "train loss:0.055600259819263996\n",
      "train loss:0.02791861444651057\n",
      "train loss:0.10144252680605799\n",
      "train loss:0.012938026182433534\n",
      "train loss:0.046002526992614874\n",
      "train loss:0.0343320975313465\n",
      "train loss:0.03435745697474332\n",
      "train loss:0.013885715153961736\n",
      "train loss:0.10333827272195693\n",
      "train loss:0.029646228235886918\n",
      "train loss:0.01757782034825703\n",
      "train loss:0.020567261457182396\n",
      "train loss:0.01607230827854547\n",
      "train loss:0.022458963785316855\n",
      "train loss:0.12142148832744402\n",
      "train loss:0.06157773474580539\n",
      "train loss:0.03406045859578286\n",
      "train loss:0.017015872811411398\n",
      "train loss:0.051548895205544634\n",
      "train loss:0.08270674784525316\n",
      "train loss:0.03627306957113736\n",
      "train loss:0.02185325289296217\n",
      "train loss:0.04242000799223783\n",
      "train loss:0.03338142185904655\n",
      "train loss:0.060875320085155\n",
      "train loss:0.010078570899169147\n",
      "train loss:0.014704581404063362\n",
      "train loss:0.05255834706461169\n",
      "train loss:0.06128879595775715\n",
      "train loss:0.027541463766185412\n",
      "train loss:0.009454094513469747\n",
      "train loss:0.03850557202750267\n",
      "train loss:0.036508622613077926\n",
      "train loss:0.023302063105298268\n",
      "train loss:0.02893292764806876\n",
      "train loss:0.004763774618640798\n",
      "train loss:0.04117734130783862\n",
      "train loss:0.01473427242989433\n",
      "train loss:0.05708040500239573\n",
      "train loss:0.036096330148483645\n",
      "train loss:0.055978298320139404\n",
      "train loss:0.08500406958960151\n",
      "train loss:0.03414820302135236\n",
      "train loss:0.05564684532870102\n",
      "train loss:0.030519645491261304\n",
      "train loss:0.050307092081633246\n",
      "train loss:0.016027977406504402\n",
      "train loss:0.05490741028391602\n",
      "train loss:0.03730426602456228\n",
      "train loss:0.031249585246860348\n",
      "train loss:0.0682837792532212\n",
      "train loss:0.04083812854056913\n",
      "train loss:0.01055435259978835\n",
      "train loss:0.024610540083956026\n",
      "train loss:0.05160529568564523\n",
      "train loss:0.016214187026439363\n",
      "train loss:0.027686276644505407\n",
      "train loss:0.038465454465510224\n",
      "train loss:0.024492090066301396\n",
      "train loss:0.05027775980061819\n",
      "train loss:0.01876798268128685\n",
      "train loss:0.02597043587606761\n",
      "train loss:0.11948802191830049\n",
      "train loss:0.05542898559686868\n",
      "train loss:0.02033471297565705\n",
      "train loss:0.021190658098276808\n",
      "train loss:0.043161782562899986\n",
      "train loss:0.03356841901996109\n",
      "train loss:0.010817008034498794\n",
      "train loss:0.016261661801298982\n",
      "train loss:0.013939386794322897\n",
      "train loss:0.024036984500651375\n",
      "train loss:0.013418837231050047\n",
      "train loss:0.029127783979437875\n",
      "train loss:0.014607112050330703\n",
      "train loss:0.20388282561240917\n",
      "train loss:0.04411557309769786\n",
      "train loss:0.008519045405187763\n",
      "train loss:0.03473067609645728\n",
      "train loss:0.03618669241656792\n",
      "train loss:0.09580003554635859\n",
      "train loss:0.048969058296078796\n",
      "train loss:0.028778505220210974\n",
      "train loss:0.020754074050467544\n",
      "train loss:0.010366439266466148\n",
      "train loss:0.02299190649039165\n",
      "train loss:0.03552564113703685\n",
      "=== epoch:4, train acc:0.984, test acc:0.98 ===\n",
      "train loss:0.006386712852606863\n",
      "train loss:0.09684848593814052\n",
      "train loss:0.036052747454571175\n",
      "train loss:0.014824661014770684\n",
      "train loss:0.03650583997322954\n",
      "train loss:0.04149003963797741\n",
      "train loss:0.046387100555519784\n",
      "train loss:0.010462273074277016\n",
      "train loss:0.028679691439156354\n",
      "train loss:0.007703635946485359\n",
      "train loss:0.019384434603015258\n",
      "train loss:0.03193970287324528\n",
      "train loss:0.03318166045495456\n",
      "train loss:0.021276014369937996\n",
      "train loss:0.02083297274229417\n",
      "train loss:0.030580973158429295\n",
      "train loss:0.0288041970696755\n",
      "train loss:0.012063400847599421\n",
      "train loss:0.01823978037931063\n",
      "train loss:0.022791425989904884\n",
      "train loss:0.013300797475944627\n",
      "train loss:0.024596678451264745\n",
      "train loss:0.05185592138168742\n",
      "train loss:0.006487336110020976\n",
      "train loss:0.15655359205754282\n",
      "train loss:0.07081584748663923\n",
      "train loss:0.05251450931245778\n",
      "train loss:0.01407502395603613\n",
      "train loss:0.04858487132963519\n",
      "train loss:0.027079601329877453\n",
      "train loss:0.022155942943585468\n",
      "train loss:0.03253817978148382\n",
      "train loss:0.03958771438888373\n",
      "train loss:0.015476902180647328\n",
      "train loss:0.07404842022149548\n",
      "train loss:0.02913315354496985\n",
      "train loss:0.023870364113302223\n",
      "train loss:0.023391209269356138\n",
      "train loss:0.01700793353822011\n",
      "train loss:0.011538014402889174\n",
      "train loss:0.06050608455479986\n",
      "train loss:0.04981884929155825\n",
      "train loss:0.08685687686555908\n",
      "train loss:0.03411723013077376\n",
      "train loss:0.03069534759782722\n",
      "train loss:0.12389684179599544\n",
      "train loss:0.08639577909351281\n",
      "train loss:0.06675798755792631\n",
      "train loss:0.07558050520503226\n",
      "train loss:0.037754555040266584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.022151848292098306\n",
      "train loss:0.027625444457339915\n",
      "train loss:0.026401643925172094\n",
      "train loss:0.05284354664951963\n",
      "train loss:0.03718993654240718\n",
      "train loss:0.192788486025613\n",
      "train loss:0.08245827748982322\n",
      "train loss:0.02794724893391684\n",
      "train loss:0.039873645549147185\n",
      "train loss:0.03394635987503526\n",
      "train loss:0.03691415547432859\n",
      "train loss:0.04590034513613429\n",
      "train loss:0.057228963280440824\n",
      "train loss:0.04695369394442352\n",
      "train loss:0.035000369317919715\n",
      "train loss:0.05463633736557022\n",
      "train loss:0.02171423169928581\n",
      "train loss:0.03328069081612476\n",
      "train loss:0.04026670174155088\n",
      "train loss:0.03742870389863264\n",
      "train loss:0.03116690121359333\n",
      "train loss:0.011787084520439585\n",
      "train loss:0.09205207553863891\n",
      "train loss:0.060267469062323616\n",
      "train loss:0.025930659391951584\n",
      "train loss:0.018086652384559178\n",
      "train loss:0.16062729666827413\n",
      "train loss:0.09636250111014645\n",
      "train loss:0.04015912887527681\n",
      "train loss:0.03685853853223965\n",
      "train loss:0.028576725584279754\n",
      "train loss:0.023427772331300223\n",
      "train loss:0.10110457836781822\n",
      "train loss:0.047559901903486364\n",
      "train loss:0.019111566062755797\n",
      "train loss:0.04798378944237838\n",
      "train loss:0.07891897244281795\n",
      "train loss:0.00674571296288067\n",
      "train loss:0.01133451311107239\n",
      "train loss:0.08319463647342686\n",
      "train loss:0.016370266599679616\n",
      "train loss:0.02791279774531569\n",
      "train loss:0.011513938472968568\n",
      "train loss:0.07157309114998121\n",
      "train loss:0.07102670095301934\n",
      "train loss:0.03997549840637258\n",
      "train loss:0.025223131314795304\n",
      "train loss:0.013903699654057841\n",
      "train loss:0.041302745087715434\n",
      "train loss:0.014944707151722192\n",
      "train loss:0.0367969617908438\n",
      "train loss:0.0645310567037139\n",
      "train loss:0.11015879048831716\n",
      "train loss:0.022568047727909647\n",
      "train loss:0.026566276834910137\n",
      "train loss:0.050329696329282754\n",
      "train loss:0.04975391558953104\n",
      "train loss:0.04611376566436751\n",
      "train loss:0.08007828795032677\n",
      "train loss:0.034398489835133106\n",
      "train loss:0.007171660105608957\n",
      "train loss:0.018143473584470528\n",
      "train loss:0.03648524669025651\n",
      "train loss:0.021441134620745574\n",
      "train loss:0.020260127596945322\n",
      "train loss:0.0367155700020822\n",
      "train loss:0.07507543925741741\n",
      "train loss:0.03077428924770338\n",
      "train loss:0.06774123946243062\n",
      "train loss:0.020680390776525656\n",
      "train loss:0.02577100403742278\n",
      "train loss:0.013662031162639478\n",
      "train loss:0.033313956503687625\n",
      "train loss:0.01839326615194616\n",
      "train loss:0.02717745730091565\n",
      "train loss:0.048685033242980576\n",
      "train loss:0.058861468203231786\n",
      "train loss:0.04085861977672523\n",
      "train loss:0.02446451770784637\n",
      "train loss:0.04781510077339928\n",
      "train loss:0.013309104310624573\n",
      "train loss:0.13232688298503364\n",
      "train loss:0.0980444405879204\n",
      "train loss:0.06698496992267301\n",
      "train loss:0.05479248028482177\n",
      "train loss:0.03443516148527942\n",
      "train loss:0.0636177717438907\n",
      "train loss:0.020258614698543873\n",
      "train loss:0.019307044622557144\n",
      "train loss:0.047601928991107705\n",
      "train loss:0.07907494610716179\n",
      "train loss:0.07768154747432446\n",
      "train loss:0.033494180403724545\n",
      "train loss:0.05222650386356641\n",
      "train loss:0.03911030282798455\n",
      "train loss:0.09766690147427162\n",
      "train loss:0.032301813749480525\n",
      "train loss:0.03845067984663523\n",
      "train loss:0.061930966526925306\n",
      "train loss:0.017449306710453555\n",
      "train loss:0.06809756910809664\n",
      "train loss:0.012246256087930557\n",
      "train loss:0.0597597352190037\n",
      "train loss:0.050877125795885936\n",
      "train loss:0.04856438996004186\n",
      "train loss:0.020449741593823224\n",
      "train loss:0.04713116151745655\n",
      "train loss:0.04912646569630615\n",
      "train loss:0.03404176569046615\n",
      "train loss:0.017873790182902482\n",
      "train loss:0.04535231121562945\n",
      "train loss:0.015825077715892234\n",
      "train loss:0.020353110777554178\n",
      "train loss:0.10278801162202203\n",
      "train loss:0.03972292970356047\n",
      "train loss:0.04385790293428769\n",
      "train loss:0.049109493353051176\n",
      "train loss:0.09474598358393543\n",
      "train loss:0.028020331602922197\n",
      "train loss:0.0883631415034189\n",
      "train loss:0.04833954789570798\n",
      "train loss:0.02694693852763115\n",
      "train loss:0.05938473672686924\n",
      "train loss:0.00771038485121126\n",
      "train loss:0.04764466467137506\n",
      "train loss:0.027019226256710906\n",
      "train loss:0.05696046427583128\n",
      "train loss:0.05287068176318987\n",
      "train loss:0.01353678653539367\n",
      "train loss:0.019247744520568003\n",
      "train loss:0.023576065843049367\n",
      "train loss:0.04607668760650054\n",
      "train loss:0.0301172935467985\n",
      "train loss:0.06159917789789038\n",
      "train loss:0.011966844792513207\n",
      "train loss:0.025464376710640294\n",
      "train loss:0.04499987548955171\n",
      "train loss:0.04777557093933722\n",
      "train loss:0.021955302622306477\n",
      "train loss:0.03579213918180425\n",
      "train loss:0.024018443035087506\n",
      "train loss:0.018697454563562216\n",
      "train loss:0.011167771108751392\n",
      "train loss:0.0685409103941958\n",
      "train loss:0.04668143357307792\n",
      "train loss:0.031763677123859274\n",
      "train loss:0.004667476298787845\n",
      "train loss:0.007857052951311833\n",
      "train loss:0.03453819074563316\n",
      "train loss:0.010392032804135993\n",
      "train loss:0.01785319763226366\n",
      "train loss:0.04306180098924613\n",
      "train loss:0.04814513038598882\n",
      "train loss:0.021257202194170738\n",
      "train loss:0.034829336395418846\n",
      "train loss:0.029245780677747305\n",
      "train loss:0.05271531223200396\n",
      "train loss:0.005320895139479238\n",
      "train loss:0.0596091793476113\n",
      "train loss:0.015497585754960422\n",
      "train loss:0.03495154957405526\n",
      "train loss:0.06001807991477058\n",
      "train loss:0.011444237029745875\n",
      "train loss:0.027653068649034038\n",
      "train loss:0.010235421645623997\n",
      "train loss:0.041656054455399535\n",
      "train loss:0.05114145747735874\n",
      "train loss:0.021889380363561473\n",
      "train loss:0.04153629191510986\n",
      "train loss:0.02433310656053648\n",
      "train loss:0.04479609299964728\n",
      "train loss:0.046308436713068335\n",
      "train loss:0.025746140136529285\n",
      "train loss:0.03612718812778056\n",
      "train loss:0.015908297706585136\n",
      "train loss:0.03196819210887814\n",
      "train loss:0.013056415998751065\n",
      "train loss:0.03409067547331529\n",
      "train loss:0.015982666552105073\n",
      "train loss:0.031381795568224874\n",
      "train loss:0.015179995136262479\n",
      "train loss:0.04246895544492408\n",
      "train loss:0.011956667227719516\n",
      "train loss:0.05027106692315516\n",
      "train loss:0.07206508562573355\n",
      "train loss:0.016236877706198055\n",
      "train loss:0.028950171478717705\n",
      "train loss:0.0051737910291274536\n",
      "train loss:0.07034877507087094\n",
      "train loss:0.01141553750434351\n",
      "train loss:0.032210001254696315\n",
      "train loss:0.01027288717148253\n",
      "train loss:0.014441161436309389\n",
      "train loss:0.049447016836580336\n",
      "train loss:0.02803093695521874\n",
      "train loss:0.0539871220674963\n",
      "train loss:0.033170985893430906\n",
      "train loss:0.05957526135659265\n",
      "train loss:0.05108000315166576\n",
      "train loss:0.023322633855954282\n",
      "train loss:0.025467272891089437\n",
      "train loss:0.01779691677339302\n",
      "train loss:0.052677005192146985\n",
      "train loss:0.030743825966328308\n",
      "train loss:0.006445672775775051\n",
      "train loss:0.029236780611078178\n",
      "train loss:0.06297569844991996\n",
      "train loss:0.012577424309041288\n",
      "train loss:0.07738409121557849\n",
      "train loss:0.021539099666466326\n",
      "train loss:0.01675887678688944\n",
      "train loss:0.09458541842381368\n",
      "train loss:0.03848258123549302\n",
      "train loss:0.01129191284697306\n",
      "train loss:0.008567853492780497\n",
      "train loss:0.0908223632339187\n",
      "train loss:0.05601092132010949\n",
      "train loss:0.0219445721241205\n",
      "train loss:0.03102849097758671\n",
      "train loss:0.010549467695241638\n",
      "train loss:0.02805527459757836\n",
      "train loss:0.0328875413976229\n",
      "train loss:0.0479420543871411\n",
      "train loss:0.042636377145562544\n",
      "train loss:0.031596081677461446\n",
      "train loss:0.021888491593023987\n",
      "train loss:0.09042713100553555\n",
      "train loss:0.015243797603501696\n",
      "train loss:0.10449041819253058\n",
      "train loss:0.019039391584233133\n",
      "train loss:0.04212949452218171\n",
      "train loss:0.07402745462929335\n",
      "train loss:0.030831463683058113\n",
      "train loss:0.01128274749947882\n",
      "train loss:0.018007904272091826\n",
      "train loss:0.030711783215275602\n",
      "train loss:0.11339702852908143\n",
      "train loss:0.06535360296790786\n",
      "train loss:0.04044441451931562\n",
      "train loss:0.019993405359186405\n",
      "train loss:0.020754114832440877\n",
      "train loss:0.0355314538711497\n",
      "train loss:0.055843936035641806\n",
      "train loss:0.05415895460713511\n",
      "train loss:0.02347630846744357\n",
      "train loss:0.04875290441795408\n",
      "train loss:0.06138846359267186\n",
      "train loss:0.013214801876811096\n",
      "train loss:0.01956445782613274\n",
      "train loss:0.020788824912658533\n",
      "train loss:0.012124216383738904\n",
      "train loss:0.02025484903493818\n",
      "train loss:0.011751367141599063\n",
      "train loss:0.10789980148841823\n",
      "train loss:0.12142007252094762\n",
      "train loss:0.04132398985964542\n",
      "train loss:0.053434592985308484\n",
      "train loss:0.014384109593368317\n",
      "train loss:0.02186833368094689\n",
      "train loss:0.014982968453968024\n",
      "train loss:0.027597253912185135\n",
      "train loss:0.02432882253935192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.022399504249613823\n",
      "train loss:0.04361841055263471\n",
      "train loss:0.01190712093067165\n",
      "train loss:0.058032463862357254\n",
      "train loss:0.10831179277449753\n",
      "train loss:0.0314688112775826\n",
      "train loss:0.04873284348361372\n",
      "train loss:0.017539936655021033\n",
      "train loss:0.06270625441421772\n",
      "train loss:0.02085329539403951\n",
      "train loss:0.007782598570656583\n",
      "train loss:0.026673939754769897\n",
      "train loss:0.013693458136758471\n",
      "train loss:0.01950683898042574\n",
      "train loss:0.024898926561730123\n",
      "train loss:0.02211471626858527\n",
      "train loss:0.0639392632019772\n",
      "train loss:0.05256606249173832\n",
      "train loss:0.024914736863885664\n",
      "train loss:0.02261591610321982\n",
      "train loss:0.02214399129628346\n",
      "train loss:0.061895162203665394\n",
      "train loss:0.060383920907327686\n",
      "train loss:0.027726137024655297\n",
      "train loss:0.021760173052103916\n",
      "train loss:0.08100943304422425\n",
      "train loss:0.04688925360544168\n",
      "train loss:0.022409055207531604\n",
      "train loss:0.017125852033906538\n",
      "train loss:0.013707834816820398\n",
      "train loss:0.059100151757137856\n",
      "train loss:0.01521025188842439\n",
      "train loss:0.012289937103451951\n",
      "train loss:0.06156186260749614\n",
      "train loss:0.02167511823474152\n",
      "train loss:0.02083007952170033\n",
      "train loss:0.07792551795454722\n",
      "train loss:0.06110202076560485\n",
      "train loss:0.06080259040070346\n",
      "train loss:0.02551475156357097\n",
      "train loss:0.019358938003413997\n",
      "train loss:0.032208374198470835\n",
      "train loss:0.011596218721213955\n",
      "train loss:0.010963407930963289\n",
      "train loss:0.07288589893951142\n",
      "train loss:0.015706293411706665\n",
      "train loss:0.04749255013851043\n",
      "train loss:0.035253194561885685\n",
      "train loss:0.022310901580272106\n",
      "train loss:0.027092758186203186\n",
      "train loss:0.010871368337016822\n",
      "train loss:0.007411042924609714\n",
      "train loss:0.013499778875537105\n",
      "train loss:0.04643129590403603\n",
      "train loss:0.02617209259024438\n",
      "train loss:0.017456648085268575\n",
      "train loss:0.011538592180986804\n",
      "train loss:0.046081112609717687\n",
      "train loss:0.025252112054929672\n",
      "train loss:0.03737606125578395\n",
      "train loss:0.010169868933660321\n",
      "train loss:0.004974786476715287\n",
      "train loss:0.012596069929022258\n",
      "train loss:0.03166432308747682\n",
      "train loss:0.03631007085232282\n",
      "train loss:0.12797643042954637\n",
      "train loss:0.008126615116440587\n",
      "train loss:0.09378711478897558\n",
      "train loss:0.06742877082749237\n",
      "train loss:0.003439031413504632\n",
      "train loss:0.034641496564696166\n",
      "train loss:0.1246414733762999\n",
      "train loss:0.01206730770091124\n",
      "train loss:0.007600437964460503\n",
      "train loss:0.015959070465722012\n",
      "train loss:0.022524774216510824\n",
      "train loss:0.021466118835371254\n",
      "train loss:0.03437070612290875\n",
      "train loss:0.04300435638432245\n",
      "train loss:0.026860588250834906\n",
      "train loss:0.030452121140171103\n",
      "train loss:0.013984437646669772\n",
      "train loss:0.028078525642751324\n",
      "train loss:0.010628059661561225\n",
      "train loss:0.003669362026799297\n",
      "train loss:0.014172789762662546\n",
      "train loss:0.03390352543268797\n",
      "train loss:0.0673147051151857\n",
      "train loss:0.005243442185940934\n",
      "train loss:0.008580272427437436\n",
      "train loss:0.007006574706554069\n",
      "train loss:0.021024706312108465\n",
      "train loss:0.013340440531234348\n",
      "train loss:0.018806836081066824\n",
      "train loss:0.05126417829499111\n",
      "train loss:0.010135441982805991\n",
      "train loss:0.09316899429696036\n",
      "train loss:0.01841636411680506\n",
      "train loss:0.01727074027199398\n",
      "train loss:0.040343429777741385\n",
      "train loss:0.04721112185161487\n",
      "train loss:0.03667453516868317\n",
      "train loss:0.0023758166543446555\n",
      "train loss:0.02797680064570031\n",
      "train loss:0.029674957561083678\n",
      "train loss:0.07406832988999006\n",
      "train loss:0.010258961419534491\n",
      "train loss:0.003924566196045687\n",
      "train loss:0.03444164298617934\n",
      "train loss:0.006809036485879486\n",
      "train loss:0.011490386566382387\n",
      "train loss:0.015433652240407571\n",
      "train loss:0.025304946238968125\n",
      "train loss:0.009891367335465154\n",
      "train loss:0.06304748907435781\n",
      "train loss:0.07574640552491158\n",
      "train loss:0.011205833926747894\n",
      "train loss:0.021722420293792347\n",
      "train loss:0.03253029999495272\n",
      "train loss:0.03730988084210893\n",
      "train loss:0.014019351037949757\n",
      "train loss:0.009221995717453746\n",
      "train loss:0.06291771590074753\n",
      "train loss:0.06018130143442031\n",
      "train loss:0.030876521396593824\n",
      "train loss:0.11083427288621461\n",
      "train loss:0.04501964702630186\n",
      "train loss:0.06012252808195886\n",
      "train loss:0.06079905893971083\n",
      "train loss:0.03433589623591438\n",
      "train loss:0.06472565426299887\n",
      "train loss:0.01957234102595772\n",
      "train loss:0.024428057383341507\n",
      "train loss:0.03557579297162241\n",
      "train loss:0.003683249986476226\n",
      "train loss:0.031828503539416124\n",
      "train loss:0.07886204308233019\n",
      "train loss:0.03540516904984606\n",
      "train loss:0.09852838164634514\n",
      "train loss:0.04601439739041482\n",
      "train loss:0.0297832574285305\n",
      "train loss:0.024594121857075994\n",
      "train loss:0.04950716661815449\n",
      "train loss:0.011369458417191386\n",
      "train loss:0.034080949543650686\n",
      "train loss:0.017097428414716266\n",
      "train loss:0.1174728674012384\n",
      "train loss:0.010564871745425088\n",
      "train loss:0.058812869942117194\n",
      "train loss:0.11302638067305787\n",
      "train loss:0.017712535857037096\n",
      "train loss:0.02678474358457608\n",
      "train loss:0.015886209694453396\n",
      "train loss:0.05521494215203681\n",
      "train loss:0.04339626027207192\n",
      "train loss:0.04141908344956616\n",
      "train loss:0.010964007765402448\n",
      "train loss:0.010872173822069843\n",
      "train loss:0.014808960011391752\n",
      "train loss:0.051524241547564846\n",
      "train loss:0.041484446207997155\n",
      "train loss:0.03904742756423281\n",
      "train loss:0.018886851473282763\n",
      "train loss:0.026270772415311706\n",
      "train loss:0.03505512646384879\n",
      "train loss:0.0250392700085369\n",
      "train loss:0.035978036738197355\n",
      "train loss:0.026822524510384412\n",
      "train loss:0.05557138320604366\n",
      "train loss:0.03999059097040888\n",
      "train loss:0.021233164237111364\n",
      "train loss:0.02693745831824109\n",
      "train loss:0.04522994196073911\n",
      "train loss:0.03060032981588034\n",
      "train loss:0.0076502121183031875\n",
      "train loss:0.016242060021875747\n",
      "train loss:0.05606629932140126\n",
      "train loss:0.016898281469323408\n",
      "train loss:0.03362682532493885\n",
      "train loss:0.025932379053556658\n",
      "train loss:0.026372424139779693\n",
      "train loss:0.03420172605629178\n",
      "train loss:0.008879611650266524\n",
      "train loss:0.012369269676916643\n",
      "train loss:0.024786044681277163\n",
      "train loss:0.04733272031698319\n",
      "train loss:0.03786225674777287\n",
      "train loss:0.005399988874098146\n",
      "train loss:0.07979011253774447\n",
      "train loss:0.028011244544718688\n",
      "train loss:0.01493594337682566\n",
      "train loss:0.028558940710112585\n",
      "train loss:0.02764523712144123\n",
      "train loss:0.042918617850376685\n",
      "train loss:0.02670142488838015\n",
      "train loss:0.03017246754741678\n",
      "train loss:0.026464082242330753\n",
      "train loss:0.06589971582253479\n",
      "train loss:0.0703576144341457\n",
      "train loss:0.03753744992611896\n",
      "train loss:0.005299883961470674\n",
      "train loss:0.00841690388699003\n",
      "train loss:0.03531283237012483\n",
      "train loss:0.06594319886910328\n",
      "train loss:0.021304238117433746\n",
      "train loss:0.09612148454564362\n",
      "train loss:0.06731858153335707\n",
      "train loss:0.07018899720863284\n",
      "train loss:0.014540549815530943\n",
      "train loss:0.052243378588299035\n",
      "train loss:0.008500454489243239\n",
      "train loss:0.016812424722065185\n",
      "train loss:0.04245440979928694\n",
      "train loss:0.04585505384569321\n",
      "train loss:0.07662865986169261\n",
      "train loss:0.009679292050551912\n",
      "train loss:0.02519337008236955\n",
      "train loss:0.006482295720522303\n",
      "train loss:0.07070867890566217\n",
      "train loss:0.013332281035626024\n",
      "train loss:0.0056807331171726365\n",
      "train loss:0.02986768029450969\n",
      "train loss:0.0333747483225381\n",
      "train loss:0.024544216681736522\n",
      "train loss:0.004012010685215487\n",
      "train loss:0.017942720671515498\n",
      "train loss:0.09205450929138419\n",
      "train loss:0.029311024033411178\n",
      "train loss:0.02985383135319698\n",
      "train loss:0.0485275594089024\n",
      "train loss:0.05443205903946789\n",
      "train loss:0.027748340037001946\n",
      "train loss:0.031135889384473224\n",
      "train loss:0.01236933325018766\n",
      "train loss:0.022465250621810418\n",
      "train loss:0.017996547772222513\n",
      "train loss:0.06441023076587776\n",
      "train loss:0.06697970875599889\n",
      "train loss:0.017475123908029033\n",
      "train loss:0.012078948931377507\n",
      "train loss:0.037757028190291135\n",
      "train loss:0.040325916806893736\n",
      "train loss:0.05633761527397256\n",
      "train loss:0.033072422772285344\n",
      "train loss:0.01414673007585388\n",
      "train loss:0.020651835507332822\n",
      "train loss:0.03168202503872954\n",
      "train loss:0.037995321469819154\n",
      "train loss:0.028057850297732823\n",
      "train loss:0.011750534707035742\n",
      "train loss:0.07169094665633499\n",
      "train loss:0.07599378126258456\n",
      "train loss:0.05708218339511019\n",
      "train loss:0.02753740200718404\n",
      "train loss:0.03920140367261297\n",
      "train loss:0.033281510032306054\n",
      "train loss:0.015227862290864485\n",
      "train loss:0.008547667766839727\n",
      "train loss:0.01150112970617535\n",
      "train loss:0.024573970469257148\n",
      "train loss:0.02012409092746001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.012693996306692592\n",
      "train loss:0.014559915371417492\n",
      "train loss:0.03535150810645447\n",
      "train loss:0.00922065903686298\n",
      "train loss:0.044167439236406654\n",
      "train loss:0.03736307401214326\n",
      "train loss:0.0953388051454932\n",
      "train loss:0.04118625631021221\n",
      "train loss:0.05686305384868033\n",
      "train loss:0.0066519768892933425\n",
      "train loss:0.02761651954724071\n",
      "train loss:0.02692743339337898\n",
      "train loss:0.050043839095460896\n",
      "train loss:0.020058194996474404\n",
      "train loss:0.007374567058784518\n",
      "train loss:0.004988284409872812\n",
      "train loss:0.060584314101808284\n",
      "train loss:0.023894049308350027\n",
      "train loss:0.01597166786828802\n",
      "train loss:0.01610948569561786\n",
      "train loss:0.01936912990602219\n",
      "train loss:0.009581165147635465\n",
      "train loss:0.0379582077642645\n",
      "train loss:0.03525647164575066\n",
      "train loss:0.0175775019460312\n",
      "train loss:0.01919703410482139\n",
      "train loss:0.013324316723318508\n",
      "=== epoch:5, train acc:0.983, test acc:0.984 ===\n",
      "train loss:0.026972404733435408\n",
      "train loss:0.05215138525628925\n",
      "train loss:0.01155433001536732\n",
      "train loss:0.004289706225470985\n",
      "train loss:0.1031695931411611\n",
      "train loss:0.020257460022988606\n",
      "train loss:0.06036592423477293\n",
      "train loss:0.04459239461602477\n",
      "train loss:0.010222388089367082\n",
      "train loss:0.003031188993709206\n",
      "train loss:0.02995614864666689\n",
      "train loss:0.027645487574879068\n",
      "train loss:0.022895133650379138\n",
      "train loss:0.05486707296490443\n",
      "train loss:0.02362400139283692\n",
      "train loss:0.008906586730068252\n",
      "train loss:0.048816654545049264\n",
      "train loss:0.01926618840672321\n",
      "train loss:0.004866977931439537\n",
      "train loss:0.019693398599490233\n",
      "train loss:0.013652176122034622\n",
      "train loss:0.0720910787761851\n",
      "train loss:0.02845953510591281\n",
      "train loss:0.029311503052499348\n",
      "train loss:0.02788027337244129\n",
      "train loss:0.021789538409853718\n",
      "train loss:0.026644356186202853\n",
      "train loss:0.037494841988160194\n",
      "train loss:0.02673010213335469\n",
      "train loss:0.01085301805756393\n",
      "train loss:0.00833345103079301\n",
      "train loss:0.007738691571141055\n",
      "train loss:0.03868100080973525\n",
      "train loss:0.002887998925090528\n",
      "train loss:0.035103712779003565\n",
      "train loss:0.021757037940958485\n",
      "train loss:0.01245592682128213\n",
      "train loss:0.0269426484028066\n",
      "train loss:0.02609404482725058\n",
      "train loss:0.04897921865296329\n",
      "train loss:0.043753584119542\n",
      "train loss:0.00820513907234382\n",
      "train loss:0.015944051596745053\n",
      "train loss:0.003887960723706346\n",
      "train loss:0.03615882389702414\n",
      "train loss:0.007544972247047187\n",
      "train loss:0.009467733315648789\n",
      "train loss:0.040995424781793126\n",
      "train loss:0.002720299608526068\n",
      "train loss:0.015784586320991327\n",
      "train loss:0.08461954913346242\n",
      "train loss:0.06282123179009502\n",
      "train loss:0.03158166199203291\n",
      "train loss:0.005478021305614722\n",
      "train loss:0.009633004460028616\n",
      "train loss:0.01934420329955061\n",
      "train loss:0.014576381207650169\n",
      "train loss:0.009397192886750547\n",
      "train loss:0.004864546295634393\n",
      "train loss:0.10827528651667576\n",
      "train loss:0.06073968500694131\n",
      "train loss:0.00886420337387281\n",
      "train loss:0.07154800036196492\n",
      "train loss:0.01517221658770261\n",
      "train loss:0.026765650219106867\n",
      "train loss:0.020663104485628313\n",
      "train loss:0.0035855204751437976\n",
      "train loss:0.048306872691619186\n",
      "train loss:0.018254376792218006\n",
      "train loss:0.010870511374482177\n",
      "train loss:0.052033839689590654\n",
      "train loss:0.02160035615244163\n",
      "train loss:0.010036555033530434\n",
      "train loss:0.06402571003164477\n",
      "train loss:0.026616586953071272\n",
      "train loss:0.009638150071974477\n",
      "train loss:0.03051324690326048\n",
      "train loss:0.015066454054167969\n",
      "train loss:0.01509302153514117\n",
      "train loss:0.031416564175249385\n",
      "train loss:0.09225001704002478\n",
      "train loss:0.00901881136419615\n",
      "train loss:0.01934677139765907\n",
      "train loss:0.013526860094197209\n",
      "train loss:0.04317355458761936\n",
      "train loss:0.02259894775379082\n",
      "train loss:0.051428125323253415\n",
      "train loss:0.002444508422899863\n",
      "train loss:0.00813336658522418\n",
      "train loss:0.01633164219002356\n",
      "train loss:0.018762902452563502\n",
      "train loss:0.025201565706189887\n",
      "train loss:0.04693115673504246\n",
      "train loss:0.00954586748010359\n",
      "train loss:0.009658672944765275\n",
      "train loss:0.06563911493601275\n",
      "train loss:0.037428809741767634\n",
      "train loss:0.04417105435833721\n",
      "train loss:0.023726163011063166\n",
      "train loss:0.03519212376030911\n",
      "train loss:0.0376661135560889\n",
      "train loss:0.012069748748248448\n",
      "train loss:0.0026761381681451196\n",
      "train loss:0.024323027754584822\n",
      "train loss:0.005265925983440032\n",
      "train loss:0.02755002750430259\n",
      "train loss:0.024754053652124353\n",
      "train loss:0.02175050931058136\n",
      "train loss:0.04349254779783341\n",
      "train loss:0.08681676162225653\n",
      "train loss:0.015208659111559357\n",
      "train loss:0.04667812979379254\n",
      "train loss:0.026331725574802237\n",
      "train loss:0.03408297083247559\n",
      "train loss:0.006591284283256974\n",
      "train loss:0.05331827234149725\n",
      "train loss:0.027951553845229894\n",
      "train loss:0.029260191462095114\n",
      "train loss:0.07822876853655869\n",
      "train loss:0.025297834021620308\n",
      "train loss:0.02761250990740877\n",
      "train loss:0.014739826154666007\n",
      "train loss:0.011428549812475392\n",
      "train loss:0.05470457679173944\n",
      "train loss:0.01789236165766241\n",
      "train loss:0.1018534626005243\n",
      "train loss:0.015502310012580418\n",
      "train loss:0.03175827141809648\n",
      "train loss:0.005851883312444641\n",
      "train loss:0.021769916296276556\n",
      "train loss:0.02534682840690039\n",
      "train loss:0.015057442192374218\n",
      "train loss:0.028810200993347482\n",
      "train loss:0.01916152811839355\n",
      "train loss:0.033075580370342555\n",
      "train loss:0.01897860025463827\n",
      "train loss:0.012311647325489068\n",
      "train loss:0.023894761291144605\n",
      "train loss:0.0750806338529427\n",
      "train loss:0.04769163152347139\n",
      "train loss:0.009947500167186854\n",
      "train loss:0.019704086433583266\n",
      "train loss:0.03112575180678237\n",
      "train loss:0.02622636995262773\n",
      "train loss:0.025201416860417273\n",
      "train loss:0.05557220592986274\n",
      "train loss:0.06602398132796324\n",
      "train loss:0.03959651582560052\n",
      "train loss:0.008978783226071218\n",
      "train loss:0.020861146680072953\n",
      "train loss:0.005368574992423852\n",
      "train loss:0.06979586319165475\n",
      "train loss:0.027588783406354622\n",
      "train loss:0.020121252457133493\n",
      "train loss:0.005734948135741589\n",
      "train loss:0.010829063555270281\n",
      "train loss:0.02163769108318233\n",
      "train loss:0.11243510160696538\n",
      "train loss:0.047262804070899465\n",
      "train loss:0.01068959081015667\n",
      "train loss:0.005065432827079072\n",
      "train loss:0.052941467576946726\n",
      "train loss:0.025604573573236592\n",
      "train loss:0.02101719684631833\n",
      "train loss:0.042032618022564645\n",
      "train loss:0.010624564126842833\n",
      "train loss:0.020473771135185538\n",
      "train loss:0.03685682406951465\n",
      "train loss:0.022393412671861134\n",
      "train loss:0.022420520609665102\n",
      "train loss:0.06550214376177538\n",
      "train loss:0.018203871016805807\n",
      "train loss:0.0026941054903012614\n",
      "train loss:0.03549713115042104\n",
      "train loss:0.014189435877957193\n",
      "train loss:0.039901362405022546\n",
      "train loss:0.008396003766285768\n",
      "train loss:0.01849851804444842\n",
      "train loss:0.008844705925010202\n",
      "train loss:0.0119184344229273\n",
      "train loss:0.006558269929988039\n",
      "train loss:0.036459302074417416\n",
      "train loss:0.09498288040260595\n",
      "train loss:0.09257410497845868\n",
      "train loss:0.023043360412723216\n",
      "train loss:0.003613697148519505\n",
      "train loss:0.027101713373824828\n",
      "train loss:0.01689610765832732\n",
      "train loss:0.023344365623898935\n",
      "train loss:0.016792583940138894\n",
      "train loss:0.009297903124957447\n",
      "train loss:0.10053709118626099\n",
      "train loss:0.008526026386193142\n",
      "train loss:0.09395395908801273\n",
      "train loss:0.035672588967430016\n",
      "train loss:0.013946632801196296\n",
      "train loss:0.00901228424071411\n",
      "train loss:0.04018717842390809\n",
      "train loss:0.05565764404012968\n",
      "train loss:0.06232846648607812\n",
      "train loss:0.007065396933069585\n",
      "train loss:0.017749717083860556\n",
      "train loss:0.013797143155330734\n",
      "train loss:0.14403805820312154\n",
      "train loss:0.026893055571930016\n",
      "train loss:0.02264273246147301\n",
      "train loss:0.06145943691584211\n",
      "train loss:0.014973994018369623\n",
      "train loss:0.017376727726058583\n",
      "train loss:0.06997670551138702\n",
      "train loss:0.050598938617095135\n",
      "train loss:0.05561536453948329\n",
      "train loss:0.0312620705425191\n",
      "train loss:0.05886884038281214\n",
      "train loss:0.016312683578725406\n",
      "train loss:0.015340071611003028\n",
      "train loss:0.026934982126506513\n",
      "train loss:0.034435409942715677\n",
      "train loss:0.050287489976898636\n",
      "train loss:0.010065552432154935\n",
      "train loss:0.05741844114724768\n",
      "train loss:0.00907257696015202\n",
      "train loss:0.067347708778534\n",
      "train loss:0.021414714610254838\n",
      "train loss:0.007083359159685823\n",
      "train loss:0.013824874209920332\n",
      "train loss:0.031067179358172693\n",
      "train loss:0.02047510435041896\n",
      "train loss:0.010898641262649335\n",
      "train loss:0.05062300287609919\n",
      "train loss:0.029827127980750042\n",
      "train loss:0.04775904363578618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.01636147492480116\n",
      "train loss:0.031433163588462654\n",
      "train loss:0.022739320551592525\n",
      "train loss:0.07656456216097575\n",
      "train loss:0.01861274260442483\n",
      "train loss:0.011266847303036827\n",
      "train loss:0.013098550187382707\n",
      "train loss:0.06678376142454645\n",
      "train loss:0.030974598845366175\n",
      "train loss:0.036955038559785346\n",
      "train loss:0.023954049044737483\n",
      "train loss:0.00865225107245801\n",
      "train loss:0.0516953429013564\n",
      "train loss:0.00840940787902244\n",
      "train loss:0.014172446285864539\n",
      "train loss:0.008917223965575224\n",
      "train loss:0.0639253304791512\n",
      "train loss:0.03925621956173341\n",
      "train loss:0.040194067335017535\n",
      "train loss:0.0390476947202033\n",
      "train loss:0.0361200641616522\n",
      "train loss:0.01583224249623626\n",
      "train loss:0.012486497385014991\n",
      "train loss:0.004799758795203152\n",
      "train loss:0.05908066413993386\n",
      "train loss:0.01804067525032397\n",
      "train loss:0.010702141807339617\n",
      "train loss:0.01918010059776696\n",
      "train loss:0.019984251479228565\n",
      "train loss:0.01570752356276114\n",
      "train loss:0.04324017856682739\n",
      "train loss:0.04522633530302274\n",
      "train loss:0.04452098064995899\n",
      "train loss:0.04922567596567348\n",
      "train loss:0.05855083329520847\n",
      "train loss:0.026465188362392263\n",
      "train loss:0.015665839730024864\n",
      "train loss:0.03142229868716103\n",
      "train loss:0.03026628173768533\n",
      "train loss:0.007458248266933092\n",
      "train loss:0.03779760395798078\n",
      "train loss:0.017798766627838036\n",
      "train loss:0.03486705895219102\n",
      "train loss:0.05194733922908549\n",
      "train loss:0.012689380943918534\n",
      "train loss:0.016442259293905935\n",
      "train loss:0.01208280247823433\n",
      "train loss:0.032269954536299925\n",
      "train loss:0.016198822286808513\n",
      "train loss:0.01295179966604323\n",
      "train loss:0.01002683868180568\n",
      "train loss:0.010380352591642002\n",
      "train loss:0.07035920062170349\n",
      "train loss:0.042234689862745915\n",
      "train loss:0.028125726581659398\n",
      "train loss:0.02118468412337069\n",
      "train loss:0.011077059166168834\n",
      "train loss:0.014111244300083632\n",
      "train loss:0.007602854497559538\n",
      "train loss:0.020982020229555864\n",
      "train loss:0.011826027475097063\n",
      "train loss:0.01238024986286669\n",
      "train loss:0.03813243333992271\n",
      "train loss:0.01369878948892499\n",
      "train loss:0.007214709872374616\n",
      "train loss:0.0069445270384458635\n",
      "train loss:0.09985722069590067\n",
      "train loss:0.030620064144199887\n",
      "train loss:0.0432426201555192\n",
      "train loss:0.0563675951131877\n",
      "train loss:0.06833957481999872\n",
      "train loss:0.0413729747090076\n",
      "train loss:0.05477181586106238\n",
      "train loss:0.024927651822599216\n",
      "train loss:0.059663239266081546\n",
      "train loss:0.010926724331353492\n",
      "train loss:0.012008685077343295\n",
      "train loss:0.016561883124172512\n",
      "train loss:0.033134210612947405\n",
      "train loss:0.04015312738378151\n",
      "train loss:0.03665050348083008\n",
      "train loss:0.020855751141709657\n",
      "train loss:0.0017751166797920184\n",
      "train loss:0.019027376615036475\n",
      "train loss:0.047722711148770226\n",
      "train loss:0.011467656345987398\n",
      "train loss:0.051167247556843536\n",
      "train loss:0.013852937886521506\n",
      "train loss:0.030114225710428454\n",
      "train loss:0.0081079025284641\n",
      "train loss:0.035873848550959035\n",
      "train loss:0.007732258918284558\n",
      "train loss:0.009168723984578792\n",
      "train loss:0.03503692736404206\n",
      "train loss:0.0058003437111994295\n",
      "train loss:0.013583793536634131\n",
      "train loss:0.05439810284024919\n",
      "train loss:0.019497797668297248\n",
      "train loss:0.0732073060574253\n",
      "train loss:0.07463420289035463\n",
      "train loss:0.03496519082296607\n",
      "train loss:0.020194279039270367\n",
      "train loss:0.0064912333820839295\n",
      "train loss:0.022778595452314185\n",
      "train loss:0.018590368019951863\n",
      "train loss:0.00410447846634616\n",
      "train loss:0.008258821820805031\n",
      "train loss:0.011029475089332228\n",
      "train loss:0.014031153924519823\n",
      "train loss:0.013185095724662527\n",
      "train loss:0.04058133549662864\n",
      "train loss:0.037285527889676554\n",
      "train loss:0.009055705824250408\n",
      "train loss:0.026382977602840137\n",
      "train loss:0.011019042943652102\n",
      "train loss:0.016744696658171097\n",
      "train loss:0.035458593556419955\n",
      "train loss:0.08165344503765475\n",
      "train loss:0.01899020801879081\n",
      "train loss:0.010661352096878553\n",
      "train loss:0.06262879831683338\n",
      "train loss:0.006382907957521138\n",
      "train loss:0.024287934842826354\n",
      "train loss:0.05326995360497093\n",
      "train loss:0.01476812089399774\n",
      "train loss:0.024408056636823387\n",
      "train loss:0.10530861414268461\n",
      "train loss:0.032026112809366264\n",
      "train loss:0.0318888753113558\n",
      "train loss:0.04779337641732072\n",
      "train loss:0.034568493995738796\n",
      "train loss:0.009313770424080308\n",
      "train loss:0.022466898926242814\n",
      "train loss:0.01749155392159904\n",
      "train loss:0.025479612968504414\n",
      "train loss:0.030232586464139618\n",
      "train loss:0.017416520513958797\n",
      "train loss:0.04304378442281024\n",
      "train loss:0.02543608820779326\n",
      "train loss:0.03333265538338226\n",
      "train loss:0.02025271102252675\n",
      "train loss:0.007223615585411791\n",
      "train loss:0.0357012832945857\n",
      "train loss:0.00509548008938111\n",
      "train loss:0.05160819285159237\n",
      "train loss:0.007937888708210675\n",
      "train loss:0.023018105307719635\n",
      "train loss:0.003778221977012858\n",
      "train loss:0.009466957575535527\n",
      "train loss:0.010669594294200951\n",
      "train loss:0.02771323539506215\n",
      "train loss:0.05307057212236257\n",
      "train loss:0.006680674441007625\n",
      "train loss:0.013128373598227036\n",
      "train loss:0.005937946411060002\n",
      "train loss:0.012092832973119375\n",
      "train loss:0.042236743495669284\n",
      "train loss:0.009904746885732469\n",
      "train loss:0.018666999844015736\n",
      "train loss:0.00931529834364475\n",
      "train loss:0.014796603685962594\n",
      "train loss:0.012408227362003268\n",
      "train loss:0.006988638318738455\n",
      "train loss:0.009360715085157269\n",
      "train loss:0.021257648527816794\n",
      "train loss:0.08091387072313591\n",
      "train loss:0.0052769382387749\n",
      "train loss:0.031847795954267795\n",
      "train loss:0.0076201963502214\n",
      "train loss:0.01580001313719019\n",
      "train loss:0.021416672854990187\n",
      "train loss:0.006868046271624386\n",
      "train loss:0.044804351351241445\n",
      "train loss:0.012581237279053527\n",
      "train loss:0.01638655608001463\n",
      "train loss:0.0169446785354824\n",
      "train loss:0.017255474025057095\n",
      "train loss:0.01004392129908181\n",
      "train loss:0.022697067709579334\n",
      "train loss:0.00864116042146275\n",
      "train loss:0.01743321879744334\n",
      "train loss:0.030067947163319907\n",
      "train loss:0.011484704380002077\n",
      "train loss:0.032287487781605145\n",
      "train loss:0.005864609285696853\n",
      "train loss:0.05119837191936793\n",
      "train loss:0.005360870428585881\n",
      "train loss:0.025206005524871178\n",
      "train loss:0.005204026957092668\n",
      "train loss:0.005598695065227298\n",
      "train loss:0.0114458866752642\n",
      "train loss:0.005474794732660375\n",
      "train loss:0.018354252967092446\n",
      "train loss:0.044909185654528005\n",
      "train loss:0.01685275011595217\n",
      "train loss:0.008807265129983165\n",
      "train loss:0.005891079397203312\n",
      "train loss:0.025689517168585384\n",
      "train loss:0.011795502792566588\n",
      "train loss:0.067195555155965\n",
      "train loss:0.06534689786823403\n",
      "train loss:0.022394972952910958\n",
      "train loss:0.010150621070223203\n",
      "train loss:0.012473650574044846\n",
      "train loss:0.006085558201686315\n",
      "train loss:0.002928521829795029\n",
      "train loss:0.013817943641294558\n",
      "train loss:0.005794297389875077\n",
      "train loss:0.005711317117479602\n",
      "train loss:0.0027442407280130424\n",
      "train loss:0.011022753018879488\n",
      "train loss:0.02173344066986079\n",
      "train loss:0.0052383747317275185\n",
      "train loss:0.06337355320686153\n",
      "train loss:0.013848109780245216\n",
      "train loss:0.13416177411196056\n",
      "train loss:0.026914378122117236\n",
      "train loss:0.03541526135963331\n",
      "train loss:0.007576836020406267\n",
      "train loss:0.08508385711615948\n",
      "train loss:0.03703235205722715\n",
      "train loss:0.01448645791679001\n",
      "train loss:0.02771952101113032\n",
      "train loss:0.0035178670549173884\n",
      "train loss:0.022208024167410793\n",
      "train loss:0.029232523060272565\n",
      "train loss:0.025934510483032514\n",
      "train loss:0.030162320141365776\n",
      "train loss:0.0077618846455570715\n",
      "train loss:0.01476599079505548\n",
      "train loss:0.05129540947106606\n",
      "train loss:0.04807282678681406\n",
      "train loss:0.011805229540912676\n",
      "train loss:0.036402619959020825\n",
      "train loss:0.023957821213861107\n",
      "train loss:0.04083828188000748\n",
      "train loss:0.012928953152412739\n",
      "train loss:0.01788395494641349\n",
      "train loss:0.054371634858513665\n",
      "train loss:0.039928770066010345\n",
      "train loss:0.012616926262831502\n",
      "train loss:0.026996789291134\n",
      "train loss:0.013574722151980412\n",
      "train loss:0.021027110179748103\n",
      "train loss:0.01836946083737023\n",
      "train loss:0.08118059502465902\n",
      "train loss:0.07675895869592743\n",
      "train loss:0.016909892363770895\n",
      "train loss:0.050082885227549584\n",
      "train loss:0.01163342327003985\n",
      "train loss:0.007919538251339994\n",
      "train loss:0.03379049460906946\n",
      "train loss:0.03630328910010826\n",
      "train loss:0.015485764085054643\n",
      "train loss:0.011297239849699443\n",
      "train loss:0.0028504481020972115\n",
      "train loss:0.002072389014270812\n",
      "train loss:0.050827821078511\n",
      "train loss:0.029123333127334314\n",
      "train loss:0.007346753208749707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.012225818861607831\n",
      "train loss:0.0077451940529392384\n",
      "train loss:0.00727464486576491\n",
      "train loss:0.011036234075799352\n",
      "train loss:0.0013103885931862029\n",
      "train loss:0.03224031066587707\n",
      "train loss:0.038900817931867726\n",
      "train loss:0.14268645631789456\n",
      "train loss:0.007885997137313104\n",
      "train loss:0.02544723380707408\n",
      "train loss:0.051298937406783526\n",
      "train loss:0.057345724785408496\n",
      "train loss:0.008799987080424811\n",
      "train loss:0.015471674526535799\n",
      "train loss:0.07035263814698578\n",
      "train loss:0.005037881853269355\n",
      "train loss:0.011917569618225486\n",
      "train loss:0.010619174084525864\n",
      "train loss:0.029978335741241778\n",
      "train loss:0.09927877366587647\n",
      "train loss:0.018675629199045037\n",
      "train loss:0.015082277386918688\n",
      "train loss:0.07512461955163668\n",
      "train loss:0.014473841623099871\n",
      "train loss:0.042123500553295\n",
      "train loss:0.04343431168892544\n",
      "train loss:0.01692259677761422\n",
      "train loss:0.01538179745144363\n",
      "train loss:0.03405784939239512\n",
      "train loss:0.002729761137806894\n",
      "train loss:0.01973988826595155\n",
      "train loss:0.016836542764820567\n",
      "train loss:0.045726958522220335\n",
      "train loss:0.013566139724246376\n",
      "train loss:0.011416587540870217\n",
      "train loss:0.013323841274626248\n",
      "train loss:0.03937963099614616\n",
      "train loss:0.016180622167235276\n",
      "train loss:0.005476854233283591\n",
      "train loss:0.008046272739974245\n",
      "train loss:0.02018081956136643\n",
      "train loss:0.01821330105087177\n",
      "train loss:0.007102915063158511\n",
      "train loss:0.022708444958299188\n",
      "train loss:0.006974288972604632\n",
      "train loss:0.04694993479546314\n",
      "train loss:0.015013446754262989\n",
      "train loss:0.0058431592655296604\n",
      "train loss:0.016008142812494762\n",
      "train loss:0.03276486635986245\n",
      "train loss:0.00523926710863758\n",
      "train loss:0.01948962327127711\n",
      "train loss:0.007626900589872495\n",
      "train loss:0.014567496616202949\n",
      "train loss:0.0191056297898927\n",
      "train loss:0.06034303043082351\n",
      "train loss:0.014000275793493827\n",
      "train loss:0.015214972134412464\n",
      "train loss:0.0056127324705433305\n",
      "train loss:0.006979704365103773\n",
      "train loss:0.006893355498027547\n",
      "train loss:0.025296995095819202\n",
      "train loss:0.024891031764290018\n",
      "train loss:0.023050778687221943\n",
      "train loss:0.04359781913444824\n",
      "train loss:0.011010192844429723\n",
      "train loss:0.011864061508918396\n",
      "train loss:0.00911626567546918\n",
      "train loss:0.00643360903910079\n",
      "train loss:0.041092228811520304\n",
      "train loss:0.09339124257710944\n",
      "train loss:0.008196339592060325\n",
      "train loss:0.023033193043975192\n",
      "train loss:0.03748051589551153\n",
      "train loss:0.034885479328255406\n",
      "train loss:0.037591025322714876\n",
      "train loss:0.020765283230935926\n",
      "train loss:0.02584703598324938\n",
      "train loss:0.0036686071509922013\n",
      "train loss:0.02241076915326766\n",
      "train loss:0.012732320299110889\n",
      "train loss:0.017764397726638362\n",
      "train loss:0.022734463002656722\n",
      "train loss:0.010057433424958215\n",
      "train loss:0.0679802600102259\n",
      "train loss:0.044554666516470956\n",
      "train loss:0.013881389095555948\n",
      "train loss:0.010025412662029422\n",
      "train loss:0.06754087037373738\n",
      "train loss:0.009239711777404725\n",
      "train loss:0.01999326208651546\n",
      "train loss:0.006596168081959094\n",
      "train loss:0.033481390279466894\n",
      "train loss:0.003152080891928298\n",
      "train loss:0.02252631692267831\n",
      "train loss:0.03154241501543937\n",
      "train loss:0.013762124844814809\n",
      "train loss:0.005326029583862216\n",
      "train loss:0.0023498269242835704\n",
      "train loss:0.05811055908116931\n",
      "train loss:0.010168621706187719\n",
      "train loss:0.0033086272366593915\n",
      "train loss:0.016430332324804515\n",
      "train loss:0.0028704818783654574\n",
      "train loss:0.004650655991807674\n",
      "train loss:0.022545404367590404\n",
      "train loss:0.05093549185151702\n",
      "train loss:0.011014936210917087\n",
      "=== epoch:6, train acc:0.988, test acc:0.985 ===\n",
      "train loss:0.011429192190755149\n",
      "train loss:0.016115816188484792\n",
      "train loss:0.002929018127135667\n",
      "train loss:0.011591525465396036\n",
      "train loss:0.025928319750975252\n",
      "train loss:0.03607663554291825\n",
      "train loss:0.019428006260363374\n",
      "train loss:0.026036975437428948\n",
      "train loss:0.10090341630328142\n",
      "train loss:0.0018473300085891779\n",
      "train loss:0.02447495523963225\n",
      "train loss:0.0039763789218785804\n",
      "train loss:0.002290958308847296\n",
      "train loss:0.022549805318129574\n",
      "train loss:0.06663912903098397\n",
      "train loss:0.01831539776277137\n",
      "train loss:0.04350905706611188\n",
      "train loss:0.019793391777897247\n",
      "train loss:0.014210107486344095\n",
      "train loss:0.007189758655200013\n",
      "train loss:0.003358538827249555\n",
      "train loss:0.010383435174077487\n",
      "train loss:0.007861023981317213\n",
      "train loss:0.04666067955161228\n",
      "train loss:0.006389575841759794\n",
      "train loss:0.043600461446545176\n",
      "train loss:0.045277590962007154\n",
      "train loss:0.02112083123332875\n",
      "train loss:0.0656136046274898\n",
      "train loss:0.009475752482729109\n",
      "train loss:0.02879028146333429\n",
      "train loss:0.05436508714558472\n",
      "train loss:0.07399776828039072\n",
      "train loss:0.008567678276175786\n",
      "train loss:0.023234773393082178\n",
      "train loss:0.02408234843258077\n",
      "train loss:0.004849167186016809\n",
      "train loss:0.021906251897353162\n",
      "train loss:0.03511013691774111\n",
      "train loss:0.009919477009459452\n",
      "train loss:0.011519250190293233\n",
      "train loss:0.011687777505746193\n",
      "train loss:0.008377062804960825\n",
      "train loss:0.03841902828876115\n",
      "train loss:0.009716773249649662\n",
      "train loss:0.00196519867427115\n",
      "train loss:0.0034067191527713393\n",
      "train loss:0.0748481660074516\n",
      "train loss:0.03084919818133054\n",
      "train loss:0.02860091691182761\n",
      "train loss:0.0010053581636473847\n",
      "train loss:0.006196443775726923\n",
      "train loss:0.021791611435671654\n",
      "train loss:0.022965256759371878\n",
      "train loss:0.02786789012156433\n",
      "train loss:0.02529288263636735\n",
      "train loss:0.05095590454626372\n",
      "train loss:0.014382427746492963\n",
      "train loss:0.007210896062675903\n",
      "train loss:0.01762437371294989\n",
      "train loss:0.0020770918657153623\n",
      "train loss:0.08994151006099943\n",
      "train loss:0.019788257815591872\n",
      "train loss:0.01003901435889028\n",
      "train loss:0.03413771023174318\n",
      "train loss:0.015266092925604213\n",
      "train loss:0.0119589536535101\n",
      "train loss:0.008037912948670014\n",
      "train loss:0.010381633043708456\n",
      "train loss:0.009797436389067755\n",
      "train loss:0.00707834119227751\n",
      "train loss:0.005303437840696169\n",
      "train loss:0.00959260642999723\n",
      "train loss:0.0048631364148252094\n",
      "train loss:0.05014098980975395\n",
      "train loss:0.00887466667191233\n",
      "train loss:0.007984029992080859\n",
      "train loss:0.06002934193215287\n",
      "train loss:0.012722917723820617\n",
      "train loss:0.00929447390867715\n",
      "train loss:0.06831153390838993\n",
      "train loss:0.009888829240578566\n",
      "train loss:0.01342288057975036\n",
      "train loss:0.05392538577805357\n",
      "train loss:0.008778272335780778\n",
      "train loss:0.0171652030355104\n",
      "train loss:0.04984856497030652\n",
      "train loss:0.045746173488421166\n",
      "train loss:0.010289562061700473\n",
      "train loss:0.010917213868244551\n",
      "train loss:0.008031468924745043\n",
      "train loss:0.10314561047813983\n",
      "train loss:0.004007666664939364\n",
      "train loss:0.05772400371565246\n",
      "train loss:0.01255325583006643\n",
      "train loss:0.03155508975948081\n",
      "train loss:0.006037483335121838\n",
      "train loss:0.008639501268257008\n",
      "train loss:0.05264200212604308\n",
      "train loss:0.0189569286757177\n",
      "train loss:0.031990043865517785\n",
      "train loss:0.00674552664769653\n",
      "train loss:0.027181792137103276\n",
      "train loss:0.00507877871747488\n",
      "train loss:0.0067011238005953025\n",
      "train loss:0.026754122854920875\n",
      "train loss:0.01974330668142908\n",
      "train loss:0.00409412689580454\n",
      "train loss:0.01110085843081766\n",
      "train loss:0.02578714160749743\n",
      "train loss:0.01084575336027192\n",
      "train loss:0.007754613226377286\n",
      "train loss:0.006185526266279206\n",
      "train loss:0.018511049113873575\n",
      "train loss:0.03326667664340809\n",
      "train loss:0.03149885038208016\n",
      "train loss:0.01922385642463853\n",
      "train loss:0.05241094387825978\n",
      "train loss:0.03180199823622487\n",
      "train loss:0.014485969351321324\n",
      "train loss:0.003574780323803222\n",
      "train loss:0.04633554789461309\n",
      "train loss:0.013526104029426347\n",
      "train loss:0.013426960327094038\n",
      "train loss:0.027773451390930286\n",
      "train loss:0.1421893544568762\n",
      "train loss:0.015994668265121234\n",
      "train loss:0.018063629510048713\n",
      "train loss:0.0313294921850694\n",
      "train loss:0.024430167832451742\n",
      "train loss:0.024644773490877063\n",
      "train loss:0.030220671870998736\n",
      "train loss:0.02700938611379323\n",
      "train loss:0.015276375243657953\n",
      "train loss:0.012755427734709912\n",
      "train loss:0.01655033883306228\n",
      "train loss:0.04889474363173487\n",
      "train loss:0.030977769197702166\n",
      "train loss:0.02069534240634284\n",
      "train loss:0.005452445943909424\n",
      "train loss:0.026900374289907866\n",
      "train loss:0.007555510241557566\n",
      "train loss:0.03692830579519278\n",
      "train loss:0.007889226151205863\n",
      "train loss:0.010721942420302256\n",
      "train loss:0.010024941740297949\n",
      "train loss:0.09634371844771847\n",
      "train loss:0.027196155053962424\n",
      "train loss:0.023091715998444444\n",
      "train loss:0.030489266959754017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00343265336038214\n",
      "train loss:0.003911594179470532\n",
      "train loss:0.007326144670009504\n",
      "train loss:0.01838784099599672\n",
      "train loss:0.008869829888806495\n",
      "train loss:0.05479405651075143\n",
      "train loss:0.018394218553962872\n",
      "train loss:0.016181114818262017\n",
      "train loss:0.006435970773732752\n",
      "train loss:0.0311661655155688\n",
      "train loss:0.02837892048079195\n",
      "train loss:0.06719644983777534\n",
      "train loss:0.014023375818843144\n",
      "train loss:0.005804258932009289\n",
      "train loss:0.01665387491856607\n",
      "train loss:0.029405224690701025\n",
      "train loss:0.04798767901246616\n",
      "train loss:0.022554355928094662\n",
      "train loss:0.0018568143727809817\n",
      "train loss:0.01819252888916825\n",
      "train loss:0.053461323587217266\n",
      "train loss:0.008104836016083953\n",
      "train loss:0.011408889531064033\n",
      "train loss:0.015253042523126155\n",
      "train loss:0.025776446136151772\n",
      "train loss:0.009368345875499695\n",
      "train loss:0.015517313714124592\n",
      "train loss:0.006417919217618846\n",
      "train loss:0.0029881186199568425\n",
      "train loss:0.025250160218120415\n",
      "train loss:0.018344675353669057\n",
      "train loss:0.010171181593309962\n",
      "train loss:0.0311862772967462\n",
      "train loss:0.03761532973629872\n",
      "train loss:0.012770763699053696\n",
      "train loss:0.014458653452123183\n",
      "train loss:0.016472669951800335\n",
      "train loss:0.011468814801230118\n",
      "train loss:0.02154276073202474\n",
      "train loss:0.006385555172838415\n",
      "train loss:0.07742506796768446\n",
      "train loss:0.0071614751745531155\n",
      "train loss:0.09127300855349738\n",
      "train loss:0.007780188255536727\n",
      "train loss:0.02116313604871634\n",
      "train loss:0.01609834412628888\n",
      "train loss:0.019212932847501595\n",
      "train loss:0.023748449717042788\n",
      "train loss:0.013843916596471741\n",
      "train loss:0.015651796768133573\n",
      "train loss:0.030242499926187442\n",
      "train loss:0.009897063768152246\n",
      "train loss:0.004444936669362041\n",
      "train loss:0.056298412662100554\n",
      "train loss:0.027627963389336\n",
      "train loss:0.03467697855718644\n",
      "train loss:0.017950782989242887\n",
      "train loss:0.010041288462495822\n",
      "train loss:0.01684467084869936\n",
      "train loss:0.007887818256033175\n",
      "train loss:0.007313092886256974\n",
      "train loss:0.026371817872685176\n",
      "train loss:0.01823683185361542\n",
      "train loss:0.007665338004431268\n",
      "train loss:0.028414457407714533\n",
      "train loss:0.05111154137685925\n",
      "train loss:0.0060671605343853045\n",
      "train loss:0.008976060824410641\n",
      "train loss:0.017346261703334098\n",
      "train loss:0.01026760441449543\n",
      "train loss:0.03592791428636976\n",
      "train loss:0.020325680267105337\n",
      "train loss:0.0027661740235613045\n",
      "train loss:0.024715948812531967\n",
      "train loss:0.061834913594402706\n",
      "train loss:0.07205486635292352\n",
      "train loss:0.15012120459874784\n",
      "train loss:0.008145595463303318\n",
      "train loss:0.014649212370219917\n",
      "train loss:0.004733706351835691\n",
      "train loss:0.004970265513101339\n",
      "train loss:0.01929471260522324\n",
      "train loss:0.03862515174933832\n",
      "train loss:0.013093259301971472\n",
      "train loss:0.011378672380316295\n",
      "train loss:0.04840712434419594\n",
      "train loss:0.10468052462372379\n",
      "train loss:0.03461649703216584\n",
      "train loss:0.019548840335975218\n",
      "train loss:0.04937993796392972\n",
      "train loss:0.027119114288737364\n",
      "train loss:0.028496459640904737\n",
      "train loss:0.1083361547709972\n",
      "train loss:0.0029764418240934786\n",
      "train loss:0.018227965476120978\n",
      "train loss:0.03537684884844944\n",
      "train loss:0.007172745549232541\n",
      "train loss:0.020224660331850202\n",
      "train loss:0.024735786572691473\n",
      "train loss:0.017681029216949383\n",
      "train loss:0.06552545133121551\n",
      "train loss:0.011140973705798316\n",
      "train loss:0.011662076362285225\n",
      "train loss:0.011433681152402125\n",
      "train loss:0.027077465177392478\n",
      "train loss:0.012186254111118114\n",
      "train loss:0.02221196848179149\n",
      "train loss:0.016805037631813083\n",
      "train loss:0.007747519452258801\n",
      "train loss:0.03374538028858738\n",
      "train loss:0.001994379552248202\n",
      "train loss:0.00980810244494655\n",
      "train loss:0.05483671709346929\n",
      "train loss:0.12484604127724987\n",
      "train loss:0.07207591427706003\n",
      "train loss:0.005978410702683846\n",
      "train loss:0.01895829162012297\n",
      "train loss:0.013023247472571805\n",
      "train loss:0.046644228219074275\n",
      "train loss:0.06493900719263707\n",
      "train loss:0.08288726266325443\n",
      "train loss:0.013987845567221437\n",
      "train loss:0.010124117649374145\n",
      "train loss:0.006773972923345029\n",
      "train loss:0.02336686982943326\n",
      "train loss:0.013348860264464975\n",
      "train loss:0.014407932634107466\n",
      "train loss:0.018991616043027234\n",
      "train loss:0.03211441729244326\n",
      "train loss:0.0043317597403630835\n",
      "train loss:0.02943479195817601\n",
      "train loss:0.08954112657998618\n",
      "train loss:0.012200442358227006\n",
      "train loss:0.058516894129617684\n",
      "train loss:0.011446985378649886\n",
      "train loss:0.03597374307527994\n",
      "train loss:0.03469215759789719\n",
      "train loss:0.01692844383948942\n",
      "train loss:0.009131662245400116\n",
      "train loss:0.005890252360027156\n",
      "train loss:0.013671367092603439\n",
      "train loss:0.009175410944817532\n",
      "train loss:0.015413639803572387\n",
      "train loss:0.020110209078442076\n",
      "train loss:0.02455678297243048\n",
      "train loss:0.014747877501590765\n",
      "train loss:0.04454259990651439\n",
      "train loss:0.014412232866410593\n",
      "train loss:0.014463027725050613\n",
      "train loss:0.015722013532404246\n",
      "train loss:0.013670629578889664\n",
      "train loss:0.0366779721027849\n",
      "train loss:0.05694726335847873\n",
      "train loss:0.03413786607619141\n",
      "train loss:0.016396497644790013\n",
      "train loss:0.027515517263846302\n",
      "train loss:0.027603595813410396\n",
      "train loss:0.027325080478730744\n",
      "train loss:0.007552286555577197\n",
      "train loss:0.011750777444955092\n",
      "train loss:0.01832074683744789\n",
      "train loss:0.019654801420486468\n",
      "train loss:0.003123213284065667\n",
      "train loss:0.01659893572018259\n",
      "train loss:0.02788156464616985\n",
      "train loss:0.019615985483919435\n",
      "train loss:0.08257442166674688\n",
      "train loss:0.0347376268620305\n",
      "train loss:0.03672646012666537\n",
      "train loss:0.04322862407824697\n",
      "train loss:0.0060732179654778866\n",
      "train loss:0.05644607785742388\n",
      "train loss:0.021328498295473078\n",
      "train loss:0.005292440091701431\n",
      "train loss:0.013160015217439251\n",
      "train loss:0.01292593663547827\n",
      "train loss:0.0038083621818171985\n",
      "train loss:0.010202883782146108\n",
      "train loss:0.036956967782990346\n",
      "train loss:0.023368308705944377\n",
      "train loss:0.016975314251692856\n",
      "train loss:0.014832229041567281\n",
      "train loss:0.012197433460637319\n",
      "train loss:0.004407382498311675\n",
      "train loss:0.0335350606001512\n",
      "train loss:0.016434601194301294\n",
      "train loss:0.028045593076082165\n",
      "train loss:0.014811513705267\n",
      "train loss:0.020842810144379022\n",
      "train loss:0.01502010660586517\n",
      "train loss:0.013411630523317302\n",
      "train loss:0.008236871445445427\n",
      "train loss:0.0048486450295723476\n",
      "train loss:0.004533572121499429\n",
      "train loss:0.022650882972727115\n",
      "train loss:0.017656000136819047\n",
      "train loss:0.022389176180783017\n",
      "train loss:0.017189694116802587\n",
      "train loss:0.06532691827897452\n",
      "train loss:0.0034447283703999774\n",
      "train loss:0.019826733215499052\n",
      "train loss:0.007155855979322921\n",
      "train loss:0.00560533110350734\n",
      "train loss:0.025527666603678376\n",
      "train loss:0.08421479274931828\n",
      "train loss:0.016186613440471485\n",
      "train loss:0.02393828933189255\n",
      "train loss:0.01979303044773936\n",
      "train loss:0.012250572387601601\n",
      "train loss:0.01615987366548949\n",
      "train loss:0.014621042005334824\n",
      "train loss:0.032128007143411595\n",
      "train loss:0.023676711355414028\n",
      "train loss:0.010599243977356248\n",
      "train loss:0.020332341870108513\n",
      "train loss:0.008510240972990935\n",
      "train loss:0.016963285704165776\n",
      "train loss:0.00778851246019362\n",
      "train loss:0.0051313562499334395\n",
      "train loss:0.04353670388309551\n",
      "train loss:0.009778565147592951\n",
      "train loss:0.012074267381595519\n",
      "train loss:0.002729034991540989\n",
      "train loss:0.008769433701862025\n",
      "train loss:0.03700933620326921\n",
      "train loss:0.009177570179504426\n",
      "train loss:0.02526959385976096\n",
      "train loss:0.009682627006781443\n",
      "train loss:0.018959430963302836\n",
      "train loss:0.018217458353894198\n",
      "train loss:0.018713582190667058\n",
      "train loss:0.008964212070611436\n",
      "train loss:0.04062777255120124\n",
      "train loss:0.006302542972110611\n",
      "train loss:0.0118189714892049\n",
      "train loss:0.006859836402013505\n",
      "train loss:0.003508497941836574\n",
      "train loss:0.01812946617297762\n",
      "train loss:0.016349670564227137\n",
      "train loss:0.06721636282100639\n",
      "train loss:0.004452941198284094\n",
      "train loss:0.07584399718266681\n",
      "train loss:0.008643765065340933\n",
      "train loss:0.0076624247928945965\n",
      "train loss:0.009372319338907026\n",
      "train loss:0.024014841502952092\n",
      "train loss:0.020108956307774296\n",
      "train loss:0.039203941765231516\n",
      "train loss:0.10927546489381705\n",
      "train loss:0.023022437613496934\n",
      "train loss:0.0065845849342575265\n",
      "train loss:0.012701940239966378\n",
      "train loss:0.010919009743169955\n",
      "train loss:0.019677788758029915\n",
      "train loss:0.05261838419712183\n",
      "train loss:0.01242635045108236\n",
      "train loss:0.00691963009999277\n",
      "train loss:0.022531688980510042\n",
      "train loss:0.045710592090939965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.02539276551085317\n",
      "train loss:0.007159241711185805\n",
      "train loss:0.01446386333170536\n",
      "train loss:0.05682604733307274\n",
      "train loss:0.025891490617514803\n",
      "train loss:0.07827999978145628\n",
      "train loss:0.00759874599403478\n",
      "train loss:0.005122441710454894\n",
      "train loss:0.0458795969123959\n",
      "train loss:0.030078502094478092\n",
      "train loss:0.028356017225615697\n",
      "train loss:0.01033484801163307\n",
      "train loss:0.0066563535777328245\n",
      "train loss:0.019780457408751072\n",
      "train loss:0.02322872025829773\n",
      "train loss:0.018710585887553116\n",
      "train loss:0.023999241058940707\n",
      "train loss:0.009728949799579644\n",
      "train loss:0.018318731826734644\n",
      "train loss:0.08293775325936148\n",
      "train loss:0.0391143280694301\n",
      "train loss:0.009950491187846987\n",
      "train loss:0.07112075385203313\n",
      "train loss:0.029816689380764923\n",
      "train loss:0.012439473333767275\n",
      "train loss:0.018731342928543525\n",
      "train loss:0.03260407381129191\n",
      "train loss:0.023908954421671503\n",
      "train loss:0.022372476527510145\n",
      "train loss:0.055857088725454467\n",
      "train loss:0.010106613055355412\n",
      "train loss:0.05080038360493034\n",
      "train loss:0.04683341340361806\n",
      "train loss:0.014575949465197585\n",
      "train loss:0.023510568937617674\n",
      "train loss:0.006175637033583525\n",
      "train loss:0.007561994890979925\n",
      "train loss:0.018189270039963127\n",
      "train loss:0.0381495568100683\n",
      "train loss:0.023557316257398198\n",
      "train loss:0.0019015189656365174\n",
      "train loss:0.009019241575168977\n",
      "train loss:0.023607185295538503\n",
      "train loss:0.017205483851445894\n",
      "train loss:0.007857960476331206\n",
      "train loss:0.019939657103027177\n",
      "train loss:0.005416770075987982\n",
      "train loss:0.009132584176432507\n",
      "train loss:0.013813212010992035\n",
      "train loss:0.015923903741748603\n",
      "train loss:0.0149507431622065\n",
      "train loss:0.026972727618257024\n",
      "train loss:0.017757326456938723\n",
      "train loss:0.009142438656669171\n",
      "train loss:0.028874159512856646\n",
      "train loss:0.015878606115023154\n",
      "train loss:0.025171233492642196\n",
      "train loss:0.028387100287735292\n",
      "train loss:0.05867334966662597\n",
      "train loss:0.003304053295751784\n",
      "train loss:0.044542537330410206\n",
      "train loss:0.04787755147646965\n",
      "train loss:0.003224272885735434\n",
      "train loss:0.005326272832796346\n",
      "train loss:0.003799174843606372\n",
      "train loss:0.02409380303615237\n",
      "train loss:0.0296061859545475\n",
      "train loss:0.0025134558404892276\n",
      "train loss:0.0038440814819891683\n",
      "train loss:0.031078902963004792\n",
      "train loss:0.01446502897061773\n",
      "train loss:0.021694158828216556\n",
      "train loss:0.006051132812405351\n",
      "train loss:0.04994662824911069\n",
      "train loss:0.024370964377207333\n",
      "train loss:0.017657956868492287\n",
      "train loss:0.009692203631550978\n",
      "train loss:0.023208473709565403\n",
      "train loss:0.006389454316124451\n",
      "train loss:0.0052332385954590565\n",
      "train loss:0.030857765776648602\n",
      "train loss:0.006417065145008613\n",
      "train loss:0.011416593476518171\n",
      "train loss:0.001672709183283232\n",
      "train loss:0.01713553735178878\n",
      "train loss:0.029189029911290833\n",
      "train loss:0.018681856033619285\n",
      "train loss:0.013753282211343778\n",
      "train loss:0.009491150092234012\n",
      "train loss:0.005701088975053544\n",
      "train loss:0.018913964718307114\n",
      "train loss:0.005939070345450321\n",
      "train loss:0.00347750050791228\n",
      "train loss:0.009426372629173917\n",
      "train loss:0.011590245674912816\n",
      "train loss:0.0011500534717018684\n",
      "train loss:0.0025737691662359374\n",
      "train loss:0.0062014049589095765\n",
      "train loss:0.05587098672625526\n",
      "train loss:0.0022796074803975415\n",
      "train loss:0.013063279499037678\n",
      "train loss:0.011000893072631548\n",
      "train loss:0.01163931291793374\n",
      "train loss:0.03287113108440784\n",
      "train loss:0.03151438214753287\n",
      "train loss:0.03358152163032144\n",
      "train loss:0.015166814031516696\n",
      "train loss:0.010729402418815539\n",
      "train loss:0.017875884222464625\n",
      "train loss:0.047201445917733176\n",
      "train loss:0.006089811973799717\n",
      "train loss:0.010865751864371005\n",
      "train loss:0.015879248740850123\n",
      "train loss:0.028753427127274463\n",
      "train loss:0.002073765635478316\n",
      "train loss:0.018729597737980264\n",
      "train loss:0.11505662722346247\n",
      "train loss:0.021825306150332655\n",
      "train loss:0.047088402744887986\n",
      "train loss:0.014000386779086297\n",
      "train loss:0.010731640103784936\n",
      "train loss:0.008498555948937612\n",
      "train loss:0.017558755657947574\n",
      "train loss:0.03152798075544742\n",
      "train loss:0.004138832302242113\n",
      "train loss:0.0074757590753618776\n",
      "train loss:0.0142209206140812\n",
      "train loss:0.004632256345147391\n",
      "train loss:0.0218204882759629\n",
      "train loss:0.07101904689257268\n",
      "train loss:0.010612000489205076\n",
      "train loss:0.014477127699251484\n",
      "train loss:0.02301286397692941\n",
      "train loss:0.023240979141466455\n",
      "train loss:0.012439883888304377\n",
      "train loss:0.018592796887082423\n",
      "train loss:0.0032640394695142906\n",
      "train loss:0.00486369771329753\n",
      "train loss:0.00857647390327888\n",
      "train loss:0.006422083677277954\n",
      "train loss:0.003656029483969016\n",
      "train loss:0.013862817277999373\n",
      "train loss:0.10623314319348416\n",
      "train loss:0.028468753194397026\n",
      "train loss:0.012204442078872932\n",
      "train loss:0.013642653180022155\n",
      "train loss:0.03342543915295403\n",
      "train loss:0.03433875192993868\n",
      "train loss:0.005733724708594007\n",
      "train loss:0.02933862088882749\n",
      "train loss:0.027623341357933807\n",
      "train loss:0.030674479177849077\n",
      "train loss:0.010182537716508415\n",
      "train loss:0.0029030470804677637\n",
      "train loss:0.005881521309580758\n",
      "train loss:0.005400408737549449\n",
      "train loss:0.05565712935968693\n",
      "train loss:0.010693552544477163\n",
      "train loss:0.009121084111332552\n",
      "train loss:0.04642224451459164\n",
      "train loss:0.021903938430384143\n",
      "train loss:0.03410426809363026\n",
      "train loss:0.015114951392506484\n",
      "train loss:0.024411888069320244\n",
      "train loss:0.007012431996273434\n",
      "train loss:0.008816968606780036\n",
      "train loss:0.020137993125451722\n",
      "train loss:0.02373139928878585\n",
      "train loss:0.032125132925574396\n",
      "train loss:0.015303644547650866\n",
      "train loss:0.009753769040466504\n",
      "train loss:0.008353788980617222\n",
      "train loss:0.04802132017218089\n",
      "train loss:0.022038872697613336\n",
      "train loss:0.013797302972246928\n",
      "train loss:0.009691417841474807\n",
      "train loss:0.004700101294297449\n",
      "train loss:0.016724106356168923\n",
      "train loss:0.007920953324507981\n",
      "train loss:0.008398907200090639\n",
      "train loss:0.014601266848127993\n",
      "train loss:0.03759993635282096\n",
      "train loss:0.03264624142254166\n",
      "train loss:0.012180074298650794\n",
      "train loss:0.04185133663632924\n",
      "train loss:0.009881102165203232\n",
      "train loss:0.011031422815889605\n",
      "train loss:0.05229907902477331\n",
      "train loss:0.007824605582213238\n",
      "train loss:0.02281847055201756\n",
      "train loss:0.011296602265732883\n",
      "=== epoch:7, train acc:0.99, test acc:0.988 ===\n",
      "train loss:0.025493387433391027\n",
      "train loss:0.013344214831419305\n",
      "train loss:0.009749961698460059\n",
      "train loss:0.008772064365106276\n",
      "train loss:0.015617024614008707\n",
      "train loss:0.022452030272833802\n",
      "train loss:0.016117689995560184\n",
      "train loss:0.048778707084497365\n",
      "train loss:0.04433551704482182\n",
      "train loss:0.018074235039899144\n",
      "train loss:0.003435578478752881\n",
      "train loss:0.01182683340890766\n",
      "train loss:0.02729306816208684\n",
      "train loss:0.07563456165897897\n",
      "train loss:0.0720326161698209\n",
      "train loss:0.005331872866010102\n",
      "train loss:0.029314366285588526\n",
      "train loss:0.02618839372714124\n",
      "train loss:0.013809631451802036\n",
      "train loss:0.01031953812496281\n",
      "train loss:0.008998294503585632\n",
      "train loss:0.010386381377232476\n",
      "train loss:0.006363473688977502\n",
      "train loss:0.013346328551951612\n",
      "train loss:0.005114343244373982\n",
      "train loss:0.00905836560852135\n",
      "train loss:0.05622326141476137\n",
      "train loss:0.008779983155735334\n",
      "train loss:0.011484734922429534\n",
      "train loss:0.043590013271357114\n",
      "train loss:0.006057526061651595\n",
      "train loss:0.013379446178555047\n",
      "train loss:0.01214830229937109\n",
      "train loss:0.02229922043016773\n",
      "train loss:0.006640927817709124\n",
      "train loss:0.01807056303791568\n",
      "train loss:0.0034208209745805952\n",
      "train loss:0.0768070725700132\n",
      "train loss:0.016870404932792564\n",
      "train loss:0.027616255882206148\n",
      "train loss:0.0795659882141244\n",
      "train loss:0.008919962523007129\n",
      "train loss:0.006061650358666859\n",
      "train loss:0.025555844402071322\n",
      "train loss:0.007003210058728192\n",
      "train loss:0.014194470093296522\n",
      "train loss:0.06431400478214055\n",
      "train loss:0.016440285072845742\n",
      "train loss:0.008036371849901486\n",
      "train loss:0.013397814698404644\n",
      "train loss:0.018127088598474388\n",
      "train loss:0.006673278076319433\n",
      "train loss:0.0646123059938545\n",
      "train loss:0.011307805546990783\n",
      "train loss:0.0008873632986811891\n",
      "train loss:0.055282585453743135\n",
      "train loss:0.026967981382729923\n",
      "train loss:0.0336291212101878\n",
      "train loss:0.01539653782824309\n",
      "train loss:0.015852429091585196\n",
      "train loss:0.03230474814593411\n",
      "train loss:0.010758176705811433\n",
      "train loss:0.03869989536813764\n",
      "train loss:0.02968734293389249\n",
      "train loss:0.0391352444566164\n",
      "train loss:0.03553341299690032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.018397697220414146\n",
      "train loss:0.08822334355236115\n",
      "train loss:0.011937323613035728\n",
      "train loss:0.006381823860646169\n",
      "train loss:0.007402396429577767\n",
      "train loss:0.02778854358635118\n",
      "train loss:0.013340863339485403\n",
      "train loss:0.011107943690237343\n",
      "train loss:0.030124319819981985\n",
      "train loss:0.007772024689471873\n",
      "train loss:0.005196722714960149\n",
      "train loss:0.025782919366559263\n",
      "train loss:0.0038484241903024856\n",
      "train loss:0.008276893195124308\n",
      "train loss:0.017985686621525946\n",
      "train loss:0.02063151429296278\n",
      "train loss:0.013171949540568691\n",
      "train loss:0.005961631527902367\n",
      "train loss:0.012888865868646134\n",
      "train loss:0.028563744607020592\n",
      "train loss:0.003912972514576938\n",
      "train loss:0.04997491654441296\n",
      "train loss:0.0047976476358073976\n",
      "train loss:0.05198225559751769\n",
      "train loss:0.03735397283418653\n",
      "train loss:0.01246500650404045\n",
      "train loss:0.015574032428342911\n",
      "train loss:0.013974893587731223\n",
      "train loss:0.055132076598764866\n",
      "train loss:0.026448000414597668\n",
      "train loss:0.021623239326964538\n",
      "train loss:0.0517302020451179\n",
      "train loss:0.008753925800850007\n",
      "train loss:0.0017378136311250894\n",
      "train loss:0.03998414916631175\n",
      "train loss:0.010603316481604317\n",
      "train loss:0.008632854075195935\n",
      "train loss:0.03184476625503229\n",
      "train loss:0.007354946352890387\n",
      "train loss:0.01163570755881327\n",
      "train loss:0.0034430052463402527\n",
      "train loss:0.021104920123759783\n",
      "train loss:0.013814354123280985\n",
      "train loss:0.003963108211217043\n",
      "train loss:0.00967790008335547\n",
      "train loss:0.0041092465527947306\n",
      "train loss:0.014806289624457474\n",
      "train loss:0.02641679416781335\n",
      "train loss:0.015058753140684491\n",
      "train loss:0.007802146711093383\n",
      "train loss:0.009642603850862594\n",
      "train loss:0.021019543104038814\n",
      "train loss:0.018997475825386943\n",
      "train loss:0.004203417089726782\n",
      "train loss:0.0024096587820179793\n",
      "train loss:0.013125989340405143\n",
      "train loss:0.013629292458272838\n",
      "train loss:0.01113529904233622\n",
      "train loss:0.044442197629514305\n",
      "train loss:0.03137505206893317\n",
      "train loss:0.00924281933850812\n",
      "train loss:0.00902755673440071\n",
      "train loss:0.005613086092418413\n",
      "train loss:0.004933082483322562\n",
      "train loss:0.01751062411438628\n",
      "train loss:0.03333684049615365\n",
      "train loss:0.010748478708980096\n",
      "train loss:0.03403186454542324\n",
      "train loss:0.009615130388038016\n",
      "train loss:0.05494115132802295\n",
      "train loss:0.012646172241275733\n",
      "train loss:0.02503620249966681\n",
      "train loss:0.02752300593174788\n",
      "train loss:0.02907451193697917\n",
      "train loss:0.04060685099490842\n",
      "train loss:0.013930623314892018\n",
      "train loss:0.07507537890796281\n",
      "train loss:0.023394135803457464\n",
      "train loss:0.010568317577355981\n",
      "train loss:0.022258966741727358\n",
      "train loss:0.052234088432067595\n",
      "train loss:0.05408647800733412\n",
      "train loss:0.03457714754148305\n",
      "train loss:0.006467535372481591\n",
      "train loss:0.025024621573060193\n",
      "train loss:0.02507620934858159\n",
      "train loss:0.008268709887805125\n",
      "train loss:0.00495522526643747\n",
      "train loss:0.013199428960978317\n",
      "train loss:0.021572739034484974\n",
      "train loss:0.10074503510163237\n",
      "train loss:0.004544595991398898\n",
      "train loss:0.05499318323871859\n",
      "train loss:0.015223212040505934\n",
      "train loss:0.009091904324443678\n",
      "train loss:0.0011170183789073144\n",
      "train loss:0.008208133682147354\n",
      "train loss:0.01165433344889408\n",
      "train loss:0.004926755038299563\n",
      "train loss:0.0071452504894703025\n",
      "train loss:0.039710394899172094\n",
      "train loss:0.025670461408378687\n",
      "train loss:0.004095391065241521\n",
      "train loss:0.0018814554406756164\n",
      "train loss:0.025180237693129274\n",
      "train loss:0.0746729565770548\n",
      "train loss:0.006290928534345788\n",
      "train loss:0.011236306864584752\n",
      "train loss:0.009834863656527991\n",
      "train loss:0.028185819239741033\n",
      "train loss:0.04289587835962579\n",
      "train loss:0.01894976186795823\n",
      "train loss:0.009296137086174768\n",
      "train loss:0.009566317050381155\n",
      "train loss:0.002337283051765064\n",
      "train loss:0.004362635096487842\n",
      "train loss:0.002736021176913565\n",
      "train loss:0.031922345633741946\n",
      "train loss:0.023029156019033565\n",
      "train loss:0.011226432586987382\n",
      "train loss:0.01748240764360366\n",
      "train loss:0.00688836007268783\n",
      "train loss:0.04368737759511383\n",
      "train loss:0.010993105599586175\n",
      "train loss:0.007657055842971165\n",
      "train loss:0.0050018226442276005\n",
      "train loss:0.0076348979144901154\n",
      "train loss:0.009138913803250177\n",
      "train loss:0.04886603048621457\n",
      "train loss:0.01647774355278514\n",
      "train loss:0.005429202115893816\n",
      "train loss:0.010113348243100293\n",
      "train loss:0.005513453964894253\n",
      "train loss:0.011671904105351048\n",
      "train loss:0.006802947758642886\n",
      "train loss:0.004160578171340978\n",
      "train loss:0.003056606836342665\n",
      "train loss:0.01601913366096074\n",
      "train loss:0.024708914023830866\n",
      "train loss:0.05726448496702396\n",
      "train loss:0.008239098764315736\n",
      "train loss:0.0038936526416145584\n",
      "train loss:0.007183916321643059\n",
      "train loss:0.04215149759133814\n",
      "train loss:0.008101016767863284\n",
      "train loss:0.006993160800203833\n",
      "train loss:0.02929842977608778\n",
      "train loss:0.010042753972378828\n",
      "train loss:0.012779034178245234\n",
      "train loss:0.005756567167284846\n",
      "train loss:0.003929543740660431\n",
      "train loss:0.014882278594954381\n",
      "train loss:0.003903403539015219\n",
      "train loss:0.010919152190590744\n",
      "train loss:0.006306845260224403\n",
      "train loss:0.006961203406970876\n",
      "train loss:0.01668957012113061\n",
      "train loss:0.019415124489132605\n",
      "train loss:0.0037391303809288477\n",
      "train loss:0.006127449900001803\n",
      "train loss:0.008633606057795033\n",
      "train loss:0.04341810892173435\n",
      "train loss:0.02329251218204499\n",
      "train loss:0.004147566763690219\n",
      "train loss:0.011781931070777341\n",
      "train loss:0.009885501535268041\n",
      "train loss:0.006943114193804702\n",
      "train loss:0.010515212642729522\n",
      "train loss:0.06210375366308586\n",
      "train loss:0.02725265023551839\n",
      "train loss:0.008732519512701079\n",
      "train loss:0.007506653701522307\n",
      "train loss:0.018365115123773642\n",
      "train loss:0.014924898343668246\n",
      "train loss:0.01073584638642169\n",
      "train loss:0.03978044441815015\n",
      "train loss:0.0020888566821432824\n",
      "train loss:0.008907009176699739\n",
      "train loss:0.013319508269554777\n",
      "train loss:0.06264654798137624\n",
      "train loss:0.024289344863499554\n",
      "train loss:0.023602142078774726\n",
      "train loss:0.009661897925245909\n",
      "train loss:0.002148962173381449\n",
      "train loss:0.0044360348166946765\n",
      "train loss:0.05236271440443752\n",
      "train loss:0.01621284473000014\n",
      "train loss:0.007146222526684919\n",
      "train loss:0.0026152319914432426\n",
      "train loss:0.019849391499416457\n",
      "train loss:0.0020136774943667044\n",
      "train loss:0.055698974676674895\n",
      "train loss:0.025238885385005703\n",
      "train loss:0.006782395990972655\n",
      "train loss:0.024956403644504733\n",
      "train loss:0.03653798458300048\n",
      "train loss:0.03969454012165202\n",
      "train loss:0.005373193657153058\n",
      "train loss:0.006171950745244705\n",
      "train loss:0.011492270942015608\n",
      "train loss:0.010184154018723359\n",
      "train loss:0.00843869293919297\n",
      "train loss:0.02293860920175432\n",
      "train loss:0.004089354576919524\n",
      "train loss:0.003466771626434341\n",
      "train loss:0.007213120254151448\n",
      "train loss:0.0033513936296408085\n",
      "train loss:0.046708851601884635\n",
      "train loss:0.005854324429873229\n",
      "train loss:0.030310593659552222\n",
      "train loss:0.008847989676173153\n",
      "train loss:0.003715143511296733\n",
      "train loss:0.004116110662565963\n",
      "train loss:0.004019255727529075\n",
      "train loss:0.005578377574953279\n",
      "train loss:0.033361131781078246\n",
      "train loss:0.0018802396514123704\n",
      "train loss:0.016668924652421056\n",
      "train loss:0.02723656515579473\n",
      "train loss:0.006366772981503693\n",
      "train loss:0.017096117784622687\n",
      "train loss:0.07105068310029145\n",
      "train loss:0.014243424468970131\n",
      "train loss:0.004780063000655516\n",
      "train loss:0.025453662977188995\n",
      "train loss:0.016275133390721796\n",
      "train loss:0.007114526526746765\n",
      "train loss:0.017957850078171755\n",
      "train loss:0.006787599209157443\n",
      "train loss:0.005911023568000744\n",
      "train loss:0.011875305624299283\n",
      "train loss:0.003110914380900074\n",
      "train loss:0.022102155851035962\n",
      "train loss:0.024764632294866192\n",
      "train loss:0.012875322042325776\n",
      "train loss:0.04363079545074314\n",
      "train loss:0.003500995793837591\n",
      "train loss:0.005528706923568731\n",
      "train loss:0.013763702055437506\n",
      "train loss:0.01427995283077284\n",
      "train loss:0.005073966200579819\n",
      "train loss:0.019644496305523774\n",
      "train loss:0.02782873463599836\n",
      "train loss:0.0223749555104782\n",
      "train loss:0.022916746979654495\n",
      "train loss:0.007632714843798943\n",
      "train loss:0.0032148437921211827\n",
      "train loss:0.05764416188739159\n",
      "train loss:0.008277115651831706\n",
      "train loss:0.008687131067124642\n",
      "train loss:0.005409910466556933\n",
      "train loss:0.034148916345972745\n",
      "train loss:0.0241526813708941\n",
      "train loss:0.003522525607020924\n",
      "train loss:0.03583707714255246\n",
      "train loss:0.020165194770399256\n",
      "train loss:0.00662273484127545\n",
      "train loss:0.0056504324564555205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.007122710638048969\n",
      "train loss:0.017813767155393335\n",
      "train loss:0.056672522182631455\n",
      "train loss:0.0069031284349590025\n",
      "train loss:0.012763407742927519\n",
      "train loss:0.026855562288827538\n",
      "train loss:0.010291268927110771\n",
      "train loss:0.05281306439850144\n",
      "train loss:0.02266140006785408\n",
      "train loss:0.0028031830909425086\n",
      "train loss:0.05530838199386156\n",
      "train loss:0.04597514613537713\n",
      "train loss:0.0013253116562884101\n",
      "train loss:0.02046688407888946\n",
      "train loss:0.02082691149944871\n",
      "train loss:0.005764640905533833\n",
      "train loss:0.009420700175032229\n",
      "train loss:0.03722880794479472\n",
      "train loss:0.008021468971412671\n",
      "train loss:0.025794436490848813\n",
      "train loss:0.003588150105988461\n",
      "train loss:0.042627834496714846\n",
      "train loss:0.04348320018233934\n",
      "train loss:0.005764291293976712\n",
      "train loss:0.06555017066680045\n",
      "train loss:0.0020286834376732187\n",
      "train loss:0.033003801656085775\n",
      "train loss:0.00261470765054601\n",
      "train loss:0.009337451171349808\n",
      "train loss:0.002331935779090856\n",
      "train loss:0.018647179778092018\n",
      "train loss:0.00974965615790845\n",
      "train loss:0.015744515665968527\n",
      "train loss:0.01225774941562307\n",
      "train loss:0.03489395276505956\n",
      "train loss:0.007191375799675757\n",
      "train loss:0.011205427181769536\n",
      "train loss:0.04814961252570685\n",
      "train loss:0.020950740792058373\n",
      "train loss:0.04366852525839139\n",
      "train loss:0.06143229521930207\n",
      "train loss:0.002914168478141789\n",
      "train loss:0.0201558126459501\n",
      "train loss:0.04589223315348221\n",
      "train loss:0.017244617033752744\n",
      "train loss:0.03410777534490484\n",
      "train loss:0.005763292660164487\n",
      "train loss:0.0164529552554035\n",
      "train loss:0.012377326315041565\n",
      "train loss:0.012520736108440319\n",
      "train loss:0.00816389624719353\n",
      "train loss:0.010960940429952024\n",
      "train loss:0.0181360748987867\n",
      "train loss:0.03278872197317806\n",
      "train loss:0.011956767954557548\n",
      "train loss:0.0021777131334016426\n",
      "train loss:0.02089910665465857\n",
      "train loss:0.03851393892105451\n",
      "train loss:0.008384176922655136\n",
      "train loss:0.005107722489578453\n",
      "train loss:0.010243216947969168\n",
      "train loss:0.009923531846314433\n",
      "train loss:0.018615394497744832\n",
      "train loss:0.0029920082658987882\n",
      "train loss:0.018691677737990683\n",
      "train loss:0.02847099780803246\n",
      "train loss:0.0302509755114974\n",
      "train loss:0.020370950416046547\n",
      "train loss:0.03217840247785906\n",
      "train loss:0.004640160048342869\n",
      "train loss:0.011890277434613216\n",
      "train loss:0.020349531090150218\n",
      "train loss:0.010808754685978559\n",
      "train loss:0.014277003628600578\n",
      "train loss:0.016190559387133853\n",
      "train loss:0.03686339661527409\n",
      "train loss:0.01225522700313771\n",
      "train loss:0.047771871324206155\n",
      "train loss:0.004240873384163307\n",
      "train loss:0.006660296017931246\n",
      "train loss:0.009778206861206158\n",
      "train loss:0.00522446426229739\n",
      "train loss:0.02530613439609579\n",
      "train loss:0.009414174379531091\n",
      "train loss:0.008412074772495299\n",
      "train loss:0.009732799829812762\n",
      "train loss:0.003058108042172824\n",
      "train loss:0.026164269443609167\n",
      "train loss:0.009869221614715614\n",
      "train loss:0.0038541307483319605\n",
      "train loss:0.04080335563580672\n",
      "train loss:0.009963821803521842\n",
      "train loss:0.0038071909373164838\n",
      "train loss:0.0063683898418142515\n",
      "train loss:0.010234971818698358\n",
      "train loss:0.005576546863631242\n",
      "train loss:0.00781971613365657\n",
      "train loss:0.026991865620693018\n",
      "train loss:0.0008935621454495213\n",
      "train loss:0.008382905850915157\n",
      "train loss:0.001976886735486263\n",
      "train loss:0.06588178781544164\n",
      "train loss:0.005828782608964625\n",
      "train loss:0.00783758588545918\n",
      "train loss:0.005804233413148074\n",
      "train loss:0.0014267704608525297\n",
      "train loss:0.011693448864730928\n",
      "train loss:0.00864777252333169\n",
      "train loss:0.00553577808196905\n",
      "train loss:0.014179606026936986\n",
      "train loss:0.010404589423624235\n",
      "train loss:0.015553415811107514\n",
      "train loss:0.030036865851550923\n",
      "train loss:0.009927940100562449\n",
      "train loss:0.010571078694313752\n",
      "train loss:0.0036400657601270035\n",
      "train loss:0.010291592761043014\n",
      "train loss:0.013879292812044577\n",
      "train loss:0.004942446070764401\n",
      "train loss:0.008158503325873326\n",
      "train loss:0.007857377205035094\n",
      "train loss:0.001552810197388959\n",
      "train loss:0.025748554477517144\n",
      "train loss:0.005809807833837318\n",
      "train loss:0.011616770470146214\n",
      "train loss:0.01975231277837537\n",
      "train loss:0.005731366197453711\n",
      "train loss:0.0059171677060461635\n",
      "train loss:0.014339901649661266\n",
      "train loss:0.011571996826990436\n",
      "train loss:0.006223270769207661\n",
      "train loss:0.028063247230424925\n",
      "train loss:0.013452201206105168\n",
      "train loss:0.006305270184582518\n",
      "train loss:0.009413978581303184\n",
      "train loss:0.018888452089010684\n",
      "train loss:0.00910684976695126\n",
      "train loss:0.01819600094497832\n",
      "train loss:0.020240908573832418\n",
      "train loss:0.0022456038253419377\n",
      "train loss:0.03246224011073229\n",
      "train loss:0.0028072033148834564\n",
      "train loss:0.026754200539833968\n",
      "train loss:0.0051324699743559386\n",
      "train loss:0.007956020379952168\n",
      "train loss:0.03583750568489039\n",
      "train loss:0.0479723893851453\n",
      "train loss:0.02066024006336558\n",
      "train loss:0.00637795754796613\n",
      "train loss:0.008342229968661291\n",
      "train loss:0.012468739468018164\n",
      "train loss:0.039278698810252984\n",
      "train loss:0.008966124235940393\n",
      "train loss:0.010702248151031544\n",
      "train loss:0.003373580709510652\n",
      "train loss:0.0094725956470948\n",
      "train loss:0.010847169925710736\n",
      "train loss:0.006363805061334716\n",
      "train loss:0.057170913430739656\n",
      "train loss:0.03355170850325866\n",
      "train loss:0.011546846666922237\n",
      "train loss:0.012560104766635532\n",
      "train loss:0.011970546972342146\n",
      "train loss:0.004317882401908933\n",
      "train loss:0.0024072508676486765\n",
      "train loss:0.048474810064878915\n",
      "train loss:0.0014534451993248668\n",
      "train loss:0.0689874199211966\n",
      "train loss:0.015945635812287008\n",
      "train loss:0.01577815141186765\n",
      "train loss:0.027794081931909943\n",
      "train loss:0.005798708833438622\n",
      "train loss:0.044037074419992696\n",
      "train loss:0.012488002536525848\n",
      "train loss:0.007593261010030149\n",
      "train loss:0.01639957426128406\n",
      "train loss:0.03886724129983478\n",
      "train loss:0.005134427870118303\n",
      "train loss:0.007011135542820433\n",
      "train loss:0.04407935970643686\n",
      "train loss:0.02072681742952964\n",
      "train loss:0.049193686142993326\n",
      "train loss:0.034488532635351205\n",
      "train loss:0.03476286827653502\n",
      "train loss:0.0016946548661658698\n",
      "train loss:0.006838728543042592\n",
      "train loss:0.014856539130406576\n",
      "train loss:0.003222971266513643\n",
      "train loss:0.007818454988106462\n",
      "train loss:0.020014403456922958\n",
      "train loss:0.0071099636343282336\n",
      "train loss:0.013234097243461378\n",
      "train loss:0.0012769981495298365\n",
      "train loss:0.030132988458602305\n",
      "train loss:0.00664135208887365\n",
      "train loss:0.024050059180683994\n",
      "train loss:0.010221381390684544\n",
      "train loss:0.004404988107410469\n",
      "train loss:0.005738878233985052\n",
      "train loss:0.0026173865416944564\n",
      "train loss:0.009053711293036277\n",
      "train loss:0.04179025671937962\n",
      "train loss:0.007277246316304506\n",
      "train loss:0.004679624905570231\n",
      "train loss:0.007635999172697845\n",
      "train loss:0.05917417946760779\n",
      "train loss:0.01589508872094262\n",
      "train loss:0.027364792595249356\n",
      "train loss:0.00967332891283169\n",
      "train loss:0.022367339772151808\n",
      "train loss:0.0031481882619809522\n",
      "train loss:0.02405710196209437\n",
      "train loss:0.01383184096634395\n",
      "train loss:0.018777488307755788\n",
      "train loss:0.003981362070168357\n",
      "train loss:0.003929299899804179\n",
      "train loss:0.016690961809928347\n",
      "train loss:0.012119341783012525\n",
      "train loss:0.02374768277946081\n",
      "train loss:0.04906722942843678\n",
      "train loss:0.011428286266471127\n",
      "train loss:0.016095836593159742\n",
      "train loss:0.010052517873846306\n",
      "train loss:0.015841440248669972\n",
      "train loss:0.012098495143184493\n",
      "train loss:0.013328601596251622\n",
      "train loss:0.011631140523053777\n",
      "train loss:0.024527290539830608\n",
      "train loss:0.011154796032613741\n",
      "train loss:0.015696492312101712\n",
      "train loss:0.005742055850069975\n",
      "train loss:0.011567448961177009\n",
      "train loss:0.014902444698159512\n",
      "train loss:0.015720592690736696\n",
      "train loss:0.008461348529843435\n",
      "train loss:0.006965714362168377\n",
      "train loss:0.02232074204952897\n",
      "train loss:0.010312440296661663\n",
      "train loss:0.008360168759273624\n",
      "train loss:0.020194690025783665\n",
      "train loss:0.0023117748122485123\n",
      "train loss:0.009078998715519346\n",
      "train loss:0.004627351036060505\n",
      "train loss:0.04724831180415509\n",
      "train loss:0.0018806043192920224\n",
      "train loss:0.011207480742488538\n",
      "train loss:0.010701456249018687\n",
      "train loss:0.0020464085206473347\n",
      "train loss:0.0035105664364316596\n",
      "train loss:0.005750630125604269\n",
      "train loss:0.0009766450800494204\n",
      "train loss:0.004168551823584759\n",
      "train loss:0.0064067261330759025\n",
      "train loss:0.026901359885135343\n",
      "train loss:0.025789005956341083\n",
      "train loss:0.010726079010380491\n",
      "train loss:0.008176652338653322\n",
      "train loss:0.008769036831941721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.011724302834133447\n",
      "train loss:0.006174170756016328\n",
      "train loss:0.025631038768424822\n",
      "train loss:0.0028946247704944384\n",
      "train loss:0.005273420272722121\n",
      "train loss:0.010048939418326774\n",
      "train loss:0.004470825161994893\n",
      "train loss:0.0342844114426036\n",
      "train loss:0.019004466275270564\n",
      "train loss:0.00405686429988474\n",
      "train loss:0.012405749776914539\n",
      "train loss:0.018583839331376167\n",
      "train loss:0.022871444284568818\n",
      "train loss:0.0029086607205536246\n",
      "train loss:0.0024711745321747635\n",
      "train loss:0.024815196629804612\n",
      "train loss:0.04473915986643057\n",
      "train loss:0.006553916719616772\n",
      "=== epoch:8, train acc:0.991, test acc:0.986 ===\n",
      "train loss:0.0007361969360463469\n",
      "train loss:0.005752088399444504\n",
      "train loss:0.00725954578663467\n",
      "train loss:0.009132478222948248\n",
      "train loss:0.02856561857268436\n",
      "train loss:0.0014382441573503083\n",
      "train loss:0.003920129351369683\n",
      "train loss:0.024587119688405195\n",
      "train loss:0.0044656587756079814\n",
      "train loss:0.13088619221505213\n",
      "train loss:0.01356187328569467\n",
      "train loss:0.025694563370767812\n",
      "train loss:0.008894033745095018\n",
      "train loss:0.006986307225764946\n",
      "train loss:0.00997838551207796\n",
      "train loss:0.001914498957633696\n",
      "train loss:0.014327222199153118\n",
      "train loss:0.01726612994904901\n",
      "train loss:0.003228877757768536\n",
      "train loss:0.005945666602680798\n",
      "train loss:0.0064225722312764855\n",
      "train loss:0.008466274449753102\n",
      "train loss:0.009147649928331184\n",
      "train loss:0.010351216161889255\n",
      "train loss:0.0025559606068852585\n",
      "train loss:0.00567166765078162\n",
      "train loss:0.0014904614974941843\n",
      "train loss:0.003553732695267175\n",
      "train loss:0.014050919498125107\n",
      "train loss:0.017074648854809117\n",
      "train loss:0.01442442896983631\n",
      "train loss:0.038668917837074324\n",
      "train loss:0.007549404979005346\n",
      "train loss:0.002829830676555589\n",
      "train loss:0.019154547525641597\n",
      "train loss:0.016002524749174688\n",
      "train loss:0.009109708489828575\n",
      "train loss:0.010327665428279452\n",
      "train loss:0.010632809956673291\n",
      "train loss:0.021761315441217552\n",
      "train loss:0.011877504760100634\n",
      "train loss:0.0053137998854675475\n",
      "train loss:0.020636248116037016\n",
      "train loss:0.008309994953865725\n",
      "train loss:0.004090644967752405\n",
      "train loss:0.003989596877087127\n",
      "train loss:0.04335449345936067\n",
      "train loss:0.021749114267078526\n",
      "train loss:0.036101068970312934\n",
      "train loss:0.039531481869695656\n",
      "train loss:0.00708588680270349\n",
      "train loss:0.06007597323626958\n",
      "train loss:0.008739159094986446\n",
      "train loss:0.02590279688177098\n",
      "train loss:0.03494234148647042\n",
      "train loss:0.005539924196752398\n",
      "train loss:0.005912548251590618\n",
      "train loss:0.023301371626862095\n",
      "train loss:0.005688048543920612\n",
      "train loss:0.004674702308569188\n",
      "train loss:0.05642443809358348\n",
      "train loss:0.0018810708250646662\n",
      "train loss:0.006636262119338376\n",
      "train loss:0.008252691658813111\n",
      "train loss:0.007196241515748838\n",
      "train loss:0.005310414498161754\n",
      "train loss:0.0104439775934876\n",
      "train loss:0.0033337743991441017\n",
      "train loss:0.009713739675722184\n",
      "train loss:0.006489480140631176\n",
      "train loss:0.023473505725132057\n",
      "train loss:0.03867644239717387\n",
      "train loss:0.002498386851910742\n",
      "train loss:0.007611610337146356\n",
      "train loss:0.018697754648264387\n",
      "train loss:0.032230683998738685\n",
      "train loss:0.02134944871411927\n",
      "train loss:0.018526524298694728\n",
      "train loss:0.0035119825095168296\n",
      "train loss:0.020240131332164533\n",
      "train loss:0.015868008904197033\n",
      "train loss:0.014819803795915309\n",
      "train loss:0.0058263014435564545\n",
      "train loss:0.015099484821926216\n",
      "train loss:0.00432294056018821\n",
      "train loss:0.01654560359013898\n",
      "train loss:0.0042661225807189085\n",
      "train loss:0.017500397806344724\n",
      "train loss:0.004856189506064425\n",
      "train loss:0.05771974444101516\n",
      "train loss:0.002309517822622561\n",
      "train loss:0.007713465765535433\n",
      "train loss:0.008289394205285534\n",
      "train loss:0.008214909973764794\n",
      "train loss:0.008357725414391094\n",
      "train loss:0.00990813550513259\n",
      "train loss:0.02386752380843745\n",
      "train loss:0.003015205044636067\n",
      "train loss:0.011490047108332941\n",
      "train loss:0.036923632183780294\n",
      "train loss:0.01075491416620463\n",
      "train loss:0.0796032357611849\n",
      "train loss:0.006215262299326956\n",
      "train loss:0.003778673133300757\n",
      "train loss:0.00956610899321697\n",
      "train loss:0.010329820948622536\n",
      "train loss:0.014522729531586589\n",
      "train loss:0.005533502336121835\n",
      "train loss:0.005668098712861642\n",
      "train loss:0.014557933287066893\n",
      "train loss:0.08599217401179128\n",
      "train loss:0.005743013153893678\n",
      "train loss:0.011789406427517038\n",
      "train loss:0.009698402735549397\n",
      "train loss:0.0038421193078533294\n",
      "train loss:0.018441477452783687\n",
      "train loss:0.01596568971365915\n",
      "train loss:0.047310784402389804\n",
      "train loss:0.016302111446832204\n",
      "train loss:0.007772927864741113\n",
      "train loss:0.007355263712281861\n",
      "train loss:0.011273785473974335\n",
      "train loss:0.006384826186515342\n",
      "train loss:0.02008777359830165\n",
      "train loss:0.0335190326553901\n",
      "train loss:0.009721231374736876\n",
      "train loss:0.1252523501371769\n",
      "train loss:0.0010877171553925796\n",
      "train loss:0.028734285354838556\n",
      "train loss:0.006403968952787491\n",
      "train loss:0.05174793008377212\n",
      "train loss:0.14657185069307693\n",
      "train loss:0.024523547884346932\n",
      "train loss:0.01981131241828147\n",
      "train loss:0.03943709933773829\n",
      "train loss:0.010013193149886548\n",
      "train loss:0.023206978403408546\n",
      "train loss:0.03825695540051779\n",
      "train loss:0.00773080006838258\n",
      "train loss:0.034913804362735716\n",
      "train loss:0.0274671376429376\n",
      "train loss:0.0027225748466745475\n",
      "train loss:0.012708480441876797\n",
      "train loss:0.00794170870754889\n",
      "train loss:0.03355408073052207\n",
      "train loss:0.008072920380103935\n",
      "train loss:0.0034570805749274975\n",
      "train loss:0.021886476644291105\n",
      "train loss:0.005725812579072982\n",
      "train loss:0.008213411246860503\n",
      "train loss:0.036087358542789265\n",
      "train loss:0.0016000008022382202\n",
      "train loss:0.04503728965238207\n",
      "train loss:0.045815096798109384\n",
      "train loss:0.002373433274615251\n",
      "train loss:0.011323296481420459\n",
      "train loss:0.004594240142936728\n",
      "train loss:0.009192617716557643\n",
      "train loss:0.019869393600053863\n",
      "train loss:0.00963510433921795\n",
      "train loss:0.013581914614579917\n",
      "train loss:0.0008366299836588958\n",
      "train loss:0.01329024077322757\n",
      "train loss:0.07983290494237621\n",
      "train loss:0.01597315219253185\n",
      "train loss:0.00908016967784125\n",
      "train loss:0.009208634656111124\n",
      "train loss:0.005880196744300809\n",
      "train loss:0.005863683752087582\n",
      "train loss:0.00955366755817499\n",
      "train loss:0.01172908933276603\n",
      "train loss:0.05582183094121199\n",
      "train loss:0.01754766417209381\n",
      "train loss:0.004831804290943412\n",
      "train loss:0.01075616606143623\n",
      "train loss:0.0026152272835495978\n",
      "train loss:0.00990586496633805\n",
      "train loss:0.017718152520368683\n",
      "train loss:0.011993890617480225\n",
      "train loss:0.009759970934148137\n",
      "train loss:0.015756798766057428\n",
      "train loss:0.009471361288570972\n",
      "train loss:0.006457419537218973\n",
      "train loss:0.003020542919748419\n",
      "train loss:0.004961588621617422\n",
      "train loss:0.03997150763989618\n",
      "train loss:0.0019173560235272633\n",
      "train loss:0.005154308125493807\n",
      "train loss:0.009518363404858468\n",
      "train loss:0.007024928317551769\n",
      "train loss:0.01032122660507049\n",
      "train loss:0.00491590219041929\n",
      "train loss:0.005445905068378203\n",
      "train loss:0.012461957126100693\n",
      "train loss:0.011143545281176616\n",
      "train loss:0.0018418294622054793\n",
      "train loss:0.0005570611688926287\n",
      "train loss:0.0016378472626057854\n",
      "train loss:0.002799111115126845\n",
      "train loss:0.014750978469908662\n",
      "train loss:0.01519341321099855\n",
      "train loss:0.021558729291615685\n",
      "train loss:0.0026169448728570093\n",
      "train loss:0.023235031267708012\n",
      "train loss:0.007336467210600629\n",
      "train loss:0.022060106513886932\n",
      "train loss:0.004783933225472913\n",
      "train loss:0.01177234602616379\n",
      "train loss:0.03735367579664462\n",
      "train loss:0.01974516353817485\n",
      "train loss:0.03668742325538901\n",
      "train loss:0.0352462709509588\n",
      "train loss:0.0202481276751513\n",
      "train loss:0.00918043557385918\n",
      "train loss:0.01628355002418898\n",
      "train loss:0.01636097280541536\n",
      "train loss:0.031733705071123955\n",
      "train loss:0.018190474994394502\n",
      "train loss:0.00500012282624199\n",
      "train loss:0.016239982308459983\n",
      "train loss:0.0016694840962114107\n",
      "train loss:0.004122397371523026\n",
      "train loss:0.00872476791775812\n",
      "train loss:0.0017219925587822632\n",
      "train loss:0.0004832589659740892\n",
      "train loss:0.007095674247987516\n",
      "train loss:0.03304845097795655\n",
      "train loss:0.024981359101364772\n",
      "train loss:0.003045802442976969\n",
      "train loss:0.001253880588422273\n",
      "train loss:0.0033223092794716913\n",
      "train loss:0.008912816316179566\n",
      "train loss:0.010370523953528414\n",
      "train loss:0.010043568415630633\n",
      "train loss:0.013598007578644362\n",
      "train loss:0.04119964043660194\n",
      "train loss:0.0064531485704038384\n",
      "train loss:0.05935156905848627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0040870470927650586\n",
      "train loss:0.003874120835286527\n",
      "train loss:0.01386092234863306\n",
      "train loss:0.005689295277175496\n",
      "train loss:0.01737337518286407\n",
      "train loss:0.008173623101288562\n",
      "train loss:0.003037940128926202\n",
      "train loss:0.01873922644251929\n",
      "train loss:0.018041640716616236\n",
      "train loss:0.009914392693613931\n",
      "train loss:0.013508800049778925\n",
      "train loss:0.017005475987413688\n",
      "train loss:0.014470955456169847\n",
      "train loss:0.0035131013367344726\n",
      "train loss:0.003940749291563673\n",
      "train loss:0.05264882773569653\n",
      "train loss:0.006752075784813085\n",
      "train loss:0.013217647056774043\n",
      "train loss:0.019690985365968593\n",
      "train loss:0.04470945898069733\n",
      "train loss:0.008381937657594409\n",
      "train loss:0.002795782815663373\n",
      "train loss:0.011107457881362161\n",
      "train loss:0.03049032279617452\n",
      "train loss:0.03291664010240462\n",
      "train loss:0.005753492002410322\n",
      "train loss:0.01966409147060851\n",
      "train loss:0.0041403078723407285\n",
      "train loss:0.002258908612866405\n",
      "train loss:0.008863664527720819\n",
      "train loss:0.002153722719723612\n",
      "train loss:0.000478020723319037\n",
      "train loss:0.006200771853787304\n",
      "train loss:0.008407314351944367\n",
      "train loss:0.00540125520326623\n",
      "train loss:0.016544281014367154\n",
      "train loss:0.0011058673544189733\n",
      "train loss:0.012013001849307224\n",
      "train loss:0.01745108658461583\n",
      "train loss:0.008025947127459054\n",
      "train loss:0.008118759164008851\n",
      "train loss:0.01855893310642708\n",
      "train loss:0.04544300004778092\n",
      "train loss:0.0014433618830967713\n",
      "train loss:0.0039139659937665085\n",
      "train loss:0.00840858331516053\n",
      "train loss:0.003956944967320466\n",
      "train loss:0.06916142191081709\n",
      "train loss:0.010414225563963922\n",
      "train loss:0.004886967275951519\n",
      "train loss:0.018366901916786905\n",
      "train loss:0.01681650033129327\n",
      "train loss:0.0010815342351175366\n",
      "train loss:0.0498098090367075\n",
      "train loss:0.0292058884278335\n",
      "train loss:0.012986627332029342\n",
      "train loss:0.012249088685936174\n",
      "train loss:0.00262607363917711\n",
      "train loss:0.005076358376521195\n",
      "train loss:0.015025731614403539\n",
      "train loss:0.024042819234185872\n",
      "train loss:0.017223031487394867\n",
      "train loss:0.0228637891091105\n",
      "train loss:0.012218416168471372\n",
      "train loss:0.012600287512705895\n",
      "train loss:0.004695266090164793\n",
      "train loss:0.01588421902917692\n",
      "train loss:0.0982606918430063\n",
      "train loss:0.05051008571111808\n",
      "train loss:0.004501764853628436\n",
      "train loss:0.008730964741009763\n",
      "train loss:0.0015354257216160547\n",
      "train loss:0.010447916305594194\n",
      "train loss:0.0038196005026388245\n",
      "train loss:0.0024416498419536398\n",
      "train loss:0.008148279450800738\n",
      "train loss:0.005505083472902178\n",
      "train loss:0.009321290435378812\n",
      "train loss:0.012421541423677816\n",
      "train loss:0.013048376463689258\n",
      "train loss:0.0007571297523865785\n",
      "train loss:0.02876550531006559\n",
      "train loss:0.024265030068784017\n",
      "train loss:0.008667273037167033\n",
      "train loss:0.03188419439621763\n",
      "train loss:0.024662199906140766\n",
      "train loss:0.007415770895488837\n",
      "train loss:0.051265478345611466\n",
      "train loss:0.021791258583598764\n",
      "train loss:0.005649845295108408\n",
      "train loss:0.00852151354256871\n",
      "train loss:0.02476353704551054\n",
      "train loss:0.005269201683616133\n",
      "train loss:0.0427877408214283\n",
      "train loss:0.013697620397221874\n",
      "train loss:0.004381550743732322\n",
      "train loss:0.011281478368344772\n",
      "train loss:0.0074941572946955\n",
      "train loss:0.007761808050798145\n",
      "train loss:0.022928325871206665\n",
      "train loss:0.001328869565331643\n",
      "train loss:0.047267359442512494\n",
      "train loss:0.10307731404866095\n",
      "train loss:0.01796968135353116\n",
      "train loss:0.014400735600583088\n",
      "train loss:0.008322585718164589\n",
      "train loss:0.0036065324474989576\n",
      "train loss:0.007062040286825863\n",
      "train loss:0.005892204854362799\n",
      "train loss:0.0076191506306940325\n",
      "train loss:0.05467170765429031\n",
      "train loss:0.016531778334246055\n",
      "train loss:0.0055740374711038395\n",
      "train loss:0.002023223739212813\n",
      "train loss:0.005159315196119219\n",
      "train loss:0.006974684834690896\n",
      "train loss:0.028412273994538054\n",
      "train loss:0.00349181716224366\n",
      "train loss:0.003298227973398149\n",
      "train loss:0.03394516907028147\n",
      "train loss:0.014707335627635926\n",
      "train loss:0.10195667615519229\n",
      "train loss:0.023085088637598502\n",
      "train loss:0.02302582674603751\n",
      "train loss:0.029082531444004394\n",
      "train loss:0.04939507209083991\n",
      "train loss:0.02802338448887531\n",
      "train loss:0.014564747554575765\n",
      "train loss:0.009923765919605441\n",
      "train loss:0.09591881904264897\n",
      "train loss:0.1633657968232466\n",
      "train loss:0.01855958975106194\n",
      "train loss:0.0036531526292977083\n",
      "train loss:0.005681875962997382\n",
      "train loss:0.01208514052435142\n",
      "train loss:0.006448472754066928\n",
      "train loss:0.019088297934688487\n",
      "train loss:0.007759162349626402\n",
      "train loss:0.015297475366757473\n",
      "train loss:0.00476301084426197\n",
      "train loss:0.012940034355195857\n",
      "train loss:0.006615727376127095\n",
      "train loss:0.03384886683869265\n",
      "train loss:0.009924652989287755\n",
      "train loss:0.003900963848012799\n",
      "train loss:0.012699699875157448\n",
      "train loss:0.008508797206930466\n",
      "train loss:0.011718379007876669\n",
      "train loss:0.008928713326204828\n",
      "train loss:0.03240610427601997\n",
      "train loss:0.002378918102368796\n",
      "train loss:0.006683849495927692\n",
      "train loss:0.008988274562052409\n",
      "train loss:0.027326856122923758\n",
      "train loss:0.005794806458649159\n",
      "train loss:0.02930514453174303\n",
      "train loss:0.0022972169066846977\n",
      "train loss:0.038813847783014825\n",
      "train loss:0.019967974607890383\n",
      "train loss:0.005493812920800456\n",
      "train loss:0.005828877350807131\n",
      "train loss:0.013582002139620701\n",
      "train loss:0.005002392197127891\n",
      "train loss:0.00706514192766933\n",
      "train loss:0.00466893238681676\n",
      "train loss:0.006404708973101327\n",
      "train loss:0.007470875861802004\n",
      "train loss:0.006206259444203913\n",
      "train loss:0.012508851823181783\n",
      "train loss:0.10366861910761775\n",
      "train loss:0.012186506999804638\n",
      "train loss:0.009627438416226496\n",
      "train loss:0.0037304660958363905\n",
      "train loss:0.015774273552818737\n",
      "train loss:0.0024977422235088185\n",
      "train loss:0.0021379744920844448\n",
      "train loss:0.025338604107097925\n",
      "train loss:0.00979617201487839\n",
      "train loss:0.010151545624769616\n",
      "train loss:0.0019225486129906138\n",
      "train loss:0.02598396951493599\n",
      "train loss:0.005142334087751793\n",
      "train loss:0.011158108491510978\n",
      "train loss:0.002116542091776007\n",
      "train loss:0.0035794673157684338\n",
      "train loss:0.005928600564066817\n",
      "train loss:0.009271849758932272\n",
      "train loss:0.03538710191160273\n",
      "train loss:0.011567619271124527\n",
      "train loss:0.020405536549531363\n",
      "train loss:0.01469187820576175\n",
      "train loss:0.002872605478694737\n",
      "train loss:0.0339059479464082\n",
      "train loss:0.003457852322267779\n",
      "train loss:0.004004982680779089\n",
      "train loss:0.0033774209498366216\n",
      "train loss:0.03944373347182073\n",
      "train loss:0.02903334538717508\n",
      "train loss:0.004692271456988251\n",
      "train loss:0.02960137899612637\n",
      "train loss:0.01372438893642815\n",
      "train loss:0.024520633121290283\n",
      "train loss:0.003609351792147381\n",
      "train loss:0.0018932923901888315\n",
      "train loss:0.05591694544144799\n",
      "train loss:0.01700514605850616\n",
      "train loss:0.02098839078249639\n",
      "train loss:0.004131229039812684\n",
      "train loss:0.009076260728302353\n",
      "train loss:0.027542328428904587\n",
      "train loss:0.0032653899141992133\n",
      "train loss:0.009461104610289485\n",
      "train loss:0.002506087904653493\n",
      "train loss:0.012210664296352343\n",
      "train loss:0.028681541973342713\n",
      "train loss:0.004891902730691816\n",
      "train loss:0.009459517456991097\n",
      "train loss:0.0019422437004568112\n",
      "train loss:0.010010280222814374\n",
      "train loss:0.002809565862399817\n",
      "train loss:0.01897343295200686\n",
      "train loss:0.0015946270592383325\n",
      "train loss:0.003927071821100904\n",
      "train loss:0.016867009649707\n",
      "train loss:0.0014550608911384113\n",
      "train loss:0.008985090850955146\n",
      "train loss:0.007173758625920668\n",
      "train loss:0.002447863871221155\n",
      "train loss:0.0033271221728945993\n",
      "train loss:0.014297073101818478\n",
      "train loss:0.01867873070870514\n",
      "train loss:0.009797524513597168\n",
      "train loss:0.018166840285024084\n",
      "train loss:0.004583557724479464\n",
      "train loss:0.008802215984838465\n",
      "train loss:0.016916663719469472\n",
      "train loss:0.011431755781404603\n",
      "train loss:0.008644880572303228\n",
      "train loss:0.005740495876950924\n",
      "train loss:0.013827257270829407\n",
      "train loss:0.016088024606242247\n",
      "train loss:0.0028176604409963003\n",
      "train loss:0.008169990116348108\n",
      "train loss:0.008213989964429543\n",
      "train loss:0.021497885936945883\n",
      "train loss:0.011797406373968899\n",
      "train loss:0.003867821333198186\n",
      "train loss:0.005212150703583045\n",
      "train loss:0.00593741698155734\n",
      "train loss:0.03261151510503058\n",
      "train loss:0.004430954900110053\n",
      "train loss:0.03754390584718864\n",
      "train loss:0.016095771582575455\n",
      "train loss:0.004946446092022521\n",
      "train loss:0.06092395815499902\n",
      "train loss:0.002829333243167246\n",
      "train loss:0.009257546624397393\n",
      "train loss:0.007226188312133051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.001510427888066737\n",
      "train loss:0.02892915885793053\n",
      "train loss:0.004110747115967729\n",
      "train loss:0.010442836375874898\n",
      "train loss:0.011875270430071633\n",
      "train loss:0.005481393410313927\n",
      "train loss:0.009071863690667968\n",
      "train loss:0.00470503740784601\n",
      "train loss:0.004126599651472907\n",
      "train loss:0.011952653341901678\n",
      "train loss:0.0068198901071365205\n",
      "train loss:0.02035448492404971\n",
      "train loss:0.00884317582926594\n",
      "train loss:0.024457143065716194\n",
      "train loss:0.020944620524130587\n",
      "train loss:0.0009319890833455925\n",
      "train loss:0.004620850124145213\n",
      "train loss:0.006366845366391525\n",
      "train loss:0.04026268320107864\n",
      "train loss:0.0038993728824061524\n",
      "train loss:0.006233207441095543\n",
      "train loss:0.0018398753769265155\n",
      "train loss:0.00583716295466149\n",
      "train loss:0.012271479165426084\n",
      "train loss:0.0035998942641957744\n",
      "train loss:0.004067352993648441\n",
      "train loss:0.008462876719413608\n",
      "train loss:0.012023282491580136\n",
      "train loss:0.01567815530211123\n",
      "train loss:0.006970304707076776\n",
      "train loss:0.017768116114051945\n",
      "train loss:0.02350253168604187\n",
      "train loss:0.006317256896655513\n",
      "train loss:0.010683953413282902\n",
      "train loss:0.0038651758036409023\n",
      "train loss:0.01966608738602202\n",
      "train loss:0.0028464193947015455\n",
      "train loss:0.005743460173107635\n",
      "train loss:0.03389939149564872\n",
      "train loss:0.0010671557885509144\n",
      "train loss:0.020530081469017945\n",
      "train loss:0.0034127693017111634\n",
      "train loss:0.006111998905830875\n",
      "train loss:0.00574316157574403\n",
      "train loss:0.0028967092137388356\n",
      "train loss:0.010390294278335137\n",
      "train loss:0.009243566231929758\n",
      "train loss:0.00957180286857853\n",
      "train loss:0.0829558107355605\n",
      "train loss:0.0022324180104316207\n",
      "train loss:0.009966398194886247\n",
      "train loss:0.003592726154294486\n",
      "train loss:0.004639368214044377\n",
      "train loss:0.002417146258622298\n",
      "train loss:0.011263382029864568\n",
      "train loss:0.0072227484828660685\n",
      "train loss:0.005500387125216924\n",
      "train loss:0.014236486573254048\n",
      "train loss:0.013316823336595065\n",
      "train loss:0.0028411189124246334\n",
      "train loss:0.004797799847347112\n",
      "train loss:0.0059256067569385526\n",
      "train loss:0.026777526748631902\n",
      "train loss:0.003469374144765601\n",
      "train loss:0.013545427782256475\n",
      "train loss:0.0038817663916382947\n",
      "train loss:0.0019817083316835816\n",
      "train loss:0.0047398098696858175\n",
      "train loss:0.002995356056337074\n",
      "train loss:0.006480440945966271\n",
      "train loss:0.003950660361643044\n",
      "train loss:0.007038499630958659\n",
      "train loss:0.004076398398421912\n",
      "train loss:0.013446889477365269\n",
      "train loss:0.003112643841582152\n",
      "train loss:0.004351555937639226\n",
      "train loss:0.006928394830272578\n",
      "train loss:0.01762368390796757\n",
      "train loss:0.020105001475973446\n",
      "train loss:0.007186298794623056\n",
      "train loss:0.011628601349623013\n",
      "train loss:0.015452468952050331\n",
      "train loss:0.008970034617853817\n",
      "train loss:0.006523062987719092\n",
      "train loss:0.006612217113801029\n",
      "train loss:0.0023154603117193488\n",
      "train loss:0.00830639430518262\n",
      "train loss:0.0029981363890358354\n",
      "train loss:0.003909333101536303\n",
      "train loss:0.015543924377796772\n",
      "train loss:0.020261990699125026\n",
      "train loss:0.013275402077684016\n",
      "train loss:0.0028902890865007834\n",
      "train loss:0.001460002820095356\n",
      "train loss:0.007359167924781701\n",
      "train loss:0.008645401018162342\n",
      "train loss:0.004260899090342654\n",
      "train loss:0.008412758097886391\n",
      "train loss:0.0016384322892309686\n",
      "train loss:0.02326346418917157\n",
      "train loss:0.006125329290966085\n",
      "train loss:0.00481526145788416\n",
      "train loss:0.007281264265754655\n",
      "train loss:0.009411835815706461\n",
      "=== epoch:9, train acc:0.99, test acc:0.987 ===\n",
      "train loss:0.013454951082505454\n",
      "train loss:0.002602364479284991\n",
      "train loss:0.0035890841638453242\n",
      "train loss:0.0033935621849816755\n",
      "train loss:0.005401603161037602\n",
      "train loss:0.0214636342794528\n",
      "train loss:0.0015462684448079053\n",
      "train loss:0.005096664218849912\n",
      "train loss:0.020468896743661348\n",
      "train loss:0.0013572154299933685\n",
      "train loss:0.016277380431955524\n",
      "train loss:0.00996447405450446\n",
      "train loss:0.0070364517013200446\n",
      "train loss:0.03658016383930656\n",
      "train loss:0.0450943119973338\n",
      "train loss:0.02014229989899636\n",
      "train loss:0.006947937543861276\n",
      "train loss:0.005815871378265526\n",
      "train loss:0.018608395873667912\n",
      "train loss:0.008941828454751777\n",
      "train loss:0.010491080735434024\n",
      "train loss:0.034085486862109465\n",
      "train loss:0.003612196787795908\n",
      "train loss:0.00528248565485253\n",
      "train loss:0.01030664885562763\n",
      "train loss:0.001349457237837088\n",
      "train loss:0.0028096147731271914\n",
      "train loss:0.006354111479266824\n",
      "train loss:0.007369816241093881\n",
      "train loss:0.03408520561012855\n",
      "train loss:0.007272637376678525\n",
      "train loss:0.0053853825506559524\n",
      "train loss:0.0016458415524840983\n",
      "train loss:0.009350536190194188\n",
      "train loss:0.0025411651663019372\n",
      "train loss:0.00256527375086186\n",
      "train loss:0.0029532774045086516\n",
      "train loss:0.0034707914559456974\n",
      "train loss:0.0021026779253563725\n",
      "train loss:0.007589580269566631\n",
      "train loss:0.006894809384981316\n",
      "train loss:0.01826203777524672\n",
      "train loss:0.004313023282644923\n",
      "train loss:0.002008868143248843\n",
      "train loss:0.0008546843278458631\n",
      "train loss:0.0025868529766189465\n",
      "train loss:0.026774108147122586\n",
      "train loss:0.0036558356229841145\n",
      "train loss:0.04090411808953269\n",
      "train loss:0.016406597931340763\n",
      "train loss:0.003847945073781458\n",
      "train loss:0.024273362059269978\n",
      "train loss:0.013622863407337498\n",
      "train loss:0.017308307520582003\n",
      "train loss:0.0069921274645732435\n",
      "train loss:0.011091103971236211\n",
      "train loss:0.031856359542277994\n",
      "train loss:0.003315794045108783\n",
      "train loss:0.0022031650966664084\n",
      "train loss:0.004309010365599411\n",
      "train loss:0.013302013824394237\n",
      "train loss:0.013106986205882942\n",
      "train loss:0.008052604451595485\n",
      "train loss:0.0077961387336313226\n",
      "train loss:0.004803138375109153\n",
      "train loss:0.0017146358516262892\n",
      "train loss:0.01618171023169913\n",
      "train loss:0.0055104636550842466\n",
      "train loss:0.03230144458950467\n",
      "train loss:0.00041896881781718054\n",
      "train loss:0.027445190349311862\n",
      "train loss:0.009860162403886253\n",
      "train loss:0.004624096802362554\n",
      "train loss:0.0012626310378021078\n",
      "train loss:0.00101031747055891\n",
      "train loss:0.006120362589255322\n",
      "train loss:0.007280841457945794\n",
      "train loss:0.0043278830627725305\n",
      "train loss:0.0035181766864099736\n",
      "train loss:0.00503244966216712\n",
      "train loss:0.020162159914622824\n",
      "train loss:0.005518469192560494\n",
      "train loss:0.014847771424095\n",
      "train loss:0.016635473082684638\n",
      "train loss:0.07089984013456373\n",
      "train loss:0.0028454495498683287\n",
      "train loss:0.003849358577355755\n",
      "train loss:0.002410555419821445\n",
      "train loss:0.0062363612535610655\n",
      "train loss:0.0016893746933542978\n",
      "train loss:0.002126831928921542\n",
      "train loss:0.027097986135591428\n",
      "train loss:0.028727401978709953\n",
      "train loss:0.014643197373528736\n",
      "train loss:0.004597397440519048\n",
      "train loss:0.006997959389986782\n",
      "train loss:0.03825380241801347\n",
      "train loss:0.0050364725480141585\n",
      "train loss:0.0004966284670175003\n",
      "train loss:0.0030278895503101865\n",
      "train loss:0.0016432068571959166\n",
      "train loss:0.00272165737297601\n",
      "train loss:0.002467354455296918\n",
      "train loss:0.013515651514157088\n",
      "train loss:0.0017126079019425431\n",
      "train loss:0.011856639586133673\n",
      "train loss:0.004245960872677485\n",
      "train loss:0.006996032884933406\n",
      "train loss:0.016097628205133788\n",
      "train loss:0.015577719341892806\n",
      "train loss:0.02971825311855024\n",
      "train loss:0.008619536804191506\n",
      "train loss:0.012646603216161345\n",
      "train loss:0.012661193822698173\n",
      "train loss:0.007075547264936813\n",
      "train loss:0.002247658976949931\n",
      "train loss:0.0015852290478418963\n",
      "train loss:0.047817473981931195\n",
      "train loss:0.016574277053825068\n",
      "train loss:0.008702022072817069\n",
      "train loss:0.005211355834632325\n",
      "train loss:0.003486507614506025\n",
      "train loss:0.003672831890088011\n",
      "train loss:0.0028815731200149018\n",
      "train loss:0.015593137417390969\n",
      "train loss:0.0009375086529588571\n",
      "train loss:0.014024275080545441\n",
      "train loss:0.018400602556395425\n",
      "train loss:0.009296286127019944\n",
      "train loss:0.002319765187677249\n",
      "train loss:0.006061565915634429\n",
      "train loss:0.015011268294835022\n",
      "train loss:0.007105360124339755\n",
      "train loss:0.008794372347404935\n",
      "train loss:0.012652219504642586\n",
      "train loss:0.003667683800997002\n",
      "train loss:0.005288291238998253\n",
      "train loss:0.02121137356239329\n",
      "train loss:0.0061322492672459105\n",
      "train loss:0.012161471022394524\n",
      "train loss:0.020511489804867975\n",
      "train loss:0.004256583130947689\n",
      "train loss:0.009835789943204647\n",
      "train loss:0.004931399907414323\n",
      "train loss:0.0019647894487469317\n",
      "train loss:0.012995951822709127\n",
      "train loss:0.007166508163223312\n",
      "train loss:0.01729388390070751\n",
      "train loss:0.006980550656250271\n",
      "train loss:0.006952239412137772\n",
      "train loss:0.0047490150010669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.015868012465687094\n",
      "train loss:0.0013325867020251433\n",
      "train loss:0.03056002840803142\n",
      "train loss:0.00987981717317511\n",
      "train loss:0.0034877092388814583\n",
      "train loss:0.003445345917825227\n",
      "train loss:0.003186665195103755\n",
      "train loss:0.0030241871124656616\n",
      "train loss:0.03464921290780745\n",
      "train loss:0.0008415270701208962\n",
      "train loss:0.001345889758047253\n",
      "train loss:0.010357839995908873\n",
      "train loss:0.018120267162504814\n",
      "train loss:0.006248107023525099\n",
      "train loss:0.009286807376967292\n",
      "train loss:0.001851955576626075\n",
      "train loss:0.0026930130345837427\n",
      "train loss:0.0031602744537940625\n",
      "train loss:0.002200433488011448\n",
      "train loss:0.00992806267509327\n",
      "train loss:0.020654710793131175\n",
      "train loss:0.0285131466180462\n",
      "train loss:0.00991062519401294\n",
      "train loss:0.004054257155172329\n",
      "train loss:0.006159395096076152\n",
      "train loss:0.0025242918247859974\n",
      "train loss:0.003970914156680455\n",
      "train loss:0.005041643236320436\n",
      "train loss:0.004762011495693098\n",
      "train loss:0.0007236366585385201\n",
      "train loss:0.020809316141857978\n",
      "train loss:0.0014754017705333041\n",
      "train loss:0.0005023102941892029\n",
      "train loss:0.017459026189990484\n",
      "train loss:0.01349817466640584\n",
      "train loss:0.0017603803916922966\n",
      "train loss:0.004621217920285638\n",
      "train loss:0.05367635371332411\n",
      "train loss:0.016891406354467967\n",
      "train loss:0.00895866550624838\n",
      "train loss:0.0027262519276395796\n",
      "train loss:0.008083563075162318\n",
      "train loss:0.007119775374191889\n",
      "train loss:0.006397597955238852\n",
      "train loss:0.0012134922800911565\n",
      "train loss:0.005658702318556939\n",
      "train loss:0.0041698215469494196\n",
      "train loss:0.00892703739209503\n",
      "train loss:0.043077297946895715\n",
      "train loss:0.008669345193533594\n",
      "train loss:0.0083604266763226\n",
      "train loss:0.0047670734666331515\n",
      "train loss:0.0021371886828987817\n",
      "train loss:0.009393977611258035\n",
      "train loss:0.017543615699590745\n",
      "train loss:0.0006830570355225817\n",
      "train loss:0.007440141666704193\n",
      "train loss:0.007614096720023007\n",
      "train loss:0.008037025815974624\n",
      "train loss:0.013065370973317496\n",
      "train loss:0.011300740479854392\n",
      "train loss:0.008957171126141479\n",
      "train loss:0.011342871889249711\n",
      "train loss:0.0029518678412603517\n",
      "train loss:0.0022294308017627566\n",
      "train loss:0.012002831992139888\n",
      "train loss:0.0898116813461096\n",
      "train loss:0.022965097656631746\n",
      "train loss:0.015375990165760064\n",
      "train loss:0.004793551525313698\n",
      "train loss:0.02277746541739394\n",
      "train loss:0.015934592322985826\n",
      "train loss:0.002680994593051715\n",
      "train loss:0.0058633600674158045\n",
      "train loss:0.0008628463482735017\n",
      "train loss:0.011583879507142101\n",
      "train loss:0.0028160189094151022\n",
      "train loss:0.002394639052187592\n",
      "train loss:0.00215841765938419\n",
      "train loss:0.009703100604268622\n",
      "train loss:0.012386143049035219\n",
      "train loss:0.003416466382460544\n",
      "train loss:0.006235560408189016\n",
      "train loss:0.002113877016663922\n",
      "train loss:0.009841991417530215\n",
      "train loss:0.015015396734780688\n",
      "train loss:0.001425868486032967\n",
      "train loss:0.037953150768251086\n",
      "train loss:0.016091994056172173\n",
      "train loss:0.012653401507602757\n",
      "train loss:0.013843017167147116\n",
      "train loss:0.0011877757850836423\n",
      "train loss:0.006228777371081347\n",
      "train loss:0.00838546913025844\n",
      "train loss:0.0031260684731656426\n",
      "train loss:0.002062137815231256\n",
      "train loss:0.005552642909317166\n",
      "train loss:0.013133365339005434\n",
      "train loss:0.006395898970625707\n",
      "train loss:0.033128598095506376\n",
      "train loss:0.0046334245511634575\n",
      "train loss:0.012968362054079276\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
